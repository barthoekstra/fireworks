[
["index.html", "Fireworks Preface Abstract How to use this document", " Fireworks Bart Hoekstra 2020-07-24 Preface Abstract How to use this document Knit it Use the following line in the console or click Build Book in RStudio. bookdown::render_book(input = &quot;index.Rmd&quot;, output_format = &quot;bookdown::gitbook&quot;, clean = TRUE) Full-reproduction mode In the spirit of reproducibility, the entire analysis, which happens to be contained in this book, can be reproduced at the push of a button (almost). To facilitate faster reproduction, some code chunks are only run when full-reproduction mode is switched on. This can be done by setting the R variable full_repro to TRUE in build_bookdown.R. "],
["01.Selecting-take-off-moments.html", "1 Selecting firework take-off moment 1.1 Setting up the processing environment 1.2 Calculate the vertical profiles 1.3 Generate time series of vertical profiles 1.4 Identifying moment of take-off 1.5 Determining maximum range from radar to detect birds", " 1 Selecting firework take-off moment For this study we select the moment of ‘en masse’ take-off of birds at the turn of the year. To make sure birds are still fairly ‘close’ to the take-off habitat, we therefore focus on the period where the increase in VIR (Vertically Integrated Reflectivity) is the highest. Based on experience, one would expect this to occur between 00:05 and 00:15 on January 1st, as people tend to light the fireworks right after they have shared New Year’s wishes with each other. 1.1 Setting up the processing environment We use vol2bird included in the bioRad package (Dokter et al. 2019) to calculate the vertical profiles of reflectivity, from which we determine the exact take off moment of birds. This implies we assume birds take to the skies everywhere simultaneously, but that seems a realistic assumption given that the lighting of fireworks is synchronised by the local time, rather than the time of sunset/sunrise. library(bioRad) library(ggplot2) library(dplyr) library(tidyr) Sys.setenv(TZ = &quot;UTC&quot;) 1.2 Calculate the vertical profiles We calculate vertical profiles for the period between December 31st, 2017 22:00 and 01:00 UTC on January 1st, 2018, which corresponds with 23:00 til 02:00 local Amsterdam time (UTC + 1). It is not necessary to generate so many vp files, but it gives a better temporal overview if we add some ‘temporal padding’ around the fireworks event. Beware: to calculate the vertical profiles, a running instance of Docker is required. This code chunk will only run in full-reproduction mode. See Appendix [generating-vps-for-den-helder-radar] for a more detailed explanation why we limit vp generation of Den Helder to certain azimuths. fireworks_scans &lt;- Sys.glob(file.path(&quot;data/raw/pvol/fireworks-2017-2018&quot;, &quot;*_ODIM.h5&quot;)) cat(&quot;Files left to process: &quot;, length(fireworks_scans), &quot;\\n&quot;) i &lt;- 1 for (scan in fireworks_scans) { if (i %% 5 == 0) { cat(i, &quot;... &quot;) } vpfile_out &lt;- sub(&quot;raw/pvol/fireworks-2017-2018&quot;, &quot;processed/vp/fireworks-2017-2018&quot;, scan) if (grepl(&quot;RAD_NL61&quot;, vpfile_out)) { try(calculate_vp(scan, vpfile = vpfile_out, verbose = FALSE, mount = dirname(fireworks_scans[1]), azim_min = 90, azim_max = 200, h_layer = 50, n_layer = 80)) } else { try(calculate_vp(scan, vpfile = vpfile_out, verbose = FALSE, mount = dirname(fireworks_scans[1]), h_layer = 50, n_layer = 80)) } i &lt;- i + 1 } 1.3 Generate time series of vertical profiles We can now generate a time series of vertical profiles and plot the bird densities to get an idea of what was going on during NYE of 2017-2018. fw_hrw_vpts &lt;- Sys.glob(file.path(&quot;data/processed/vp/fireworks-2017-2018&quot;, &quot;*NL62*&quot;)) %&gt;% read_vpfiles() %&gt;% bind_into_vpts() %&gt;% regularize_vpts(interval = &quot;auto&quot;) ## projecting on 300 seconds interval grid... fw_dhl_vpts &lt;- Sys.glob(file.path(&quot;data/processed/vp/fireworks-2017-2018&quot;, &quot;*NL61*&quot;)) %&gt;% read_vpfiles() %&gt;% bind_into_vpts() %&gt;% regularize_vpts(interval = &quot;auto&quot;) ## projecting on 300 seconds interval grid... start &lt;- as.POSIXct(&quot;2017-12-31 22:00:00&quot;) end &lt;- as.POSIXct(&quot;2018-01-01 01:00:00&quot;) indexes_hrw &lt;- which(fw_hrw_vpts$datetime &gt;= start &amp; fw_hrw_vpts$datetime &lt;= end) indexes_dhl &lt;- which(fw_dhl_vpts$datetime &gt;= start &amp; fw_dhl_vpts$datetime &lt;= end) # Should mostly be identical title_hrw &lt;- expression(&quot;Herwijnen: volume density [#/km&quot;^3 * &quot;]&quot;) title_dhl &lt;- expression(&quot;Den Helder: volume density [#/km&quot;^3 * &quot;]&quot;) plot(fw_hrw_vpts[indexes_hrw], main = title_hrw) plot(fw_dhl_vpts[indexes_dhl], main = title_dhl) Both plots for Herwijnen and Den Helder show exactly what we would expect: comparatively low densities of birds aloft leading up to midnight (23:00 CET), then suddenly a strong increase of birds right after midnight. For Den Helder this peak appears much more pronounced, whereas for Herwijnen the period of disturbance seems to take considerably longer. This is probably due to the vastly different environment around the radar site: Herwijnen is located solidly in the center of the country, whereas Den Helder is located close to the coast on a ‘peninsula’ with much less land in the surroundings, especially as vol2bird only takes into account rangegates within 5-35km of the radar. 1.4 Identifying moment of take-off We integrate the time series of vertical profiles, so we can calculate the vir derivatives and determine in what volume scan birds really take to the skies for each radar separately. integrated_hrw &lt;- integrate_profile(fw_hrw_vpts) integrated_dhl &lt;- integrate_profile(fw_dhl_vpts) integrated_hrw$vir_deriv &lt;- c(NA, diff(integrated_hrw$vir, 1)) integrated_dhl$vir_deriv &lt;- c(NA, diff(integrated_dhl$vir, 1)) integrated_hrw$radar &lt;- &quot;Herwijnen&quot; integrated_dhl$radar &lt;- &quot;Den Helder&quot; integrated &lt;- rbind(integrated_hrw, integrated_dhl) integrated_l &lt;- integrated %&gt;% pivot_longer(-c(&quot;datetime&quot;, &quot;radar&quot;), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) %&gt;% filter(variable == &quot;vir&quot; | variable == &quot;vir_deriv&quot;) %&gt;% filter(datetime &gt;= start &amp; datetime &lt;= end) max_vir_deriv &lt;- integrated_l %&gt;% filter(variable == &quot;vir_deriv&quot;) %&gt;% drop_na() %&gt;% group_by(radar) %&gt;% summarize(max_value = max(value), datetime = datetime[which.max(value)], .groups = &quot;drop_last&quot;) theme_set(theme_bw()) ggplot(integrated_l, aes(x = datetime)) + geom_line(aes(y = value, colour = radar, linetype = variable)) + scale_x_datetime(breaks = &quot;10 min&quot;, date_labels = &quot;%H:%M&quot;, expand = c(0, 0)) + scale_color_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + scale_linetype_discrete(name = &quot;Line type&quot;, labels = c(&quot;VIR&quot;, expression(paste(Delta,&quot;VIR/scan&quot;)))) + labs(title = &quot;Time series of Vertically Integrated Reflectivities (VIR)&quot;, subtitle = &quot;NYE 2017-2018&quot;, x = &quot;Time (CET)&quot;, y = &quot;VIR&quot;, colour = &quot;Radar&quot;, linetype = &quot;Linetype&quot;) + theme(axis.text.x = element_text(angle = -90), panel.grid.minor = element_blank()) The plot shows a rapid increase in VIR in the first 15 and 20 minutes after midnight (23:00 CET) for the Den Helder and Herwijnen radar respectively. After that period, VIR starts to drop, faster for Den Helder than for Herwijnen, possibly as a result of birds dispersing from the North Holland mainland towards the IJsselmeer area, which we have deliberately excluded from the generation of the vertical profiles (by selecting azimuths that are above land). To reduce the effect of bird dispersal on our analysis, we will focus only on the first 5 minute-scan during which VIR grows rapidly. That is from 23:05 CET until 23:10 CET. pvol_folder &lt;- &quot;data/raw/pvol/fireworks-2017-2018/&quot; scan_timestamp &lt;- lubridate::ymd_hm(&quot;2017-12-31 23:05&quot;) pvol_hrw_path &lt;- paste(pvol_folder, &quot;RAD_NL62_VOL_NA_&quot;, format(scan_timestamp, &quot;%Y%m%d%H%M&quot;), &quot;_ODIM.h5&quot;, sep = &quot;&quot;) pvol_dhl_path &lt;- paste(pvol_folder, &quot;RAD_NL61_VOL_NA_&quot;, format(scan_timestamp, &quot;%Y%m%d%H%M&quot;), &quot;_ODIM.h5&quot;, sep = &quot;&quot;) save(pvol_hrw_path, pvol_dhl_path, file = &quot;data/processed/pvol_selection.RData&quot;) So at first we will continue using the following files in this study: Herwijnen: RAD_NL62_VOL_NA_201712312305_ODIM.h5 Den Helder: RAD_NL61_VOL_NA_201712312305_ODIM.h5 1.5 Determining maximum range from radar to detect birds We can assume that the flight altitudes derived from vol2bird are representative for flight altitudes throughout the country. Given that most of the disturbance happens at comparatively low altitudes, the radar is likely to ‘overshoot’ this entirely at substantial distances away from the radar where compensating for range-effects (e.g. using (Kranstauber et al. 2020)) will thus have no effect. Therefore we need to determina a maximum range we could still feasibly measure birds aloft. We can do so by reinspecting the vpts plots from before, but now focussing on the lowest 1km. plot(fw_hrw_vpts[indexes_hrw], ylim = c(0, 1000)) plot(fw_dhl_vpts[indexes_dhl], ylim = c(0, 1000)) We can now see that above 600m ASL or so the density starts to drop substantially, so we will set this as the altitudinal cutoff to determine the range up to which we will be using the data coming from the range-bias correction (Kranstauber et al. 2020). Although birds are somewhat lower in height overall in Den Helder than Herwijnen, this is an acceptable threshold as Herwijnen anyways covers much more land area within our study domain than Den Helder, and thus a more representative range-bias correction for Herwijnen is more important than Den Helder. Now we can calculate the distance at which that height is the height of the lowest elevation (0.3 degrees) and round to the nearest kilometer. altitude_cutoff &lt;- 600 ranges &lt;- seq(0, 180000, 100) beamheights &lt;- beam_height(ranges, 0.3) nearest_index &lt;- which(abs(beamheights - altitude_cutoff) == min(abs(beamheights - altitude_cutoff))) distance_cutoff &lt;- plyr::round_any(ranges[nearest_index], 1000) distance_cutoff ## [1] 66000 So at roughly 66 kilometers from the radar, the lowest elevation scan pierces the sky at an altitude of 600m. References "],
["02.Radar-data-preprocessing.html", "2 Radar Data Preprocessing 2.1 Setting-up the pre-processing environment 2.2 Removing electromagnetic interference 2.3 Filter meteorology using the depolarization ratio 2.4 Remove classified precipitation from polar volumes 2.5 Filter ground clutter 2.6 Range-bias correction 2.7 Keep biology/meteorology classification 2.8 Distance to radar 2.9 Spatial coordinates 2.10 Visualising range-bias correction 2.11 Preprocess additional scans", " 2 Radar Data Preprocessing Weather radar data of the firework events at the turns of the years usually contain some degree of precipitation clutter. To filter out precipitation advanced algorithms such as MistNet have been developed, but as we are dealing with dual-pol data here, we can use a simpler and yet robust method using the depolarization ratio (Kilambi, Fabry, and Meunier 2018). To make sure our processed weather radar data does not contain any significant proportions of precipitation or ground clutter anymore, we process the data as follows: We remove electromagnetic interference based on a visual inspection of the scan and throw out all data of affected rays. We calculate the depolarization ratio (Kilambi, Fabry, and Meunier 2018) and separate biology from meteorology by classifying all range gates with a depolarization ratio \\(&gt;-12dB\\) as biology. We subsequently ‘despeckle’ this, to remove obvious misclassifications. We average reflectivity over a number of scans before the time of the fireworks event and throw out the range-gates with highest average reflectivities. All these steps can be undertaken directly on the polar volume data, so we can subsequently directly plug the cleaned up volume into the range-bias correction. 2.1 Setting-up the pre-processing environment As usual, we use bioRad (Dokter et al. 2019), but this time we include plotly for some interactive plotting. library(bioRad) library(plotly) library(gridExtra) library(ggpubr) library(mapview) library(viridis) library(raster) library(dplyr) library(magrittr) 2.2 Removing electromagnetic interference We have determined in which scans birds are taking off based on the maximum increase in reflectivity in the scan for each of the involved radars. Let’s now look at these scans to see how much filtering for electromagnetic interference we need to do. The easiest way to determine which rays are subject to this interference is by plotting the scans in polar coordinates (\\((r, \\alpha)\\)), so interference stands out as horizontal lines of more or less constant, or very gradually changing reflectivities. Plotting using plotly makes it easier to identify the specific problematic rays as one can zoom in to identify the exact azimuths \\(\\alpha\\). The scans we will be using: Herwijnen: RAD_NL62_VOL_NA_201712312305_ODIM.h5 Den Helder: RAD_NL61_VOL_NA_201712312305_ODIM.h5 For illustrative purposes we will only illustrate removal of EM interference for the Herwijnen, as the procedure for Den Helder is exactly identical, but this scan contains very little of said clutter. pvol_hrw &lt;- read_pvolfile(pvol_hrw_path, param = &quot;all&quot;) pvol_dhl &lt;- read_pvolfile(pvol_dhl_path, param = &quot;all&quot;) scan &lt;- plot(pvol_hrw$scans[[1]], param = &quot;DBZH&quot;, xlim = c(0, 180000)) + theme_dark() ggplotly(scan) Right away we can see that rays at two places in the scan are subject to electromagnetic interference. This is probably most problematic in the lowest elevations of the volume scan, but nevertheless each of the length(pvol_hrw$scans) scans have be checked manually. Doing so results in the identification of the following rays that contain electromagnetic interference (ei_rays), organised in a list with the scan numbers (organised ascendingly per elevation angle) as keys. Admittedly: there is another ray that seems to contain interference in the first scan, but this is so far away from the radar (150km+) it should not affect our results as no meaningful numbers of birds can be detected at that range anyways, so there is no need to remove the entire ray. Similarly, there are similar patterns of interference/clutter in higher elevation scans, but these too should not affect our results. ei_rays_hrw &lt;- list(c(201, 202, 214, 215), # scan 1 c(201, 202, 214, 215), # scan 2 c(201, 202, 214, 215), # scan 3 c(202, 214, 215)) # scan 4 names(ei_rays_hrw) &lt;- c(1, 2, 3, 4) ei_rays_dhl &lt;- list(c(60, 61)) # scan 1 names(ei_rays_dhl) &lt;- c(1) We can now remove the data for the affected rays in the corresponding scans by setting the values to NA (see R/remove_rays.R). source(&quot;R/remove_rays.R&quot;) pvol_hrw &lt;- remove_rays(pvol_hrw, rays = ei_rays_hrw) pvol_dhl &lt;- remove_rays(pvol_dhl, rays = ei_rays_dhl) 2.2.1 Verify removal of rays with EM interference If removal is correct, the \\((r,\\alpha)\\) plots should not show clear horizontal structures anymore. i = 1 plot(pvol_hrw$scans[[i]], param = &quot;DBZH&quot;, xlim = c(0, 180000)) + theme_dark() + labs(title = &quot;Herwijnen: Cleaned from EM interference&quot;, subtitle = paste(&quot;Elevation:&quot;, round(pvol_hrw$scans[[i]]$attributes$where$elangle, 1))) plot(pvol_dhl$scans[[i]], param = &quot;DBZH&quot;, xlim = c(0, 180000)) + theme_dark() + labs(title = &quot;Den Helder: Cleaned from EM interference&quot;, subtitle = paste(&quot;Elevation:&quot;, round(pvol_dhl$scans[[i]]$attributes$where$elangle, 1))) That seems to work nicely. The remaining bits of clutter won’t affect the results much as they cover fairly small areas and consistently appear at longer distances away from the radar, where birds are difficult to detect anyways. 2.3 Filter meteorology using the depolarization ratio Meteorology can be filtered using the depolarization ratio following Kilambi et al. (2018). We calculate the depolarization ratio for the raw pvol data after EM interference has been removed and subsequently ‘despeckle’ the results to improve the classification. Despeckling works by comparing the classification of the majority of the neighbourhood rangegates with the classification of the center rangegate, and changing the latter to reflect the majority of the neighbourhood classification if there is a difference. We define the ‘neighbourhood’ as a \\(3^{\\circ}\\) by \\(3 \\times rscale\\) area centered around a focal rangegate (3 rangegates in azimuth \\(\\times\\) 3 rangegates in range). Selecting the rangegates while taking the sphericity of the radar scan into account (e.g. ray 360 should be directly adjacent to ray 1) is made easier with the R/window_coords.R function. The despeckling is implemented in R/despeckle_scan_logical.R. With the despeckling algorithm in place, we can: Calculate the depolarization ratio (DPR). Classify biology as rangegates where DPR &gt; -12 and store this classification as BIOLR (Biology Raw) scan parameter in the pvol object. Despeckle the classification and store the outcome in the BIOLD (Biology Despeckled) scan parameter in the pvol object. source(&quot;R/window_coords.R&quot;) source(&quot;R/despeckle_scan_logical.R&quot;) # Calculate depolarization ratio, classify and despeckle biology classifications for the entire volume calculate_dpr &lt;- function(pvol){ for (i in seq_along(pvol$scans)) { # Calculate ZDR as ZDR = DBZH - DBZV pvol$scans[[i]]$params$ZDR &lt;- pvol$scans[[i]]$params$DBZH - pvol$scans[[i]]$params$DBZV attributes(pvol$scans[[i]]$params$ZDR)$param &lt;- &quot;ZDR&quot; # Calculate depolarization ratio zdr_linear &lt;- 10 ** (pvol$scans[[i]]$params$ZDR / 10) dpr_linear &lt;- (zdr_linear + 1 - 2 * sqrt(zdr_linear) * pvol$scans[[i]]$params$RHOHV) / (zdr_linear + 1 + 2 * sqrt(zdr_linear) * pvol$scans[[i]]$params$RHOHV) pvol$scans[[i]]$params$DPR &lt;- 10 * log10(dpr_linear) attributes(pvol$scans[[i]]$params$DPR)$param &lt;- &quot;DPR&quot; # Classify based on depolarization ratio biology &lt;- (pvol$scans[[i]]$params$DPR &gt; -12) * 1 # multiply by 1 to convert TRUE/FALSE to 1/0 class(biology) &lt;- c(&quot;param&quot;, &quot;matrix&quot;) attributes(biology) &lt;- attributes(pvol$scans[[i]]$params$DPR) # copy attributes from DPR attributes(biology)$param &lt;- &quot;BIOLR&quot; pvol$scans[[i]]$params$BIOLR &lt;- biology # Despeckle biology classification pvol$scans[[i]]$params$BIOLD &lt;- pvol$scans[[i]]$params$BIOLR pvol$scans[[i]]$params$BIOLD &lt;- despeckle_scan_logical(pvol$scans[[i]]$params$BIOLD) attributes(pvol$scans[[i]]$params$BIOLD)$param &lt;- &quot;BIOLD&quot; } return(pvol) } pvol_hrw &lt;- suppressWarnings(calculate_dpr(pvol_hrw)) # Will throw NaN warnings if not suppressed pvol_dhl &lt;- suppressWarnings(calculate_dpr(pvol_dhl)) 2.3.1 Verify DPR-based classification Now let’s plot some PPIs to verify the accuracy of DPR-based classification and the subsequent despeckling, by plotting DBZH, VRADH, DPR, BIOLR and BIOLD. source(&quot;R/side_by_side_ppi.R&quot;) side_by_side_ppi(pvol_hrw, pvol_dhl, &quot;Herwijnen&quot;, &quot;Den Helder&quot;, params = c(&quot;DBZH&quot;, &quot;VRADH&quot;, &quot;DPR&quot;, &quot;BIOLR&quot;, &quot;BIOLD&quot;)) The plots show accurate classification of the obvious precipitation zones, except at the edges of these echoes, where BIOLD is a vast improvement over BIOLR, showing the value of despeckling. Similarly, there is a lot of ‘noise’ where birds should be, but despeckling takes care of most of that quite nicely as well. Additionally, it shows a pattern we would expect to see: at closer distances to the radar most ‘speckles’ that are not near to precipitation zones are turned into biology, and at distances further from the radar they are more often ‘flipped’ to meteorology. This method may not be perfect, but it classifies birds quite conservatively. The few misclassifications that remain should not affect the results so much, as they are few in number and do not occur at the centers of precipitation echoes, so they are not likely to turn into numerical outliers. 2.4 Remove classified precipitation from polar volumes Now that we have accurate classifications of the rangegates based on depolarization ratios, we can start to remove the precipitation from the polar volumes, to retain a scan that comprises of only birds (with a few occasional misclassifications). As there are areas where DPR and DBZH do not overlap, we also have to remove all rangegates that are not classified. source(&quot;R/remove_precipitation.R&quot;) pvol_hrw &lt;- remove_precipitation(pvol_hrw) pvol_dhl &lt;- remove_precipitation(pvol_dhl) Plotting the same PPIs as before should now show a cleaned-up/precipitation-free scan next to the classifications. side_by_side_ppi(pvol_hrw, pvol_dhl, &quot;Herwijnen&quot;, &quot;Den Helder&quot;, params = c(&quot;DBZH&quot;, &quot;VRADH&quot;)) That looks very good for both Herwijnen and Den Helder radars, but for the latter we have a lot of sea clutter that still needs to be removed, but that will come when ground clutter is filtered. 2.5 Filter ground clutter We will filter out ground clutter by calculating summary statistics of the rangegate reflectivities over: The 36 scans preceding the scans selected for the study of the fireworks event (= 3 hours worth of scans). A day of clear weather closest to the 31st of December 2017. For each we will filter ground clutter based on the mean DBZH values. Using the variance and mad of the DBZH was tested, but has a few difficulties: variance is very sensitive to the outliers caused by rangegates with NA values (detection below the ‘mds’, the minimum detectable signal) occasionally flipping over to a noisy measurement, resulting in very high variances. mad is much more robust to outliers, but to compute these values we need to set NA cells to the ‘mds’ (minimum detectable signal), which will result in mad values close to, or exactly 0 for cells that never reflected as well as true static clutter, so it’s difficult to separate those. Finally, a visual inspection showed the mean and mad of DBZH (assuming one could overcome the aforementioned problem with the latter) do not differ much, but the mean is somewhat more ‘aggressive’ in filtering, which in this case is quite good. Combining the clutter removal based on a clear day as well as the 36 preceding scans lets us account for both truly static clutter (e.g. buildings) as well as clutter that is more dynamic such as sea and wind park clutter, without also requiring us to resort to filtering of dynamic clutter using a VRADH threshold. The quality of filtering is assessed visually. 2.5.1 Dynamic clutter We select 36 (3 hours worth of scans) preceding the start of the fireworks (23:00 UTC) and add an additional margin of 3 scans (15 minutes of scans) as that the VIR plots in the previous chapter have shown numbers of birds aloft are very low and stable up to that period. available_scans_hrw &lt;- Sys.glob(file.path(&quot;data/raw/pvol/clutter-removal-20171231&quot;, &quot;*NL62*20171231*&quot;)) available_scans_dhl &lt;- Sys.glob(file.path(&quot;data/raw/pvol/clutter-removal-20171231&quot;, &quot;*NL61*20171231*&quot;)) fw_start_hrw_pvol_path &lt;- &quot;data/raw/pvol/fireworks-2017-2018/RAD_NL62_VOL_NA_201712312300_ODIM.h5&quot; fw_start_dhl_pvol_path &lt;- &quot;data/raw/pvol/fireworks-2017-2018/RAD_NL61_VOL_NA_201712312300_ODIM.h5&quot; selected_scan_hrw &lt;- sub(&quot;fireworks-2017-2018&quot;, &quot;clutter-removal-20171231&quot;, fw_start_hrw_pvol_path) selected_scan_dhl &lt;- sub(&quot;fireworks-2017-2018&quot;, &quot;clutter-removal-20171231&quot;, fw_start_dhl_pvol_path) selected_scan_id_hrw &lt;- match(selected_scan_hrw, available_scans_hrw) selected_scan_id_dhl &lt;- match(selected_scan_dhl, available_scans_dhl) usable_scans_hrw &lt;- available_scans_hrw[(selected_scan_id_hrw-dynamic_time_margin-dynamic_nr_preceding_scans+1): (selected_scan_id_hrw-dynamic_time_margin)] usable_scans_dhl &lt;- available_scans_dhl[(selected_scan_id_dhl-dynamic_time_margin-dynamic_nr_preceding_scans+1): (selected_scan_id_dhl-dynamic_time_margin)] We can now loop over the files one by one and stack reflectivity data (DBZH) — after filtering out precipitation — in a multidimensional array. Note: the following code chunk will only run in full-reproduction mode as it takes quite a lot of time. Results are saved, so the next iteration this chunk can be skipped. source(&quot;R/stack_rainfree_reflectivities.R&quot;) stack_rainfree_reflectivities(usable_scans_hrw, outputfile = &quot;data/processed/clutter_dynamic_hrw.RDS&quot;) stack_rainfree_reflectivities(usable_scans_dhl, outputfile = &quot;data/processed/clutter_dynamic_dhl.RDS&quot;) With all DBZH compiled in a single multidimensional array, we can calculate mean reflectivity, which we store as DBZH_AVG in a pvol that now contains the dynamic clutter map. pvol_clutter_dynamic_hrw &lt;- readRDS(&quot;data/processed/clutter_dynamic_hrw.RDS&quot;) pvol_clutter_dynamic_dhl &lt;- readRDS(&quot;data/processed/clutter_dynamic_dhl.RDS&quot;) source(&quot;R/calculate_reflectivity_stack_mean.R&quot;) pvol_clutter_dynamic_hrw &lt;- calculate_reflectivity_stack_mean(pvol_clutter_dynamic_hrw, mds) pvol_clutter_dynamic_dhl &lt;- calculate_reflectivity_stack_mean(pvol_clutter_dynamic_dhl, mds) saveRDS(pvol_clutter_dynamic_hrw, &quot;data/processed/clutter_dynamic_hrw_avg.RDS&quot;) saveRDS(pvol_clutter_dynamic_dhl, &quot;data/processed/clutter_dynamic_dhl_avg.RDS&quot;) pvol_clutter_dynamic_hrw &lt;- readRDS(&quot;data/processed/clutter_dynamic_hrw_avg.RDS&quot;) pvol_clutter_dynamic_dhl &lt;- readRDS(&quot;data/processed/clutter_dynamic_dhl_avg.RDS&quot;) 2.5.1.1 Verify dynamic clutter map Let’s see what that looks like on a basemap, using a DBZH_AVG threshold of \\(-10dbZ\\), following (Dokter et al. 2011). scan_hrw &lt;- pvol_clutter_dynamic_hrw$scans[[1]] scan_dhl &lt;- pvol_clutter_dynamic_dhl$scans[[1]] side_by_side_ppi(pvol_clutter_dynamic_hrw, pvol_clutter_dynamic_dhl, &quot;Herwijnen dynamic clutter&quot;, &quot;Den Helder dynamic clutter&quot;, params = &quot;DBZH_AVG&quot;, range_max = 50000, scan_id = 1, basemap = TRUE, zlim = c(-11, -10)) Visually assessing this clutter map shows that it works quite well, selecting e.g. areas with wind parks, sea clutter, high buildings, industry, etc. Exactly what we hoped to achieve. 2.5.2 Static clutter Now, let’s retry exactly the same procedure, but this time selecting a day with no precipitation, which can be done using this tool by KNMI, so we can filter for truly static clutter. We select the following days: Herwijnen: December 29th, 2017 Den Helder: December 25th, 2017 Note: the following code chunk will only run in full-reproduction mode as it takes a lot of time to run. Results are saved, so the next iteration this chunk can be skipped. source(&quot;R/stack_rainfree_reflectivities.R&quot;) available_scans_hrw &lt;- Sys.glob(file.path(&quot;data/raw/pvol/clutter-removal-20171229-hrw&quot;, &quot;*NL62*20171229*&quot;)) available_scans_dhl &lt;- Sys.glob(file.path(&quot;data/raw/pvol/clutter-removal-20171225-dhl&quot;, &quot;*NL61*20171225*&quot;)) stack_rainfree_reflectivities(available_scans_hrw, outputfile = &quot;data/processed/clutter_static_hrw.RDS&quot;) stack_rainfree_reflectivities(available_scans_dhl, outputfile = &quot;data/processed/clutter_static_dhl.RDS&quot;) And we calculate mean DBZH values (DBZH_AVG). pvol_clutter_static_hrw &lt;- readRDS(&quot;data/processed/clutter_static_hrw.RDS&quot;) pvol_clutter_static_dhl &lt;- readRDS(&quot;data/processed/clutter_static_dhl.RDS&quot;) source(&quot;R/calculate_reflectivity_stack_mean.R&quot;) # Source because full_repro may be set to FALSE pvol_clutter_static_hrw &lt;- calculate_reflectivity_stack_mean(pvol_clutter_static_hrw, mds) pvol_clutter_static_dhl &lt;- calculate_reflectivity_stack_mean(pvol_clutter_static_dhl, mds) saveRDS(pvol_clutter_static_hrw, &quot;data/processed/clutter_static_hrw_avg.RDS&quot;) saveRDS(pvol_clutter_static_dhl, &quot;data/processed/clutter_static_dhl_avg.RDS&quot;) pvol_clutter_static_hrw &lt;- readRDS(&quot;data/processed/clutter_static_hrw_avg.RDS&quot;) pvol_clutter_static_dhl &lt;- readRDS(&quot;data/processed/clutter_static_dhl_avg.RDS&quot;) 2.5.2.1 Verify static clutter map Let’s see what that looks like on a basemap, using a DBZH_AVG threshold of \\(-10dbZ\\), following (Dokter et al. 2011). scan_hrw &lt;- pvol_clutter_static_hrw$scans[[1]] scan_dhl &lt;- pvol_clutter_static_dhl$scans[[1]] side_by_side_ppi(pvol_clutter_static_hrw, pvol_clutter_static_dhl, &quot;Herwijnen static clutter&quot;, &quot;Den Helder static clutter&quot;, params = &quot;DBZH_AVG&quot;, range_max = 50000, scan_id = 1, basemap = TRUE, zlim = c(-11, -10)) 2.5.3 Remove dynamic and static clutter Now that we have identified both dynamic and static clutter, we can create the final cleaned up polar volume. source(&quot;R/remove_groundclutter.R&quot;) pvol_hrw &lt;- remove_groundclutter(remove_groundclutter(pvol_hrw, pvol_clutter_dynamic_hrw), pvol_clutter_static_hrw) pvol_dhl &lt;- remove_groundclutter(remove_groundclutter(pvol_dhl, pvol_clutter_dynamic_dhl), pvol_clutter_static_dhl) saveRDS(pvol_hrw, file = &quot;data/processed/pvol_clean_hrw.RDS&quot;) saveRDS(pvol_dhl, file = &quot;data/processed/pvol_clean_dhl.RDS&quot;) 2.6 Range-bias correction With all identifiable sources of clutter removed from the raw polar volume, we can apply the range-bias correction (Kranstauber et al. 2020). For this it is necessary to calculate the local vertical profile for each of the radars. Ideally, this would be done using the filtered pvol we have now generated, but the vol2bird algorithm (Dokter et al. 2011) only takes pvol files as input, rather than R objects. As there is no implementation of a converter yet, for now a vp of the raw pvol files will have to do. As there is no precipitation within the relevant distance to the radars (5-35km), the calculated vp based on the raw pvol files should not differ wildly from that of the filtered pvol R object we have generated in the previous steps. For the Den Helder radar we calculate the vp by setting azimuthal limits to cover the mainland of North Holland, rather than the whole radar domain, as the latter will result in vps that underestimate the true density of birds aloft. See Appendix [generating-vps-for-den-helder-radar] for a more detailed explanation. vp_hrw &lt;- calculate_vp(file = pvol_hrw_path, vpfile = paste(&quot;data/processed/vp/&quot;, basename(pvol_hrw_path), sep = &quot;&quot;), verbose = FALSE) vp_dhl &lt;- calculate_vp(file = pvol_dhl_path, vpfile = paste(&quot;data/processed/vp/&quot;, basename(pvol_dhl_path), sep = &quot;&quot;), verbose = FALSE, azim_min = 90, azim_max = 200) corrected_ppi_hrw &lt;- integrate_to_ppi(pvol_hrw, vp_hrw, res = 500, xlim = c(-150000, 150000), ylim = c(-150000, 150000)) corrected_ppi_dhl &lt;- integrate_to_ppi(pvol_dhl, vp_dhl, res = 500, xlim = c(-150000, 150000), ylim = c(-150000, 150000)) saveRDS(corrected_ppi_hrw, file = &quot;data/processed/corrected_ppi_hrw.RDS&quot;) saveRDS(corrected_ppi_dhl, file = &quot;data/processed/corrected_ppi_dhl.RDS&quot;) We can now plot the final range-corrected PPIs. p_vir_hrw &lt;- plot(corrected_ppi_hrw, param = &quot;VIR&quot;, zlim = c(0, 20000)) + labs(title = &quot;Herwijnen: VIR&quot;) p_vir_dhl &lt;- plot(corrected_ppi_dhl, param = &quot;VIR&quot;, zlim = c(0, 20000)) + labs(title = &quot;Den Helder: VIR&quot;) ggarrange(p_vir_hrw, p_vir_dhl, ncol = 2, nrow = 1, common.legend = TRUE, legend = &quot;right&quot;) 2.7 Keep biology/meteorology classification By default the range-bias correction returns values from 0 upwards, so there is no way for us to distinguish anymore between ‘pixels’ that did not reflect and those that were filtered out as meteorology. That’s why we will additionally add the class parameter to the range-bias corrected PPIs. There’s a risk of misalignment of PPI pixels, but we can still use RBC if we trick it by replacing DBZH with the value of BIOLD (despeckled biology), which contains 0s for meteorology and 1s for biology and multiply this by a 1000 to get really high VIR values for biology and very low values for meteorology. pvol_hrw_classified &lt;- calculate_param(pvol_hrw, DBZH = BIOLD * 1000) pvol_dhl_classified &lt;- calculate_param(pvol_dhl, DBZH = BIOLD * 1000) pvol_hrw_classified &lt;- remove_groundclutter(remove_groundclutter(pvol_hrw_classified, pvol_clutter_dynamic_hrw), pvol_clutter_static_hrw) pvol_dhl_classified &lt;- remove_groundclutter(remove_groundclutter(pvol_dhl_classified, pvol_clutter_dynamic_dhl), pvol_clutter_static_dhl) corrected_ppi_hrw_classified &lt;- integrate_to_ppi(pvol_hrw_classified, vp_hrw, res = 500, xlim = c(-150000, 150000), ylim = c(-150000, 150000)) corrected_ppi_dhl_classified &lt;- integrate_to_ppi(pvol_dhl_classified, vp_dhl, res = 500, xlim = c(-150000, 150000), ylim = c(-150000, 150000)) p_vir_hrw_classified &lt;- plot(corrected_ppi_hrw_classified, param = &quot;VIR&quot;, zlim = c(0, 50000)) + labs(title = &quot;Herwijnen: VIR&quot;) p_vir_dhl_classified &lt;- plot(corrected_ppi_dhl_classified, param = &quot;VIR&quot;, zlim = c(0, 50000)) + labs(title = &quot;Den Helder: VIR&quot;) ggarrange(p_vir_hrw_classified, p_vir_dhl_classified, ncol = 2, nrow = 1, common.legend = TRUE, legend = &quot;right&quot;) That looks good, so now we add it to the corrected PPIs. We reclassify the VIR as follows: Biology gets class = 2, Meteorology gets class = 1, Background gets class = 0. corrected_ppi_hrw_classified$data@data %&gt;% mutate(class = case_when( VIR &gt; 40000 ~ 2, VIR &lt;= 40000 &amp; VIR &gt; 0 ~ 1, VIR == 0 ~ 0 )) %&gt;% dplyr::select(class) -&gt; class_hrw corrected_ppi_dhl_classified$data@data %&gt;% mutate(class = case_when( VIR &gt; 40000 ~ 2, VIR &lt;= 40000 &amp; VIR &gt; 0 ~ 1, VIR == 0 ~ 0 )) %&gt;% dplyr::select(class) -&gt; class_dhl corrected_ppi_hrw$data$class &lt;- unlist(class_hrw) corrected_ppi_dhl$data$class &lt;- unlist(class_dhl) 2.8 Distance to radar To assess the quality of the range-bias correction, it is also useful to calculate the distance to the radar from a given PPI pixel. Ideally, the range-bias correction would strongly reduce the effect of distance to the radar for measured densities of birds, but we still have to see if that is the case indeed. By including the distance to the radar in the PPIs, we can also include it in our modelling efforts later on. source(&quot;R/calculate_distance_to_radar.R&quot;) corrected_ppi_hrw &lt;- calculate_distance_to_radar(corrected_ppi_hrw) corrected_ppi_dhl &lt;- calculate_distance_to_radar(corrected_ppi_dhl) 2.9 Spatial coordinates Furthermore, it can be useful to keep spatial coordinates to correct for spatial autocorrelation should this be necessary, so we calculate these as well. coords_hrw &lt;- coordinates(corrected_ppi_hrw$data) corrected_ppi_hrw$data$x &lt;- coords_hrw[, 1] corrected_ppi_hrw$data$y &lt;- coords_hrw[, 2] coords_dhl &lt;- coordinates(corrected_ppi_dhl$data) corrected_ppi_dhl$data$x &lt;- coords_dhl[, 1] corrected_ppi_dhl$data$y &lt;- coords_dhl[, 2] saveRDS(corrected_ppi_hrw, file = &quot;data/processed/corrected_ppi_hrw.RDS&quot;) saveRDS(corrected_ppi_dhl, file = &quot;data/processed/corrected_ppi_dhl.RDS&quot;) 2.10 Visualising range-bias correction Now we can finally have a proper look at the range-corrected versions of the PPI overlaid on an interactive map. It’s a little hard to interpret with the PPI pixels that contain no birds set to 0 and no landscape below, so let’s see what it looks like on a basemap with those removed. As there are some ships out at the North Sea causing reflectivities orders of magnitude higher and thus stretching the colormap, we — for now — need to ‘clip’ these values to a maximum set at the value of the 99th percentile. source(&quot;R/clip.R&quot;) filtered_corrected_ppi_hrw &lt;- corrected_ppi_hrw filtered_corrected_ppi_hrw$data@data &lt;- as.data.frame(apply(filtered_corrected_ppi_hrw$data@data, 2, clip, bounds = c(0.05, 0.99))) filtered_corrected_ppi_dhl &lt;- corrected_ppi_dhl filtered_corrected_ppi_dhl$data@data &lt;- as.data.frame(apply(filtered_corrected_ppi_dhl$data@data, 2, clip, bounds = c(0.05, 0.99))) 2.10.1 Range-corrected PPI of Herwijnen radar (???): Somehow now broken after adding biology classification mapview(corrected_ppi_hrw$data) 2.10.2 Range-corrected PPI of Den Helder radar mapview(corrected_ppi_dhl$data) 2.11 Preprocess additional scans Now that the processing steps have been explained in detail, we can now process the additional scans in our dataset in a similar fashion. For that we will focus on all scans from 22:00 CET until 01:00 CET, which corresponds with 23:00 until 02:00 in local Amsterdam time, as that should ‘cover’ most of the fireworks disturbance that occurs (based on the plots in Selecting firework take-off moment). source(&quot;R/preprocess_radar_data.R&quot;) hrw_pvols &lt;- Sys.glob(file.path(&quot;data/raw/pvol/fireworks-2017-2018&quot;, &quot;*NL62*&quot;)) dhl_pvols &lt;- Sys.glob(file.path(&quot;data/raw/pvol/fireworks-2017-2018&quot;, &quot;*NL61*&quot;)) dynamic_groundclutter_hrw &lt;- readRDS(&quot;data/processed/clutter_dynamic_hrw_avg.RDS&quot;) static_groundclutter_hrw &lt;- readRDS(&quot;data/processed/clutter_static_hrw_avg.RDS&quot;) dynamic_groundclutter_dhl &lt;- readRDS(&quot;data/processed/clutter_dynamic_dhl_avg.RDS&quot;) static_groundclutter_dhl &lt;- readRDS(&quot;data/processed/clutter_static_dhl_avg.RDS&quot;) for (pvol in hrw_pvols) { preprocess_radar_data(pvol_path = pvol, ei_rays = ei_rays_hrw, pvol_dynamic_groundclutter = dynamic_groundclutter_hrw, pvol_static_groundclutter = static_groundclutter_hrw, res = 500) } for (pvol in dhl_pvols) { preprocess_radar_data(pvol_path = pvol, ei_rays = ei_rays_dhl, pvol_dynamic_groundclutter = dynamic_groundclutter_dhl, pvol_static_groundclutter = static_groundclutter_dhl, azim_limits = c(90, 200), res = 500) } References "],
["03.Annotate-land-use.html", "3 Annotating land use 3.1 Setting up the annotation environment 3.2 Converting the land use map 3.3 Adding land use classifications to the PPIs 3.4 Calculate distance to nearest urban area 3.5 Add population density", " 3 Annotating land use In this study we aim to quantify the response to fireworks across different species groups, for which take-off habitat is probably a good indicator. In this notebook we will classify land use, and a variety of factors that can influence the ‘intensity’ of fireworks disturbance, such as the distance to the nearest urbanised area for each of the PPI ‘pixels’. The land use is based on the CORINE Land Cover dataset and specifically the 2018 version (CLC2018), which should be most relevant for the 2017-2018 fireworks event. 3.1 Setting up the annotation environment library(raster) library(sf) library(stars) library(dplyr) library(ggplot2) library(ggpubr) library(gridExtra) library(viridis) library(mapview) 3.2 Converting the land use map To start, we need to convert the land use map to the same 1) resolution, and 2) extent of the radar PPIs as we can then simply ‘overlay’ both rasters on top of each other and do calculations. We load the PPIs and extract the CRS information contained in the proj4 strings. ppi_hrw &lt;- readRDS(&quot;data/processed/corrected_ppi_hrw.RDS&quot;) ppi_dhl &lt;- readRDS(&quot;data/processed/corrected_ppi_dhl.RDS&quot;) ppi_proj4_hrw &lt;- ppi_hrw$data@proj4string ppi_proj4_dhl &lt;- ppi_dhl$data@proj4string And we load and prepare the land use map it’s all about. To aid the classification process, we also load the land use classes contained in the entire CLC2018 dataset, otherwise the classes will remain anonymous numbers. landuse &lt;- raster(&quot;data/raw/landuse/clc2018_clc2018_v2018_20_raster100m/CLC2018_CLC2018_V2018_20.tif&quot;) landuse_classes &lt;- read.csv(&quot;data/raw/landuse/clc2018_clc2018_v2018_20_raster100m/Legend/CLC2018_CLC2018_V2018_20_QGIS.txt&quot;, col.names = c(&quot;landuse_id&quot;, &quot;r&quot;, &quot;g&quot;, &quot;b&quot;, &quot;x&quot;, &quot;landuse_class&quot;), header = FALSE)[, c(&quot;landuse_id&quot;, &quot;landuse_class&quot;),] 3.2.1 Cropping the land use map As the CLC2018 dataset is so large it does not fit in memory at all in the steps below, so we have to crop the raster dataset for further processing. Even then, it still requires a beefy computer to process these files. First we calculate a bounding box for the landuse raster based on the bounding boxes of the radar data. padding &lt;- 25000 # Padding in m to make sure we crop out of the land use map with a wide margin to compensate for edge-effects later bbox_meters &lt;- abs(ppi_dhl$data@bbox[[1]]) + padding # Assuming the PPI range of DHL and HRW are the same bbox_hrw &lt;- st_bbox(c(xmin = -bbox_meters, ymin = -bbox_meters, xmax = bbox_meters, ymax = bbox_meters), crs = ppi_proj4_hrw) bbox_dhl &lt;- st_bbox(c(xmin = -bbox_meters, ymin = -bbox_meters, xmax = bbox_meters, ymax = bbox_meters), crs = ppi_proj4_dhl) bbox_hrw %&gt;% st_as_sfc() %&gt;% st_transform(crs(landuse)) %&gt;% st_bbox -&gt; bbox_landuse_hrw bbox_dhl %&gt;% st_as_sfc() %&gt;% st_transform(crs(landuse)) %&gt;% st_bbox -&gt; bbox_landuse_dhl We can now crop and plot the land use maps centered on the radar sites, with a 2.510^{4} meter padding surrounding the extent of the radar data. ext_hrw &lt;- extent(c(bbox_landuse_hrw[1], bbox_landuse_hrw[3], bbox_landuse_hrw[2], bbox_landuse_hrw[4])) ext_dhl &lt;- extent(c(bbox_landuse_dhl[1], bbox_landuse_dhl[3], bbox_landuse_dhl[2], bbox_landuse_dhl[4])) landuse_crop_hrw &lt;- crop(landuse, ext_hrw) landuse_crop_dhl &lt;- crop(landuse, ext_dhl) sea_id &lt;- match(&#39;Sea and ocean&#39;, landuse_classes$landuse_class) landuse_crop_hrw[is.na(landuse_crop_hrw)] &lt;- landuse_classes[sea_id, &quot;landuse_id&quot;] # Convert Sea that is set to NaN landuse_crop_dhl[is.na(landuse_crop_dhl)] &lt;- landuse_classes[sea_id, &quot;landuse_id&quot;] And plot the result: par(pty = &quot;s&quot;, mfrow = c(1, 2)) image(landuse_crop_hrw, main = &quot;Herwijnen&quot;) image(landuse_crop_dhl, main = &quot;Den Helder&quot;) 3.2.2 Reprojecting the land use map Now that the land use map is cropped, we can start the reprojection to the CRS of the radar PPI. As we’re dealing with categorical data, we set method = &quot;ngb&quot; to use nearest neighbour interpolation. Despite using interpolation, this reprojection does not change the resolution from the base of the CLC2018 dataset to that of the PPI, so we’ll have to do that next. landuse_hrw_reprojected &lt;- projectRaster(landuse_crop_hrw, crs = ppi_proj4_hrw, method = &quot;ngb&quot;) landuse_dhl_reprojected &lt;- projectRaster(landuse_crop_dhl, crs = ppi_proj4_dhl, method = &quot;ngb&quot;) levels(landuse_hrw_reprojected) &lt;- levels(landuse_crop_hrw) levels(landuse_dhl_reprojected) &lt;- levels(landuse_crop_dhl) If the reprojection went successful, the CRS of the reprojected land use map and the radar PPI should be the same. compareCRS(ppi_hrw$data@proj4string, landuse_hrw_reprojected@crs) compareCRS(ppi_dhl$data@proj4string, landuse_dhl_reprojected@crs) ## [1] TRUE ## [1] TRUE Apparently that is the case. 3.2.3 Reclassifying land use types to functional classes The CLC2018 dataset contains a total of 44 land use classes. For our purpose, we reduce these 44 to 5 classes with more biologically relevant groupings, specifically: Urban area Agricultural area Semi-open area Forests Wetlands Water bodies The following table is used to convert/reclassify the classes contained within the CLC2018 dataset, with the original land use classes under landuse_class and how they will be reclassified under landuse_target. We also indicate whether these areas are inhabited (inhabited = 1), or uninhabited (inhabited = 0). landuse_classes %&lt;&gt;% mutate(landuse_target = case_when( landuse_id &gt;= 100 &amp; landuse_id &lt; 200 ~ &quot;Urban area&quot;, landuse_id &gt;= 200 &amp; landuse_id &lt;= 213 ~ &quot;Agricultural area&quot;, landuse_id &gt;= 221 &amp; landuse_id &lt;= 223 ~ &quot;Semi-open area&quot;, landuse_id &gt;= 231 &amp; landuse_id &lt;= 243 ~ &quot;Agricultural area&quot;, landuse_id &gt;= 244 &amp; landuse_id &lt; 300 ~ &quot;Forests&quot;, landuse_id &gt;= 300 &amp; landuse_id &lt;= 313 ~ &quot;Forests&quot;, landuse_id &gt;= 321 &amp; landuse_id &lt;= 335 ~ &quot;Semi-open area&quot;, landuse_id &gt;= 400 &amp; landuse_id &lt; 500 ~ &quot;Wetlands&quot;, landuse_id &gt;= 500 &amp; landuse_id &lt; 999 ~ &quot;Water bodies&quot;, landuse_id == 999 ~ &quot;NODATA&quot;)) %&gt;% mutate(landuse_target_id = case_when( landuse_target == &quot;Urban area&quot; ~ 1, landuse_target == &quot;Agricultural area&quot; ~ 2, landuse_target == &quot;Semi-open area&quot; ~ 3, landuse_target == &quot;Forests&quot; ~ 4, landuse_target == &quot;Wetlands&quot; ~ 5, landuse_target == &quot;Water bodies&quot; ~ 6, landuse_target == &quot;NODATA&quot; ~ 9 )) landuse_classes$inhabited &lt;- 0 landuse_classes$inhabited[landuse_classes$landuse_class == &quot;Discontinuous urban fabric&quot;] &lt;- 1 landuse_classes landuse_id landuse_class landuse_target landuse_target_id inhabited 111 Continuous urban fabric Urban area 1 0 112 Discontinuous urban fabric Urban area 1 1 121 Industrial or commercial units Urban area 1 0 122 Road and rail networks and associated land Urban area 1 0 123 Port areas Urban area 1 0 124 Airports Urban area 1 0 131 Mineral extraction sites Urban area 1 0 132 Dump sites Urban area 1 0 133 Construction sites Urban area 1 0 141 Green urban areas Urban area 1 0 142 Sport and leisure facilities Urban area 1 0 211 Non-irrigated arable land Agricultural area 2 0 212 Permanently irrigated land Agricultural area 2 0 213 Rice fields Agricultural area 2 0 221 Vineyards Semi-open area 3 0 222 Fruit trees and berry plantations Semi-open area 3 0 223 Olive groves Semi-open area 3 0 231 Pastures Agricultural area 2 0 241 Annual crops associated with permanent crops Agricultural area 2 0 242 Complex cultivation patterns Agricultural area 2 0 243 Land principally occupied by agriculture with significant areas of natural vegetation Agricultural area 2 0 244 Agro-forestry areas Forests 4 0 311 Broad-leaved forest Forests 4 0 312 Coniferous forest Forests 4 0 313 Mixed forest Forests 4 0 321 Natural grasslands Semi-open area 3 0 322 Moors and heathland Semi-open area 3 0 323 Sclerophyllous vegetation Semi-open area 3 0 324 Transitional woodland-shrub Semi-open area 3 0 331 Beaches dunes sands Semi-open area 3 0 332 Bare rocks Semi-open area 3 0 333 Sparsely vegetated areas Semi-open area 3 0 334 Burnt areas Semi-open area 3 0 335 Glaciers and perpetual snow Semi-open area 3 0 411 Inland marshes Wetlands 5 0 412 Peat bogs Wetlands 5 0 421 Salt marshes Wetlands 5 0 422 Salines Wetlands 5 0 423 Intertidal flats Wetlands 5 0 511 Water courses Water bodies 6 0 512 Water bodies Water bodies 6 0 521 Coastal lagoons Water bodies 6 0 522 Estuaries Water bodies 6 0 523 Sea and ocean Water bodies 6 0 999 NODATA NODATA 9 0 With a sort-of ‘raster attribute table’ in place, we can now reclassify the detailed landuse classes to the broader categories listed above. landuse_dhl_reclassified &lt;- ratify(reclassify(landuse_dhl_reprojected, cbind(landuse_classes$landuse_id, landuse_classes$landuse_target_id)), count = TRUE) landuse_hrw_reclassified &lt;- ratify(reclassify(landuse_hrw_reprojected, cbind(landuse_classes$landuse_id, landuse_classes$landuse_target_id)), count = TRUE) This leaves us with the following count of ~100x100m cells per land use category cellcounts &lt;- cbind(levels(landuse_hrw_reclassified)[[1]][&quot;COUNT&quot;], levels(landuse_dhl_reclassified)[[1]][&quot;COUNT&quot;]) colnames(cellcounts) &lt;- c(&quot;Herwijnen&quot;, &quot;Den Helder&quot;) rownames(cellcounts) &lt;- unique(landuse_classes$landuse_target)[-7] cellcounts Herwijnen Den Helder Urban area 1860678 809932 Agricultural area 6256242 3384907 Semi-open area 194271 133709 Forests 1437047 498510 Wetlands 346616 364301 Water bodies 3851726 8890353 3.2.4 Resampling the land use map to a lower resolution The cellsize of the PPIs is 500, 500x500, 500 meters, but the land use map is much more finely detailed (~100x100m), so we need to resample the latter to derive a land use map at a 500, 500x500, 500 meter resolution as well. As the resolution of the PPIs is so much higher than that of the land use map, we need to resample the latter to a lower resolution. Instead of classifying a single pixel in the PPI as belonging to a single land use class, we will do so using land use proportions. We therefore calculate the proportions belonging to each of the land use classes within an area of cells roughly the size of the PPI pixels. Subsequently we resample this to match 1:1 with the PPI pixels and store the land use proportions for each of the land use classes in the PPIs. This is done in the calculate_land_use_proportion() function below. calculate_land_use_proportion &lt;- function(raster, reference_raster, overwrite = FALSE) { values &lt;- c(sort(unique(getValues(raster)))) # classes: multidimensional logical array for the classes contained within the land use map # 1 if a land use class is present on that position, 0 if not classes &lt;- layerize(raster, filename = paste(&quot;data/processed/landuse/layerize/&quot;, substitute(raster), sep = &quot;&quot;), format = &quot;raster&quot;, bylayer = TRUE, classes = values, overwrite = overwrite) # factor: nr of cells in both horizontal and vertical direction to aggregate factor &lt;- round(dim(raster)[1:2] / dim(reference_raster)[1:2]) # agg: aggregated version of classes (aggregation factor defined by factor) containing mean coverage by a class in a given area # 1 corresponds with full coverage, 0 with no coverage of that class within the pixel at all agg &lt;- aggregate(classes, factor, na.rm = TRUE, fun = mean) # x: the agg and ppi pixels almost overlap exactly, but there is a teeny tiny difference which we can # iron out by resampling once more. x &lt;- resample(agg, reference_raster) return(x) } We can now calculate the proportions and will save these to some raster files for potential inspection in GIS software. landuse_hrw &lt;- calculate_land_use_proportion(landuse_hrw_reclassified, as(ppi_hrw$data, &quot;RasterLayer&quot;), overwrite = TRUE) landuse_dhl &lt;- calculate_land_use_proportion(landuse_dhl_reclassified, as(ppi_dhl$data, &quot;RasterLayer&quot;), overwrite = TRUE) names(landuse_hrw) &lt;- c(&quot;urban&quot;, &quot;agricultural&quot;, &quot;semiopen&quot;, &quot;forests&quot;, &quot;wetlands&quot;, &quot;waterbodies&quot;) names(landuse_dhl) &lt;- c(&quot;urban&quot;, &quot;agricultural&quot;, &quot;semiopen&quot;, &quot;forests&quot;, &quot;wetlands&quot;, &quot;waterbodies&quot;) writeRaster(landuse_hrw, &quot;data/processed/landuse/landuse_hrw.tif&quot;, overwrite = TRUE) writeRaster(landuse_dhl, &quot;data/processed/landuse/landuse_dhl.tif&quot;, overwrite = TRUE) By now the resampled land use raster should be very similar to the PPI raster, with the exception of — of course — the values contained within. compareRaster(landuse_hrw, as(ppi_hrw$data, &quot;RasterLayer&quot;), extent = TRUE, rowcol = TRUE, crs = TRUE, res = TRUE, orig = TRUE) compareRaster(landuse_dhl, as(ppi_dhl$data, &quot;RasterLayer&quot;), extent = TRUE, rowcol = TRUE, crs = TRUE, res = TRUE, orig = TRUE) ## [1] TRUE ## [1] TRUE Ok, let’s save a copy of what we have so far. saveRDS(landuse_hrw, &quot;data/processed/landuse/landuse_hrw.RDS&quot;) saveRDS(landuse_dhl, &quot;data/processed/landuse/landuse_dhl.RDS&quot;) 3.3 Adding land use classifications to the PPIs With the land use rasters overlapping exactly with the PPIs, we can simply extract the values of the resampled land use rasters and add these as additional parameters to the PPIs. landuse_hrw &lt;- readRDS(&quot;data/processed/landuse/landuse_hrw.RDS&quot;) landuse_dhl &lt;- readRDS(&quot;data/processed/landuse/landuse_dhl.RDS&quot;) values_hrw &lt;- rasterToPoints(landuse_hrw, spatial = TRUE) values_dhl &lt;- rasterToPoints(landuse_dhl, spatial = TRUE) ppi_hrw$data@data &lt;- cbind(ppi_hrw$data@data, values_hrw@data) ppi_dhl$data@data &lt;- cbind(ppi_dhl$data@data, values_dhl@data) 3.4 Calculate distance to nearest urban area We can use the distance to the nearest inhabited urban area as a proxy for disturbance. To calculate this, we reclassify the raster to cells containing inhabited urban area and everything else. For every cell on the raster that is not a cell we have just classified as inhabited urban area, we will calculate the distance (in meters) to the nearest cell classified as inhabited urban area. dhl_inhabited &lt;- ratify(reclassify(landuse_dhl_reprojected, cbind(landuse_classes$landuse_id, landuse_classes$inhabited)), count = TRUE) hrw_inhabited &lt;- ratify(reclassify(landuse_hrw_reprojected, cbind(landuse_classes$landuse_id, landuse_classes$inhabited)), count = TRUE) dhl_inhabited &lt;- calculate_land_use_proportion(dhl_inhabited, as(ppi_dhl$data, &quot;RasterLayer&quot;), overwrite = TRUE) hrw_inhabited &lt;- calculate_land_use_proportion(hrw_inhabited, as(ppi_hrw$data, &quot;RasterLayer&quot;), overwrite = TRUE) names(dhl_inhabited) &lt;- c(&quot;uninhabited&quot;, &quot;inhabited&quot;) names(hrw_inhabited) &lt;- c(&quot;uninhabited&quot;, &quot;inhabited&quot;) dist_dhl &lt;- dhl_inhabited dist_dhl[dist_dhl$inhabited &lt; 0.5] &lt;- NA # Set to NA if probability of inhabited area is &lt; 0.5 dist_dhl &lt;- distance(dist_dhl$inhabited) dist_hrw &lt;- hrw_inhabited dist_hrw[dist_hrw$inhabited &lt; 0.5] &lt;- NA dist_hrw &lt;- distance(dist_hrw$inhabited) writeRaster(dist_hrw, &quot;data/processed/landuse/dist_urban_hrw.tif&quot;, overwrite = TRUE) writeRaster(dist_dhl, &quot;data/processed/landuse/dist_urban_dhl.tif&quot;, overwrite = TRUE) And we add these values to the PPIs again. values_dist_hrw &lt;- rasterToPoints(dist_hrw, spatial = TRUE) values_dist_dhl &lt;- rasterToPoints(dist_dhl, spatial = TRUE) ppi_hrw$data@data$dist_urban &lt;- values_dist_hrw@data$layer ppi_dhl$data@data$dist_urban &lt;- values_dist_dhl@data$layer 3.5 Add population density Another proxy for disturbance is simply the number of humans living in a certain area. The Dutch Central Bureau of Statistics (CBS) annually publishes a dataset containing the number of inhabitants organized in 500x500m grid cells. We will now add this to the PPIs as well. cbs_maps &lt;- st_read(&quot;data/raw/population-density/2019-CBS_VK500_2018_v1/CBS_VK500_2018_v1.shp&quot;) ## Reading layer `CBS_VK500_2018_v1&#39; from data source `/mnt/volume_ams3_01/raw/population-density/2019-CBS_VK500_2018_v1/CBS_VK500_2018_v1.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 151108 features and 31 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 13000 ymin: 306500 xmax: 278500 ymax: 619500 ## proj4string: +proj=sterea +lat_0=52.15616055555555 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=463000 +ellps=bessel +units=m +no_defs cbs_hrw &lt;- st_transform(cbs_maps, ppi_hrw$data@proj4string) cbs_dhl &lt;- st_transform(cbs_maps, ppi_dhl$data@proj4string) # Template for the rasterization following the standard 500x500m resolution of the CBS grid template_hrw &lt;- st_as_stars(st_bbox(cbs_hrw[&quot;INWONER&quot;]), values = NA_real_, dx = 500, dy = 500) template_dhl &lt;- st_as_stars(st_bbox(cbs_dhl[&quot;INWONER&quot;]), values = NA_real_, dx = 500, dy = 500) # Now rasterize pop_density_rasterized_hrw &lt;- as(st_rasterize(cbs_hrw[&quot;INWONER&quot;], template = template_hrw), &quot;Raster&quot;) pop_density_rasterized_dhl &lt;- as(st_rasterize(cbs_dhl[&quot;INWONER&quot;], template = template_dhl), &quot;Raster&quot;) # Set negative or NA raster values to 0 pop_density_rasterized_hrw[pop_density_rasterized_hrw &lt; 0] &lt;- 0 pop_density_rasterized_dhl[pop_density_rasterized_dhl &lt; 0] &lt;- 0 # Now aggregate by summing up values in cells to &#39;cover&#39; the values that fit in a PPI pixel factor_hrw &lt;- round(dim(pop_density_rasterized_hrw)[1:2] / dim(as(ppi_hrw$data, &quot;RasterLayer&quot;))[1:2]) factor_dhl &lt;- round(dim(pop_density_rasterized_dhl)[1:2] / dim(as(ppi_dhl$data, &quot;RasterLayer&quot;))[1:2]) agg_hrw &lt;- aggregate(pop_density_rasterized_hrw, factor_hrw, na.rm = TRUE, fun = sum) ## Warning in .local(x, ...): all fact(s) were 1, nothing to aggregate agg_dhl &lt;- aggregate(pop_density_rasterized_dhl, factor_dhl, na.rm = TRUE, fun = sum) ## Warning in .local(x, ...): all fact(s) were 1, nothing to aggregate # Resample to make the PPI and CBS population grids line up 1:1 pop_density_rasterized_hrw &lt;- resample(agg_hrw, as(ppi_hrw$data, &quot;RasterLayer&quot;)) pop_density_rasterized_dhl &lt;- resample(agg_dhl, as(ppi_dhl$data, &quot;RasterLayer&quot;)) Once again, verify if the PPI pixels match up exactly with the CBS population grids compareRaster(pop_density_rasterized_hrw, as(ppi_hrw$data, &quot;RasterLayer&quot;), extent = TRUE, rowcol = TRUE, crs = TRUE, res = TRUE, orig = TRUE) compareRaster(pop_density_rasterized_dhl, as(ppi_dhl$data, &quot;RasterLayer&quot;), extent = TRUE, rowcol = TRUE, crs = TRUE, res = TRUE, orig = TRUE) ## [1] TRUE ## [1] TRUE Now as a final step we will calculate the total population within the neighborhood surrounding the PPI pixels, to get a more representative measure of disturbance potential in the surrounding area. weights &lt;- matrix(1, nrow = 3, ncol = 3) pop_hrw &lt;- focal(pop_density_rasterized_hrw, w = weights, fun = sum, na.rm = TRUE) pop_dhl &lt;- focal(pop_density_rasterized_dhl, w = weights, fun = sum, na.rm = TRUE) pop_hrw[is.na(pop_hrw)] &lt;- 0 pop_dhl[is.na(pop_dhl)] &lt;- 0 And add it to the PPIs. values_pop_hrw &lt;- rasterToPoints(pop_hrw, spatial = TRUE) values_pop_dhl &lt;- rasterToPoints(pop_dhl, spatial = TRUE) ppi_hrw$data@data$human_pop &lt;- values_pop_hrw@data$layer ppi_dhl$data@data$human_pop &lt;- values_pop_dhl@data$layer And finally we save these PPIs again. saveRDS(ppi_hrw, file = &quot;data/processed/corrected_ppi_hrw_lu.RDS&quot;) saveRDS(ppi_dhl, file = &quot;data/processed/corrected_ppi_dhl_lu.RDS&quot;) 3.5.1 Plotting human parameters on an interactive map We have now generated ‘human relevant’ parameters for later modelling stages. Let’s plot them on an interactive map again for reference. 3.5.1.1 Interactive map of Herwijnen human_parameters &lt;- c(&quot;urban&quot;, &quot;agricultural&quot;, &quot;semiopen&quot;, &quot;forests&quot;, &quot;wetlands&quot;, &quot;waterbodies&quot;, &quot;dist_urban&quot;, &quot;human_pop&quot;) mapview(ppi_hrw$data[, , human_parameters], alpha.regions = 0.6, col.regions = inferno, maxpixels=2000000, na.color = &quot;#00000000&quot;, map.types = c(&quot;CartoDB.Positron&quot;, &quot;CartoDB.DarkMatter&quot;, &quot;Esri.WorldImagery&quot;), layer.name = human_parameters) 3.5.1.2 Interactive map of Den Helder mapview(ppi_dhl$data[, , human_parameters], alpha.regions = 0.6, col.regions = inferno, maxpixels=2000000, na.color = &quot;#00000000&quot;, map.types = c(&quot;CartoDB.Positron&quot;, &quot;CartoDB.DarkMatter&quot;, &quot;Esri.WorldImagery&quot;), layer.name = human_parameters) "],
["04.Annotate-count-areas.html", "4 Annotating count areas 4.1 Setting up the annotation environment 4.2 Annotate PPIs with waterbird area codes 4.3 Annotate PPIs with point-transect-counts", " 4 Annotating count areas We have data for the following counts provided by Sovon: Waterbird counts with a shapefile containing the surveyed areas and an xlsx file with the count results. Both can be linked using the GEBIEDID contained in both datasets. PTT counts, or point transect counts, contained within an xlsx file with the routes and all bird observations at an \\((X, Y)\\) location. To make processing efficient, we will ‘annotate’ the PPIs with the corresponding area codes for the waterbird counts and some identifier for the PTT counts. Doing so, we can later on calculate relevant count-based parameters (e.g. numbers of birds, average mass, etc.) and ‘join’ these by the corresponding identifiers. 4.1 Setting up the annotation environment library(bioRad) library(sf) library(stars) library(raster) library(dplyr) library(tidyr) library(readr) library(stringr) library(readxl) library(ggplot2) library(viridis) library(fasterize) ppi_hrw &lt;- readRDS(&quot;data/processed/corrected_ppi_hrw_lu.RDS&quot;) ppi_dhl &lt;- readRDS(&quot;data/processed/corrected_ppi_dhl_lu.RDS&quot;) 4.2 Annotate PPIs with waterbird area codes We rename the veriables retained in the shapefile to English and add a numerical wb_area_id which we can use to link the information retained in the shapefiles with the rasterized waterbird areas. All shapefiles are transformed to the CRS of the PPIs. wb_areas &lt;- st_read(&quot;data/raw/sovon/wavo_telgebieden.shp&quot;) %&gt;% rename(wb_area_nr = GEBIEDNR, wb_area_ha = OPPHA, xcoor = XCOOR, ycoor = YCOOR) ## Reading layer `wavo_telgebieden&#39; from data source `/mnt/volume_ams3_01/raw/sovon/wavo_telgebieden.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 4131 features and 4 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 13551.48 ymin: 307546.8 xmax: 278027 ymax: 622790 ## proj4string: +proj=sterea +lat_0=52.15616055555555 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=463000 +ellps=bessel +units=m +no_defs wb_areas$wb_area_id &lt;- seq(1, length(wb_areas$wb_area_nr)) wb_areas_hrw &lt;- st_transform(wb_areas, ppi_hrw$data@proj4string) wb_areas_dhl &lt;- st_transform(wb_areas, ppi_dhl$data@proj4string) We rasterize specifically the newly created wb_area_id (as this is a numerical and not categorical value like wb_area_nr) following the ‘template’ of the existing PPIs. tpl_hrw &lt;- st_as_stars(st_bbox(ppi_hrw$data), dx = ppi_hrw$data@grid@cellsize[1], dy = ppi_hrw$data@grid@cellsize[2], values = NA_real_) tpl_dhl &lt;- st_as_stars(st_bbox(ppi_dhl$data), dx = ppi_dhl$data@grid@cellsize[1], dy = ppi_dhl$data@grid@cellsize[2], values = NA_real_) wb_areas_rasterized_hrw &lt;- st_rasterize(wb_areas_hrw[&quot;wb_area_id&quot;], template = tpl_hrw) wb_areas_rasterized_dhl &lt;- st_rasterize(wb_areas_dhl[&quot;wb_area_id&quot;], template = tpl_dhl) Let’s see how that’s gone so far. par(pty = &quot;s&quot;, mfrow = c(1, 2)) plot(wb_areas_rasterized_hrw, main = &quot;Herwijnen: wb_area_id&quot;) plot(wb_areas_rasterized_dhl, main = &quot;Den Helder: wb_area_id&quot;) Visually that seems to have gone well, now let’s make sure the rasterized waterbird areas share the same features as the PPI ‘rasters’. compareRaster(as(wb_areas_rasterized_hrw, &quot;Raster&quot;), as(ppi_hrw$data, &quot;RasterLayer&quot;), extent = TRUE, rowcol = TRUE, crs = TRUE, res = TRUE, orig = TRUE) compareRaster(as(wb_areas_rasterized_dhl, &quot;Raster&quot;), as(ppi_dhl$data, &quot;RasterLayer&quot;), extent = TRUE, rowcol = TRUE, crs = TRUE, res = TRUE, orig = TRUE) ## [1] TRUE ## [1] TRUE Twice a TRUE, so the rasters are identical (except for the values), so we can merge the datasets using a join on the wb_area_id. # Add the wb_area_id to the PPIs ppi_hrw$data$wb_area_id &lt;- unlist(as.data.frame(as(wb_areas_rasterized_hrw, &quot;Raster&quot;))) ppi_dhl$data$wb_area_id &lt;- unlist(as.data.frame(as(wb_areas_rasterized_dhl, &quot;Raster&quot;))) # Join the additional contents of the shapefiles ppi_hrw$data@data %&gt;% left_join(dplyr::select(as.data.frame(wb_areas_hrw), wb_area_id, wb_area_nr, wb_area_ha), by = c(&quot;wb_area_id&quot; = &quot;wb_area_id&quot;)) -&gt; ppi_hrw$data@data ppi_hrw$data$geometry &lt;- NULL ppi_dhl$data@data %&gt;% left_join(dplyr::select(as.data.frame(wb_areas_dhl), wb_area_id, wb_area_nr, wb_area_ha), by = c(&quot;wb_area_id&quot; = &quot;wb_area_id&quot;)) -&gt; ppi_dhl$data@data ppi_dhl$data$geometry &lt;- NULL Let’s verify if that occurred as planned. plot(ppi_hrw, param = &quot;wb_area_id&quot;, zlim = c(min(ppi_hrw$data@data$wb_area_id, na.rm = TRUE), max(ppi_hrw$data@data$wb_area_id, na.rm = TRUE))) plot(ppi_dhl, param = &quot;wb_area_id&quot;, zlim = c(min(ppi_dhl$data@data$wb_area_id, na.rm = TRUE), max(ppi_dhl$data@data$wb_area_id, na.rm = TRUE))) This looks very comparable to the plots of the rasterized scans above and wb_area_id shows similar areas in similar colors, so this worked fine. 4.3 Annotate PPIs with point-transect-counts The PTT point transect counts are organized by routes, which consist of a few points. We can follow the same approach as above with the waterbird counts, by creating coverage shapes (like the shapefile features for the waterbird areas) for each of the routes. Using the convex hull of the points within a route seems like a good starting point to convert these points to areas. However, in that case it would appear as if birds are only counted when looking ‘inwards’ to this shape. By buffering these convex hulls with a radius of the average distance between successive points, a more representative coverage area can be generated. 4.3.1 Loading PTT points We load the PTT data directly from the xlsx file provided by Sovon and rename the variables to English. ptt &lt;- read_excel(&quot;data/raw/sovon/tel_dec_jan_1718.xlsx&quot;, sheet = &quot;ptt&quot;) %&gt;% rename(count_id = tellingid, route = route, count_point = telpunt, season = seizoen, year = teljaar, month = maand, day = dag, species = soort, number = aantal) head(ptt, 10) count_id route count_point season year month day euring species number xcoor ycoor 80917 4 1 2017 2017 12 23 720 Aalscholver 2 246342 520763 80917 4 1 2017 2017 12 23 5920 Zilvermeeuw 18 246342 520763 80917 4 1 2017 2017 12 23 6700 Houtduif 1 246342 520763 80917 4 1 2017 2017 12 23 11870 Merel 4 246342 520763 80917 4 1 2017 2017 12 23 15630 Roek 24 246342 520763 80917 4 2 2017 2017 12 23 6700 Houtduif 1 246357 522178 80917 4 2 2017 2017 12 23 11870 Merel 1 246357 522178 80917 4 2 2017 2017 12 23 15600 Kauw 6 246357 522178 80917 4 2 2017 2017 12 23 15630 Roek 78 246357 522178 80917 4 3 2017 2017 12 23 14620 Pimpelmees 2 246692 523139 As we’re not interested in all the data here, we will load a subset of the columns, specifically all unique combinations of routes and points, which will yield the corresponding xcoor and ycoor coordinates for each count_point within a route. ptt %&gt;% dplyr::select(route, count_point, xcoor, ycoor) %&gt;% group_by(route, count_point) %&gt;% slice(1) -&gt; ptt head(ptt, 10) route count_point xcoor ycoor 4 1 246342 520763 4 2 246357 522178 4 3 246692 523139 4 4 248122 522563 4 5 248249 521818 4 6 248649 523073 4 7 249142 523614 4 8 250073 523534 4 9 252142 523524 4 10 253056 523458 4.3.2 Calculate interpoint distances For each of the routes within the PTT dataset, we will calculate the average distance between the subsequent points, to buffer our convex hull by this value. ptt %&gt;% group_by(route) %&gt;% mutate(xcoor2 = c(xcoor[-1], 0), ycoor2 = c(ycoor[-1], 0)) %&gt;% rowwise() %&gt;% mutate(interpoint_distance = pointDistance(cbind(xcoor, ycoor), cbind(xcoor2, ycoor2), lonlat = FALSE)) %&gt;% ungroup() %&gt;% filter(xcoor2 != 0) %&gt;% # Throw out last point from route where distance to next point is not relevant group_by(route) %&gt;% summarise(avg_interpoint_distance = mean(interpoint_distance), .groups = &quot;keep&quot;) -&gt; ptt_interpoint_distances ptt %&gt;% left_join(ptt_interpoint_distances, by = c(&quot;route&quot; = &quot;route&quot;)) -&gt; ptt head(ptt, 10) route count_point xcoor ycoor avg_interpoint_distance 4 1 246342 520763 1162.042 4 2 246357 522178 1162.042 4 3 246692 523139 1162.042 4 4 248122 522563 1162.042 4 5 248249 521818 1162.042 4 6 248649 523073 1162.042 4 7 249142 523614 1162.042 4 8 250073 523534 1162.042 4 9 252142 523524 1162.042 4 10 253056 523458 1162.042 We can now calculate the convex hulls of the points grouped by route. ptt %&gt;% ungroup() %&gt;% st_as_sf(coords = c(&quot;xcoor&quot;, &quot;ycoor&quot;), crs = 28992) %&gt;% # original CRS = EPSG:28992 (RD New) st_transform(crs = ppi_hrw$data@proj4string) %&gt;% group_by(route, avg_interpoint_distance) %&gt;% summarise(.groups = &quot;drop&quot;) %&gt;% st_convex_hull() %&gt;% st_as_sf() -&gt; ptt_convex_hulls_hrw # Somehow it&#39;s necessary to reconvert to sf? ptt %&gt;% ungroup() %&gt;% st_as_sf(coords = c(&quot;xcoor&quot;, &quot;ycoor&quot;), crs = 28992) %&gt;% st_transform(crs = ppi_dhl$data@proj4string) %&gt;% group_by(route, avg_interpoint_distance) %&gt;% summarise(.groups = &quot;drop&quot;) %&gt;% st_convex_hull() %&gt;% st_as_sf() -&gt; ptt_convex_hulls_dhl plot(ptt_convex_hulls_hrw[1], main = &quot;PTT Routes Herwijnen: Route&quot;) plot(ptt_convex_hulls_hrw[2], main = &quot;PTT Routes Herwijnen: Avg interpoint dist.&quot;) plot(ptt_convex_hulls_dhl[1], main = &quot;PTT Routes Den Helder: Route&quot;) plot(ptt_convex_hulls_dhl[2], main = &quot;PTT Routes Den Helder: Avg interpoint dist.&quot;) As we have calculated the average distance between the points, we can now buffer the convex hulls by this value to attain more representative sizes of the covered areas. ptt_convex_hulls_hrw %&gt;% st_buffer(dist = as.double(ptt_convex_hulls_hrw$avg_interpoint_distance)) -&gt; ptt_convex_hulls_hrw ptt_convex_hulls_dhl %&gt;% st_buffer(dist = as.double(ptt_convex_hulls_dhl$avg_interpoint_distance)) -&gt; ptt_convex_hulls_dhl st_write(ptt_convex_hulls_hrw, &quot;data/processed/sovon/ptt_convex_hulls_hrw.shp&quot;, delete_dsn = TRUE) ## Warning in abbreviate_shapefile_names(obj): Field names abbreviated for ESRI ## Shapefile driver ## Deleting source `data/processed/sovon/ptt_convex_hulls_hrw.shp&#39; using driver `ESRI Shapefile&#39; ## Writing layer `ptt_convex_hulls_hrw&#39; to data source `data/processed/sovon/ptt_convex_hulls_hrw.shp&#39; using driver `ESRI Shapefile&#39; ## Writing 591 features with 2 fields and geometry type Polygon. st_write(ptt_convex_hulls_dhl, &quot;data/processed/sovon/ptt_convex_hulls_dhl.shp&quot;, delete_dsn = TRUE) ## Warning in abbreviate_shapefile_names(obj): Field names abbreviated for ESRI ## Shapefile driver ## Deleting source `data/processed/sovon/ptt_convex_hulls_dhl.shp&#39; using driver `ESRI Shapefile&#39; ## Writing layer `ptt_convex_hulls_dhl&#39; to data source `data/processed/sovon/ptt_convex_hulls_dhl.shp&#39; using driver `ESRI Shapefile&#39; ## Writing 591 features with 2 fields and geometry type Polygon. plot(ptt_convex_hulls_hrw[1], main = &quot;Buffered PTT Routes Herwijnen&quot;) plot(ptt_convex_hulls_dhl[1], main = &quot;Buffered PTT Routes Den Helder&quot;) Now that is taken care of, we can rasterize the polygons using the fasterize package. As there is overlap between the areas covered by the routes using the convex hulls and a raster can only contain a single value for every pixel, we need to resolve this overlap. In this case we will compare overlapping areas and pick those where the average distance between the points for that area is lowest. This biases towards counts that cover a smaller area, so probably resulting in more accurate estimates of birds around. ptt_hrw &lt;- raster(ppi_hrw$data) ptt_dhl &lt;- raster(ppi_dhl$data) ptt_hrw &lt;- fasterize(ptt_convex_hulls_hrw, ptt_hrw, field = &quot;route&quot;, by = &quot;avg_interpoint_distance&quot;) ptt_dhl &lt;- fasterize(ptt_convex_hulls_dhl, ptt_dhl, field = &quot;route&quot;, by = &quot;avg_interpoint_distance&quot;) ptt_hrw &lt;- suppressWarnings(stackApply(ptt_hrw, indices = rep(1, length(ptt_hrw)), fun = min, na.rm = TRUE)) ptt_dhl &lt;- suppressWarnings(stackApply(ptt_dhl, indices = rep(1, length(ptt_dhl)), fun = min, na.rm = TRUE)) plot(ptt_hrw, main = &quot;Rasterized PTT routes Herwijnen&quot;) plot(ptt_dhl, main = &quot;Rasterized PTT routes Den Helder&quot;) This approach to solving the issue of overlapping polygons introduces a possible problem here, where it will eventually result in larger numbers of birds (corresponding to a single route) being spread over a smaller area of land (after the overlap is removed from some polygons). However, I don’t really see an alternative solution to this problem, except by averaging the numbers of birds over the whole area covered by the PTT counts. This would result in much less flexibility later on when calculating relevant bird parameters, so for now we leave it as is. With the rasterization done, let’s compare the resultant raster and see if it is identical to the PPIs. compareRaster(as(ptt_hrw, &quot;Raster&quot;), as(ppi_hrw$data, &quot;RasterLayer&quot;), extent = TRUE, rowcol = TRUE, crs = TRUE, res = TRUE, orig = TRUE) compareRaster(as(ptt_dhl, &quot;Raster&quot;), as(ppi_dhl$data, &quot;RasterLayer&quot;), extent = TRUE, rowcol = TRUE, crs = TRUE, res = TRUE, orig = TRUE) ## [1] TRUE ## [1] TRUE Two TRUEs, so that’s excellent. We can now add the ptt_route to the PPIs. ppi_hrw$data$ptt_route &lt;- unlist(as.data.frame(as(ptt_hrw, &quot;Raster&quot;))) ppi_dhl$data$ptt_route &lt;- unlist(as.data.frame(as(ptt_dhl, &quot;Raster&quot;))) Let’s verify once again if that occurred as planned. plot(ppi_hrw, param = &quot;ptt_route&quot;, zlim = c(min(ppi_hrw$data@data$ptt_route, na.rm = TRUE), max(ppi_hrw$data@data$ptt_route, na.rm = TRUE))) plot(ppi_dhl, param = &quot;ptt_route&quot;, zlim = c(min(ppi_dhl$data@data$ptt_route, na.rm = TRUE), max(ppi_dhl$data@data$ptt_route, na.rm = TRUE))) That seems fine, we can now save the PPIs, so we can start linking actual count data. saveRDS(ppi_hrw, file = &quot;data/processed/corrected-ppis-lu-sovon/corrected_ppi_hrw_lu_sovon.RDS&quot;) saveRDS(ppi_dhl, file = &quot;data/processed/corrected-ppis-lu-sovon/corrected_ppi_dhl_lu_sovon.RDS&quot;) "],
["05.Processing-count-results.html", "5 Processing count results 5.1 Setting up the processing environment 5.2 Loading the Sovon count data 5.3 Filtering/preprocessing species names 5.4 Linking life-history traits to species 5.5 Calculating total bird biomass", " 5 Processing count results For now, we are interested in calculating the following parameters for the count results: The number of birds within every PPI pixel. The average mass of the birds within every PPI pixel. We can derive the total numbers of birds comparatively easily from the bird counts by Sovon, but to calculate the average mass of the birds we need to link a database of life-history traits. For the latter we first need to translate the vernacular (modern) names of bird species to the scientific ones, so we can link both. 5.1 Setting up the processing environment library(rgbif) library(stringr) library(readxl) library(dplyr) library(tidyr) library(readr) library(kableExtra) ## ## Attaching package: &#39;kableExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## group_rows 5.2 Loading the Sovon count data The count data is spread over a few sheets in an xlsx file, which we load here. For clarity, we rename all the columns to English and we filter out all counts (i.e. areas) where birds are not positively identified to species level. Although it would be possible to ‘fill’ these uncertain counts based on proportions, determining how to do that is not necessary for our purposes. Instead, we will just remove these counts altogether. Finally, subspecies identifiers for these species are removed, as we assume there is no variation between subspecies to negatively affect our results, nor does the database of life-history traits contain parameters for subspecies. sovon_data &lt;- &quot;data/raw/sovon/tel_dec_jan_1718.xlsx&quot; data &lt;- data.frame() sheets &lt;- excel_sheets(sovon_data) sheets &lt;- sheets[-c(1, 5)] # Sheet 1 and 5 contain PTT and roost counts respectively, so we ignore these for now, as they have to be processed differently # Explicit column types to suppress warnings thrown because of lacking euring codes for records with no birds coltypes &lt;- c(&quot;numeric&quot;, &quot;text&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;text&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;) for (i in seq_along(sheets)) { data &lt;- rbind(data, read_excel(sovon_data, sheet = sheets[i], col_types = coltypes)) } data %&gt;% drop_na() %&gt;% # A few rows somehow contain no birds or species &#39;geen vogels&#39; rename(count_id = TELLING_ID, area_nr = GEBIEDSCODE, year = JAAR, month = MAAND, day = DAG, start_time = BEGINTIJD, end_time = EINDTIJD, &quot;euring&quot; = &quot;EURING&quot;, species = SOORT, number = Aantal, xcoor = XCOOR, ycoor = YCOOR) %&gt;% group_by(area_nr) %&gt;% filter(!any(str_ends(species, &quot;spec.&quot;))) %&gt;% # Filter out all counts with unidentified birds filter(!any(length(str_subset(species, &quot; of &quot;)) &gt; 0)) %&gt;% # Filter out all counts with either/or totals filter(!any(str_starts(species, &quot;hybride&quot;))) %&gt;% # Filter out all counts with hybrids ungroup() %&gt;% rowwise() %&gt;% mutate(species = str_split(species, &quot;\\\\(&quot;)[[1]][1] %&gt;% str_trim()) -&gt; wb_data # And remove all subspecies identifications head(wb_data, 10) %&gt;% kable(format = &quot;html&quot;, col.names = colnames(wb_data)) %&gt;% kable_styling() %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;, height = &quot;400px&quot;) count_id area_nr year month day start_time end_time euring species number xcoor ycoor 1060908 BR1111 2017 12 19 1045 1108 1610 Grauwe Gans 280 127723 426233 1060908 BR1111 2017 12 19 1045 1108 1700 Nijlgans 2 127723 426233 1060909 BR1112 2017 12 19 1108 1126 1610 Grauwe Gans 76 125082 426701 1060909 BR1112 2017 12 19 1108 1126 1661 Grote Canadese Gans 50 125082 426701 1060909 BR1112 2017 12 19 1108 1126 1700 Nijlgans 22 125082 426701 1060906 BR1122 2017 12 19 1020 1037 1610 Grauwe Gans 250 125313 426061 1060906 BR1122 2017 12 19 1020 1037 1619 Soepgans 5 125313 426061 1060906 BR1122 2017 12 19 1020 1037 1661 Grote Canadese Gans 35 125313 426061 1060911 BR1130 2017 12 19 1145 1200 1610 Grauwe Gans 200 121962 426420 1060911 BR1130 2017 12 19 1145 1200 1661 Grote Canadese Gans 40 121962 426420 Following this logic, we can process the PTT counts similarly and see which species are contained in those. read_excel(sovon_data, sheet = 1) %&gt;% rename(count_id = tellingid, route = route, count_point = telpunt, season = seizoen, year = teljaar, month = maand, day = dag, euring = euring, species = soort, number = aantal, xcoor = xcoor, ycoor = ycoor) %&gt;% group_by(route, count_point) %&gt;% filter(!any(str_ends(species, &quot;spec.&quot;))) %&gt;% # Filter out all counts with unidentified birds filter(!any(length(str_subset(species, &quot; of &quot;)) &gt; 0)) %&gt;% # Filter out all counts with either/or totals filter(!any(str_starts(species, &quot;hybride&quot;))) %&gt;% # Filter out all counts with hybrids ungroup() %&gt;% rowwise() %&gt;% mutate(species = str_split(species, &quot;\\\\(&quot;)[[1]][1] %&gt;% str_trim()) -&gt; ptt_data # And remove all subspecies identifications-&gt; ptt_data head(ptt_data, 10) %&gt;% kable(format = &quot;html&quot;, col.names = colnames(ptt_data)) %&gt;% kable_styling() %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;, height = &quot;400px&quot;) count_id route count_point season year month day euring species number xcoor ycoor 80917 4 1 2017 2017 12 23 720 Aalscholver 2 246342 520763 80917 4 1 2017 2017 12 23 5920 Zilvermeeuw 18 246342 520763 80917 4 1 2017 2017 12 23 6700 Houtduif 1 246342 520763 80917 4 1 2017 2017 12 23 11870 Merel 4 246342 520763 80917 4 1 2017 2017 12 23 15630 Roek 24 246342 520763 80917 4 2 2017 2017 12 23 6700 Houtduif 1 246357 522178 80917 4 2 2017 2017 12 23 11870 Merel 1 246357 522178 80917 4 2 2017 2017 12 23 15600 Kauw 6 246357 522178 80917 4 2 2017 2017 12 23 15630 Roek 78 246357 522178 80917 4 3 2017 2017 12 23 14620 Pimpelmees 2 246692 523139 5.3 Filtering/preprocessing species names The following section is the result of an iterative process aimed at matching species in our count data with species in the database of life-history characteristics. Unfortunately, automatic tools only get us so far, a bit of tweaking has to be done by hand. To reduce manual corrections needed, only species to which &gt;1% of the birds belong in a single count have been corrected manually. In other words: if a species which cannot be matched always accounts for less than 1% of the total number of birds within a count, this species is discarded from the whole dataset. Besides the 1% criteria, both the waterbird and PTT counts contain some exotics (e.g. ‘Helmparelhoen’/Helmeted guineafowl) for which our life-history characteristics dataset does not contain any measurements anyways (there are others that we do have measurements of), some species that are very unlikely to ever take flight during NYE (e.g. ‘Kip’/Domesticated chicken) and some mammals for which the same applies. We will remove these from the dataset manually. exotics &lt;- c(&quot;Helmparelhoen&quot;, &quot;Kaapse Casarca&quot;, &quot;Manengans&quot;, &quot;Ringtaling&quot;, &quot;Buffelkopeend&quot;, &quot;Kokardezaagbek&quot;, &quot;Chileense Flamingo&quot;, &quot;Kaapse Taling&quot;, &quot;Bahamapijlstaart&quot;, &quot;Muskuseend&quot;, &quot;Zwarte Zwaan&quot;, &quot;Knobbelgans&quot;, &quot;Zwaangans&quot;) unlikely_flight_candidate &lt;- c(&quot;Kip&quot;) mammals &lt;- c(&quot;Damhert&quot;, &quot;Haas&quot;, &quot;Ree&quot;, &quot;Bever&quot;, &quot;Bruine Rat&quot;, &quot;Muskusrat&quot;, &quot;Mol&quot;, &quot;Vos&quot;, &quot;Kat&quot;, &quot;Otter&quot;, &quot;Grijze Zeehond&quot;, &quot;Konijn&quot;, &quot;Eekhoorn&quot;, &quot;Edelhert&quot;, &quot;Gewone Zeehond&quot;, &quot;Wild Zwijn&quot;, &quot;Steenmarter&quot;, &quot;Moeflon&quot;) input_error &lt;- c(&quot;Steltstrandloper&quot;) remove_species &lt;- c(exotics, unlikely_flight_candidate, mammals, input_error) ptt_data %&gt;% filter(!species %in% remove_species) -&gt; ptt_data wb_data %&gt;% filter(!species %in% remove_species) -&gt; wb_data With all these species removed or adjusted, we can create a species lookup table. We fetch the scientific names and corresponding GBIF species IDs from the Checklist Dutch Species Register, which is the GBIF dataset with key 4dd32523-a3a3-43b7-84df-4cda02f15cf7. We furthermore remove all unnecessary information, such as subspecies from the scientific names as well. unique_species &lt;- unique(c(wb_data$species, ptt_data$species)) build_species_lut &lt;- function(specieslist, datasetKey = NULL, class_name = NULL) { # As this function can possibly return many different records, we pick the scientific name and GBIF ID (nubKey) that are the most # common in the returned results. This should most often result in an OK result of the name lookup function. Mode &lt;- function(x) { ux &lt;- unique(na.omit(x)) ux[which.max(tabulate(match(x, ux)))] } n &lt;- length(specieslist) species_lut &lt;- data.frame(lookupname = character(n), scientificname = character(n), gbif_key = numeric(n), stringsAsFactors = FALSE) # Somehow the higherTaxonKey changes regularly, so we have to query this first higherTaxonKey &lt;- NULL if (!is.null(class_name)) { class_record &lt;- name_lookup(class_name, datasetKey = datasetKey) higherTaxonKey &lt;- class_record$data$classKey } for(i in seq_along(specieslist)) { gbif_data &lt;- tryCatch({ gbif &lt;- name_lookup(specieslist[i], datasetKey = datasetKey, higherTaxonKey = higherTaxonKey, return = &quot;data&quot;) list(paste(str_split(Mode(gbif$scientificName), pattern = &quot; &quot;)[[1]][1:2], collapse = &quot; &quot;), Mode(gbif$nubKey)) }, error = function(e) { list(&quot;&quot;, NaN) }) species_lut[i, ] &lt;- c(specieslist[i], gbif_data[1], as.numeric(as.character(gbif_data[2]))) } return(species_lut) } species_lut &lt;- build_species_lut(unique_species, datasetKey = &quot;4dd32523-a3a3-43b7-84df-4cda02f15cf7&quot;, class_name = &quot;Aves&quot;) ## Warning: Unknown or uninitialised column: `scientificName`. head(species_lut, 10) %&gt;% kable(format = &quot;html&quot;, col.names = colnames(species_lut)) %&gt;% kable_styling() %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;, height = &quot;400px&quot;) lookupname scientificname gbif_key Grauwe Gans Anser anser 2498036 Nijlgans Alopochen aegyptiaca 2498252 Grote Canadese Gans Branta canadensis 5232437 Soepgans Anser anser 9384117 Brandgans Branta leucopsis 5232464 Knobbelzwaan Cygnus olor 2498343 Kolgans Anser albifrons 2498017 Canadese Gans Branta hutchinsii 5232458 Toendrarietgans Anser serrirostris 9455781 Kleine Zwaan Cygnus bewickii 4409105 Finally, the lookup table also contains some scientific names which unfortunately will not match with the life-history characteristics dataset, so these too we will adjust manually. # Change to similar species which is in the LHT database species_lut[species_lut$lookupname == &quot;Toendrarietgans&quot;, &quot;scientificname&quot;] &lt;- &quot;Anser fabalis&quot; # Taiga Bean Goose species_lut[species_lut$lookupname == &quot;Kleine Canadese Gans&quot;, &quot;scientificname&quot;] &lt;- &quot;Branta leucopsis&quot; # Barnacle Goose species_lut[species_lut$lookupname == &quot;Kleine Barmsijs&quot;, &quot;scientificname&quot;] &lt;- &quot;Acanthis flammea&quot; # Redpoll species_lut[species_lut$lookupname == &quot;Pontische Meeuw&quot;, &quot;scientificname&quot;] &lt;- &quot;Larus argentatus&quot; # Herring Gull species_lut[species_lut$lookupname == &quot;Indische Gans&quot;, &quot;scientificname&quot;] &lt;- &quot;Anser albifrons&quot; # Greater White-fronted Goose species_lut[species_lut$lookupname == &quot;Canadese Gans&quot;, &quot;scientificname&quot;] &lt;- &quot;Branta canadensis&quot; # Canada Goose # Change scientific name for same species to match with the LHT database species_lut[species_lut$lookupname == &quot;Kleine Zwaan&quot;, &quot;scientificname&quot;] &lt;- &quot;Cygnus columbianus&quot; # Bewick&#39;s Swan species_lut[species_lut$lookupname == &quot;Kokmeeuw&quot;, &quot;scientificname&quot;] &lt;- &quot;Larus ridibundus&quot; # Black-headed Gull species_lut[species_lut$lookupname == &quot;Smient&quot;, &quot;scientificname&quot;] &lt;- &quot;Mareca penelope&quot; # Wigeon species_lut[species_lut$lookupname == &quot;Krakeend&quot;, &quot;scientificname&quot;] &lt;- &quot;Mareca strepera&quot; # Gadwall species_lut[species_lut$lookupname == &quot;Slobeend&quot;, &quot;scientificname&quot;] &lt;- &quot;Spatula clypeata&quot; # Northern Shoveler species_lut[species_lut$lookupname == &quot;Winterkoning&quot;, &quot;scientificname&quot;] &lt;- &quot;Troglodytes troglodytes&quot; # Wren species_lut[species_lut$lookupname == &quot;Grote Jager&quot;, &quot;scientificname&quot;] &lt;- &quot;Catharacta skua&quot; # Great Skua species_lut[species_lut$lookupname == &quot;Roodborsttapuit&quot;, &quot;scientificname&quot;] &lt;- &quot;Saxicola torquatus&quot; # Stonechat species_lut[species_lut$lookupname == &quot;Strandleeuwerik&quot;, &quot;scientificname&quot;] &lt;- &quot;Eremophila alpestris&quot; # Horned Lark 5.4 Linking life-history traits to species We use the Life-history characteristics of European birds-dataset (Storchová and Hořák 2018) to calculate the mean mass of all birds in a PPI pixel. This dataset is stored on Dryad and we can download it there. Unfortunately the rdryad package is severely out-of-date with the new Dryad API, so we cannot nicely automate this download yet. Anyways, the files should be downloaded manually and added to data/raw/life-history-characteristics/. We calculate the mean of the mass of both sexed and unsexed birds and assume they occur read_tsv(&quot;data/raw/life-history-characteristics/Life-history characteristics of European birds.txt&quot;, col_types = cols_only(&#39;Species&#39; = col_character(), &#39;WeightU_MEAN&#39; = col_double(), &#39;WeightM_MEAN&#39; = col_double(), &#39;WeightF_MEAN&#39; = col_double())) %&gt;% rowwise %&gt;% mutate(mean_weight = mean(c(WeightU_MEAN, WeightF_MEAN, WeightM_MEAN))) %&gt;% dplyr::select(Species, mean_weight) %&gt;% rename(species = Species) %&gt;% filter(!any(str_ends(species, &quot;ssp&quot;))) %&gt;% # Filter out birds not identified to species rowwise() %&gt;% mutate(species = paste(str_split(species, pattern = &quot; &quot;)[[1]][1:2], collapse = &quot; &quot;)) %&gt;% ungroup() %&gt;% group_by(species) %&gt;% summarise(mean_weight = mean(mean_weight)) %&gt;% drop_na() -&gt; lhc ## `summarise()` ungrouping output (override with `.groups` argument) lhc[lhc$species == &quot;Aquila nipalenis&quot;, &quot;species&quot;] &lt;- &quot;Aquila nipalensis&quot; # Small error in dataset -&gt; notified author head(lhc, 10) %&gt;% kable(format = &quot;html&quot;, col.names = colnames(lhc)) %&gt;% kable_styling() %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;, height = &quot;400px&quot;) species mean_weight Acanthis flammea 13.40 Accipiter brevipes 221.50 Accipiter gentilis 931.50 Accipiter nisus 204.00 Acridotheres cristatellus 124.35 Acridotheres tristis 124.35 Acrocephalus agricola 10.50 Acrocephalus arundinaceus 30.30 Acrocephalus dumetorum 12.00 Acrocephalus melanopogon 11.70 Now we can try to link the names once again with what can be found in GBIF. unique_species_lhc &lt;- unique(lhc$species) lhc_species_lut &lt;- build_species_lut(unique_species_lhc, dataset = NULL) head(lhc_species_lut, 10) %&gt;% kable(format = &quot;html&quot;, col.names = colnames(lhc_species_lut)) %&gt;% kable_styling() %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;, height = &quot;400px&quot;) lookupname scientificname gbif_key Acanthis flammea Acanthis flammea 5231630 Accipiter brevipes Accipiter brevipes 2480578 Accipiter gentilis Accipiter gentilis 2480589 Accipiter nisus Accipiter nisus 2480637 Acridotheres cristatellus Acridotheres cristatellus 2489010 Acridotheres tristis Acridotheres tristis 2489005 Acrocephalus agricola Acrocephalus agricola 2493137 Acrocephalus arundinaceus Acrocephalus arundinaceus 2493128 Acrocephalus dumetorum Acrocephalus dumetorum 2493145 Acrocephalus melanopogon Acrocephalus melanopogon 2493124 With the GBIF IDs/keys in place for both the life-history characteristics, as well as the Sovon counts, we can now link the datasets together. First the waterbirds lhc %&gt;% left_join(lhc_species_lut, by = c(&quot;species&quot; = &quot;scientificname&quot;)) %&gt;% dplyr::select(species, mean_weight, gbif_key) -&gt; lhc wb_data %&gt;% left_join(species_lut, by = c(&quot;species&quot; = &quot;lookupname&quot;)) %&gt;% left_join(dplyr::select(lhc, mean_weight, gbif_key), by = c(&quot;gbif_key&quot; = &quot;gbif_key&quot;)) %&gt;% left_join(dplyr::select(lhc, mean_weight, species), by = c(&quot;scientificname&quot; = &quot;species&quot;)) %&gt;% dplyr::select(-c(mean_weight.x)) %&gt;% rename(mean_weight = mean_weight.y) -&gt; wb_data_lhc head(wb_data_lhc, 10) %&gt;% kable(format = &quot;html&quot;, col.names = colnames(wb_data_lhc)) %&gt;% kable_styling() %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;, height = &quot;400px&quot;) count_id area_nr year month day start_time end_time euring species number xcoor ycoor scientificname gbif_key mean_weight 1060908 BR1111 2017 12 19 1045 1108 1610 Grauwe Gans 280 127723 426233 Anser anser 2498036 3347 1060908 BR1111 2017 12 19 1045 1108 1700 Nijlgans 2 127723 426233 Alopochen aegyptiaca 2498252 2270 1060909 BR1112 2017 12 19 1108 1126 1610 Grauwe Gans 76 125082 426701 Anser anser 2498036 3347 1060909 BR1112 2017 12 19 1108 1126 1661 Grote Canadese Gans 50 125082 426701 Branta canadensis 5232437 4635 1060909 BR1112 2017 12 19 1108 1126 1700 Nijlgans 22 125082 426701 Alopochen aegyptiaca 2498252 2270 1060906 BR1122 2017 12 19 1020 1037 1610 Grauwe Gans 250 125313 426061 Anser anser 2498036 3347 1060906 BR1122 2017 12 19 1020 1037 1619 Soepgans 5 125313 426061 Anser anser 9384117 3347 1060906 BR1122 2017 12 19 1020 1037 1661 Grote Canadese Gans 35 125313 426061 Branta canadensis 5232437 4635 1060911 BR1130 2017 12 19 1145 1200 1610 Grauwe Gans 200 121962 426420 Anser anser 2498036 3347 1060911 BR1130 2017 12 19 1145 1200 1661 Grote Canadese Gans 40 121962 426420 Branta canadensis 5232437 4635 And then the PTT counts ptt_data %&gt;% left_join(species_lut, by = c(&quot;species&quot; = &quot;lookupname&quot;)) %&gt;% left_join(dplyr::select(lhc, mean_weight, gbif_key), by = c(&quot;gbif_key&quot; = &quot;gbif_key&quot;)) %&gt;% left_join(dplyr::select(lhc, mean_weight, species), by = c(&quot;scientificname&quot; = &quot;species&quot;)) %&gt;% dplyr::select(-c(mean_weight.x)) %&gt;% rename(mean_weight = mean_weight.y) -&gt; ptt_data_lhc head(ptt_data_lhc, 10) %&gt;% kable(format = &quot;html&quot;, col.names = colnames(ptt_data_lhc)) %&gt;% kable_styling() %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;, height = &quot;400px&quot;) count_id route count_point season year month day euring species number xcoor ycoor scientificname gbif_key mean_weight 80917 4 1 2017 2017 12 23 720 Aalscholver 2 246342 520763 Phalacrocorax carbo 2481890 2254.0 80917 4 1 2017 2017 12 23 5920 Zilvermeeuw 18 246342 520763 Larus argentatus 2481139 1054.0 80917 4 1 2017 2017 12 23 6700 Houtduif 1 246342 520763 Columba palumbus 2495455 496.5 80917 4 1 2017 2017 12 23 11870 Merel 4 246342 520763 Turdus merula 2490719 97.0 80917 4 1 2017 2017 12 23 15630 Roek 24 246342 520763 Corvus frugilegus 2482513 459.0 80917 4 2 2017 2017 12 23 6700 Houtduif 1 246357 522178 Columba palumbus 2495455 496.5 80917 4 2 2017 2017 12 23 11870 Merel 1 246357 522178 Turdus merula 2490719 97.0 80917 4 2 2017 2017 12 23 15600 Kauw 6 246357 522178 Corvus monedula 2482473 236.0 80917 4 2 2017 2017 12 23 15630 Roek 78 246357 522178 Corvus frugilegus 2482513 459.0 80917 4 3 2017 2017 12 23 14620 Pimpelmees 2 246692 523139 Cyanistes caeruleus 2487879 11.5 Now we can verify if our 1% criteria for species matching is met. We do this by calculating the proportions of birds belonging to a certain species out of the total numbers of birds counted within a count. This should result in an empty dataframe, which will stop the code chunk below from running if that is not the case. wb_data_lhc %&gt;% as.data.frame() %&gt;% group_by(count_id) %&gt;% mutate(total_birds_count = sum(number)) %&gt;% group_by(count_id, species) %&gt;% mutate(proportion_species = sum(number) / total_birds_count) %&gt;% ungroup() %&gt;% arrange(count_id) %&gt;% filter((is.na(mean_weight) &amp; (proportion_species &gt; 0.01))) -&gt; wb_data_lhc_verify stopifnot(nrow(wb_data_lhc_verify) == 0) rm(wb_data_lhc_verify) And once again, we can do the same for the PTT counts. ptt_data_lhc %&gt;% as.data.frame() %&gt;% group_by(count_id) %&gt;% mutate(total_birds_count = sum(number)) %&gt;% group_by(count_id, species) %&gt;% mutate(proportion_species = sum(number) / total_birds_count) %&gt;% ungroup() %&gt;% arrange(count_id) %&gt;% filter((is.na(mean_weight) &amp; (proportion_species &gt; 0.01))) -&gt; ptt_data_lhc_verify stopifnot(nrow(ptt_data_lhc_verify) == 0) rm(ptt_data_lhc_verify) With that out of the way we can finally remove the remaining empty rows and save the PTT and waterbird counts in their final form. ptt_data_lhc %&gt;% drop_na() -&gt; ptt_data wb_data_lhc %&gt;% drop_na() -&gt; wb_data And we save the data for further use. saveRDS(ptt_data, file = &quot;data/processed/sovon/ptt.RDS&quot;) saveRDS(wb_data, file = &quot;data/processed/sovon/wb.RDS&quot;) 5.5 Calculating total bird biomass While we are at it, we will already calculate the total biomass contained within each of the count areas, so we can then add these values to the PPIs through the IDs of the count areas/routes. We use the waterbird count from January 2018, and the point-transect counts from 2017, as these are the best in terms of coverage. wb_year &lt;- 2018 ptt_year &lt;- 2017 wb_data %&gt;% mutate(area_nr = as.character(area_nr)) %&gt;% filter(year == wb_year) %&gt;% rowwise() %&gt;% mutate(specific_biomass = number * mean_weight) %&gt;% ungroup() %&gt;% group_by(area_nr, year) %&gt;% summarise(total_biomass = sum(specific_biomass), .groups = &quot;drop_last&quot;) %&gt;% ungroup() %&gt;% group_by(area_nr) %&gt;% # Below is only required if year-filter is NOT set, otherwise does nothing summarise(total_biomass = mean(total_biomass), .groups = &quot;drop_last&quot;) -&gt; wb_biomass ptt_data %&gt;% filter(year == ptt_year) %&gt;% rowwise() %&gt;% mutate(specific_biomass = number * mean_weight) %&gt;% ungroup() %&gt;% group_by(route, year) %&gt;% summarise(total_biomass = sum(specific_biomass), .groups = &quot;drop_last&quot;) %&gt;% ungroup() %&gt;% group_by(route) %&gt;% # Below is only required if year-filter is NOT set, otherwise does nothing summarise(total_biomass = mean(total_biomass), .groups = &quot;drop_last&quot;) -&gt; ptt_biomass And now we will save these datasets of total_biomass as well. saveRDS(wb_biomass, file = &quot;data/processed/sovon/wb_biomass.RDS&quot;) saveRDS(ptt_biomass, file = &quot;data/processed/sovon/ptt_biomass.RDS&quot;) References "],
["06.Applying-annotations-to-all-PPIs.html", "6 Apply processing to all PPIs 6.1 Setting-up the environment 6.2 Load the reference PPIs 6.3 Add biomass to reference PPIs 6.4 Copy annotations from reference PPIs to other PPIs", " 6 Apply processing to all PPIs We have so far explained the processing ‘pipeline’ for the PPIs for Herwijnen and Den Helder radars, but have only applied this to a single moment in time, a single volume scan. Now we will apply the processing to all other available PPIs by simply copying over the annotations. Pre-processed the radar data, by removing clutter and applying the range-bias correction (Kranstauber et al. 2020). Annotated the PPIs with land use class proportions, distance to urban areas and human population. Annotated the PPIs with corresponding count locations for the Sovon data. Connected the Sovon count data with life-history characteristics for the species contained within. 6.1 Setting-up the environment library(dplyr) library(tidyr) library(kableExtra) 6.2 Load the reference PPIs Two PPIs have been processed all the way and those will be our ‘reference’ PPIs from which we copy the data over to the other PPIs which have not had the same treatment. ppi_hrw &lt;- readRDS(&quot;data/processed/corrected-ppis-lu-sovon/corrected_ppi_hrw_lu_sovon.RDS&quot;) ppi_dhl &lt;- readRDS(&quot;data/processed/corrected-ppis-lu-sovon/corrected_ppi_dhl_lu_sovon.RDS&quot;) 6.3 Add biomass to reference PPIs We have previously calculated the total_biomass already for all the count areas/routes, but now we will annotate the reference PPIs with these values as well. We correct for the size of an area the count is conducted in by ‘spreading’ the total biomass over the number of pixels annotated with a certain count area/route. wb &lt;- readRDS(&quot;data/processed/sovon/wb_biomass.RDS&quot;) ptt &lt;- readRDS(&quot;data/processed/sovon/ptt_biomass.RDS&quot;) ppi_hrw$data@data %&lt;&gt;% left_join(dplyr::select(wb, area_nr, total_biomass), by = c(&quot;wb_area_nr&quot; = &quot;area_nr&quot;)) %&gt;% rename(wb_total_biomass = total_biomass) %&gt;% group_by(wb_area_nr) %&gt;% mutate(wb_total_biomass = wb_total_biomass / n()) %&gt;% ungroup() %&gt;% left_join(dplyr::select(ptt, route, total_biomass), by = c(&quot;ptt_route&quot; = &quot;route&quot;)) %&gt;% rename(ptt_total_biomass = total_biomass) %&gt;% group_by(ptt_route) %&gt;% mutate(ptt_total_biomass = ptt_total_biomass / n()) %&gt;% ungroup() %&gt;% rowwise() %&gt;% mutate(total_biomass = sum(wb_total_biomass, ptt_total_biomass, na.rm = TRUE)) %&gt;% ungroup() ppi_dhl$data@data %&lt;&gt;% left_join(dplyr::select(wb, area_nr, total_biomass), by = c(&quot;wb_area_nr&quot; = &quot;area_nr&quot;)) %&gt;% rename(wb_total_biomass = total_biomass) %&gt;% group_by(wb_area_nr) %&gt;% mutate(wb_total_biomass = wb_total_biomass / n()) %&gt;% ungroup() %&gt;% left_join(dplyr::select(ptt, route, total_biomass), by = c(&quot;ptt_route&quot; = &quot;route&quot;)) %&gt;% rename(ptt_total_biomass = total_biomass) %&gt;% group_by(ptt_route) %&gt;% mutate(ptt_total_biomass = ptt_total_biomass / n()) %&gt;% ungroup() %&gt;% rowwise() %&gt;% mutate(total_biomass = sum(wb_total_biomass, ptt_total_biomass, na.rm = TRUE)) %&gt;% ungroup() 6.4 Copy annotations from reference PPIs to other PPIs We select the variables related to land use, distance to urbanized areas and population density, waterbird counts and PTT counts and add these to the PPIs that contain just the variables resulting from the range-bias correction (Kranstauber et al. 2020). While we are at it, we will also create dataframes containing all the information within the PPIs, to use for later modelling exercises. columns &lt;- c(&quot;urban&quot;, &quot;agricultural&quot;, &quot;semiopen&quot;, &quot;forests&quot;, &quot;wetlands&quot;, &quot;waterbodies&quot;, &quot;dist_urban&quot;, &quot;human_pop&quot;, &quot;wb_area_id&quot;, &quot;wb_area_nr&quot;, &quot;ptt_route&quot;, &quot;wb_area_id&quot;, &quot;wb_area_nr&quot;, &quot;wb_area_ha&quot;, &quot;ptt_route&quot;, &quot;wb_total_biomass&quot;, &quot;ptt_total_biomass&quot;, &quot;total_biomass&quot;) hrw &lt;- ppi_hrw$data@data %&gt;% dplyr::select(all_of(columns)) dhl &lt;- ppi_dhl$data@data %&gt;% dplyr::select(all_of(columns)) hrw_all &lt;- ppi_hrw$data@data %&gt;% filter(row_number() == 0) dhl_all &lt;- ppi_dhl$data@data %&gt;% filter(row_number() == 0) hrw_ppis &lt;- Sys.glob(file.path(&quot;data/processed/corrected-ppis&quot;, &quot;*NL62*&quot;)) dhl_ppis &lt;- Sys.glob(file.path(&quot;data/processed/corrected-ppis&quot;, &quot;*NL61*&quot;)) for (ppi_path in hrw_ppis) { ppi &lt;- readRDS(ppi_path) ppi$data@data &lt;- bind_cols(ppi$data@data, hrw) saveRDS(ppi, file = paste(&quot;data/processed/final-ppis/&quot;, basename(ppi_path), sep = &quot;&quot;)) ppi$data@data %&gt;% mutate(datetime = as.POSIXct(ppi$datetime), pixel = row_number()) %&gt;% bind_rows(hrw_all) -&gt; hrw_all } for (ppi_path in dhl_ppis) { ppi &lt;- readRDS(ppi_path) ppi$data@data &lt;- bind_cols(ppi$data@data, dhl) saveRDS(ppi, file = paste(&quot;data/processed/final-ppis/&quot;, basename(ppi_path), sep = &quot;&quot;)) ppi$data@data %&gt;% mutate(datetime = as.POSIXct(ppi$datetime), pixel = row_number()) %&gt;% bind_rows(dhl_all) -&gt; dhl_all } saveRDS(hrw_all, file = &quot;data/processed/hrw.RDS&quot;) saveRDS(dhl_all, file = &quot;data/processed/dhl.RDS&quot;) data/processed/hrw.RDS and data/processed/dhl.RDS now contain all the PPI rows for each radar respectively. References "],
["07.Composite-all-PPIs.html", "7 Composite all PPIs 7.1 Setting-up the environment 7.2 Generating the composite PPIs 7.3 Visualising the composites", " 7 Composite all PPIs With all the preprocessing done, we can now make composites of the PPIs to ‘solve’ the overlapping areas of the Den Helder and Herwijnen radars. While we’re at it, we will also render out these composites, to visualise the en-masse take-off of birds. 7.1 Setting-up the environment library(bioRad) library(ggplot2) library(magick) ## Linking to ImageMagick 6.9.7.4 ## Enabled features: fontconfig, freetype, fftw, lcms, pango, x11 ## Disabled features: cairo, ghostscript, rsvg, webp library(purrr) ## ## Attaching package: &#39;purrr&#39; ## The following object is masked from &#39;package:magrittr&#39;: ## ## set_names ## The following object is masked from &#39;package:bioRad&#39;: ## ## map library(viridis) library(dplyr) 7.2 Generating the composite PPIs We will loop over all the Herwijnen and Den Helder PPIs we have generated so far to create composite PPIs for all included parameters. We generate these composites at a 500m, 1000m and 2000m resolution to later test at which resolution the models perform best. Additionally we will save the composites as a bunch of .png files that we can then use separately or inclued in the animated GIF below. source(&quot;R/comp_ppi.R&quot;) hrw_ppis &lt;- Sys.glob(file.path(&quot;data/processed/final-ppis&quot;, &quot;*NL62*&quot;)) dhl_ppis &lt;- Sys.glob(file.path(&quot;data/processed/final-ppis&quot;, &quot;*NL61*&quot;)) generate_composites &lt;- function(hrw_ppis, dhl_ppis, res, maxrange) { # Make a new empty PPI to store all composites in template_ppi &lt;- readRDS(hrw_ppis[1]) all &lt;- template_ppi$data@data %&gt;% filter(row_number() == 0) basemap &lt;- NULL for (i in seq_along(hrw_ppis)) { ppi_hrw &lt;- readRDS(hrw_ppis[i]) ppi_dhl &lt;- readRDS(dhl_ppis[i]) # Set all columns to NA if further than maxrange from radar ppi_hrw$data@data[ppi_hrw$data@data$dist_radar &gt; maxrange, ] &lt;- NA ppi_dhl$data@data[ppi_dhl$data@data$dist_radar &gt; maxrange, ] &lt;- NA # ppi_hrw$data$VID[ppi_hrw$data$VIR == 0] &lt;- NA # ppi_hrw$data$VIR[ppi_hrw$data$VIR == 0] &lt;- NA # ppi_dhl$data$VID[ppi_dhl$data$VIR == 0] &lt;- NA # ppi_dhl$data$VIR[ppi_dhl$data$VIR == 0] &lt;- NA params &lt;- c(&quot;VIR&quot;, &quot;VID&quot;, &quot;R&quot;, &quot;overlap&quot;, &quot;eta_sum&quot;, &quot;eta_sum_expected&quot;, &quot;dist_radar&quot;, &quot;class&quot;, &quot;urban&quot;, &quot;agricultural&quot;, &quot;semiopen&quot;, &quot;forests&quot;, &quot;wetlands&quot;, &quot;waterbodies&quot;, &quot;dist_urban&quot;, &quot;human_pop&quot;, &quot;wb_area_id&quot;, &quot;wb_area_nr&quot;, &quot;ptt_route&quot;, &quot;wb_area_ha&quot;, &quot;wb_total_biomass&quot;, &quot;ptt_total_biomass&quot;, &quot;total_biomass&quot;) # All mean methods except for factors and urban area (set to max), because we want to strictly filter out fireworks methods &lt;- c(&quot;mean&quot;, &quot;mean&quot;, &quot;mean&quot;, &quot;mean&quot;, &quot;mean&quot;, &quot;mean&quot;, &quot;min&quot;, &quot;min&quot;, &quot;max&quot;, &quot;mean&quot;, &quot;mean&quot;, &quot;mean&quot;, &quot;mean&quot;, &quot;mean&quot;, &quot;mean&quot;, &quot;mean&quot;, &quot;factor&quot;, &quot;factor&quot;, &quot;factor&quot;, &quot;factor&quot;, &quot;mean&quot;, &quot;mean&quot;, &quot;mean&quot;) cppi &lt;- comp_ppi(list(ppi_hrw, ppi_dhl), param = params, method = methods, res = c(res, res), coverage = &quot;count&quot;) # Set rain and background pixels to NA cppi$data$VIR[cppi$data$class &lt; 2] &lt;- NA # Add coordinates coords_cppi &lt;- raster::coordinates(cppi$data) cppi$data$x &lt;- coords_cppi[, 1] cppi$data$y &lt;- coords_cppi[, 2] # Add pixel ID cppi$data@data %&gt;% mutate(pixel = row_number()) -&gt; cppi$data@data saveRDS(cppi, file = paste(&quot;data/processed/composite-ppis/&quot;, res, &quot;m/&quot;, strftime(ppi_hrw$datetime, format = &quot;%Y%m%d%H%M&quot;), &quot;.RDS&quot;, sep = &quot;&quot;)) cppi$data@data %&gt;% mutate(datetime = as.POSIXct(ppi_hrw$datetime)) %&gt;% bind_rows(all) -&gt; all if (i == 1) { basemap &lt;- download_basemap(cppi, alpha = 0.3) } cppi$data$VIR &lt;- log10(cppi$data$VIR) cppi$data$VIR[is.na(cppi$data$VIR)] &lt;- 0 bioRad::map(cppi, map = basemap, radar_size = 1, xlim = c(3.1, 6.8), ylim = c(51, 54), zlim = c(0, 4.5), palette = viridis(256, option = &quot;viridis&quot;, alpha = 0.6)) + labs(title = &quot;Fireworks NYE 2017-2018&quot;, subtitle = paste(ppi_hrw$datetime, &#39; UTC&#39;, sep = &quot;&quot;)) ggsave(paste(&quot;data/plots/vir-ppis/&quot;, res, &quot;m/&quot;, strftime(ppi_hrw$datetime, format = &quot;%Y%m%d%H%M&quot;), &quot;.png&quot;, sep = &quot;&quot;)) } saveRDS(all, file = paste(&quot;data/processed/all_&quot;, res, &quot;m.RDS&quot;, sep = &quot;&quot;)) } r &lt;- parallel::mclapply(c(2000), function(x) { generate_composites(hrw_ppis, dhl_ppis, res = x, maxrange = 66000)}, mc.cores = 3, mc.preschedule = FALSE) 7.3 Visualising the composites In order to run this comparatively simple conversion to a GIF of the PPIs, it may be necessary to increase the memory available to ImageMagick by changing the memory policy in /etc/ImageMagick-6/policy.xml. Additionally, we will reduce the resolution of the animated GIF and we’ll stick to the 1000m resolution PPIs. width &lt;- 800 composites &lt;- list.files(path = &quot;data/plots/vir-ppis/500m/&quot;, pattern = &quot;*.png&quot;, full.names = TRUE) process_image &lt;- function(img_path) { image_read(img_path) %&gt;% image_resize(geometry_size_pixels(width = width)) %&gt;% image_write(path = paste(tools::file_path_sans_ext(img_path), &quot;.jpeg&quot;, sep = &quot;&quot;), format = &quot;jpeg&quot;) } for (file in list.files(path = &quot;data/plots/vir-ppis/&quot;, pattern = &quot;*.png&quot;, full.names = TRUE)) { process_image(file) } list.files(path = &quot;data/plots/vir-ppis/&quot;, pattern = &quot;*.jpeg&quot;, full.names = T) %&gt;% purrr::map(image_read) %&gt;% image_join() %&gt;% image_animate(fps = 2) %&gt;% image_write(&quot;data/plots/vir-ppis/NYE-2017-2018.gif&quot;) knitr::include_graphics(&quot;data/plots/vir-ppis/NYE-2017-2018.gif&quot;) (#fig:composite_ppi_animation)Animation of VIRs during NYE 2017-2018 "],
["08.Model-fireworks-disturbance.html", "8 Modelling fireworks disturbance 8.1 Setting-up the environment 8.2 Preparing a dataset for modelling 8.3 Determine model resolution based on performance in simple total_biomass model 8.4 Check for correlations among predictors 8.5 Correlated land use proportions 8.6 Determine the most suitable proxy for disturbance 8.7 Determine best hyperparameter settings for lr 8.8 Retraining the disturbance models using optimised learning rate and maximum number of trees 8.9 Relative influence of variables 8.10 Models for seperate land uses 8.11 Partial dependence", " 8 Modelling fireworks disturbance We have so far: Pre-processed the radar data by removing clutter and applying the range-bias correction (Kranstauber et al. 2020). Annotated the PPIs with land use proportions and indicators of disturbance, i.e. distance to inhabited urban areas and (human) population density. Annotated the PPIs with the total biomass calculated from the Sovon counts. With the dataset of annotated PPIs, we can start to explore the relation between fireworks disturbance and the birds measured aloft during NYE 2017-2018. The following parameters we assume are important predictors for measured bird densities aloft: The total biomass of birds on the ground. The take-off habitat of these birds. The human population in the vicinity of birds. The distance to the nearest inhabited urban area. 8.1 Setting-up the environment Load the required packages. library(ggstatsplot) library(ggplot2) library(dplyr) library(readr) library(tidyr) library(gbm) library(dismo) library(ggBRT) library(patchwork) library(pdp) library(parallel) library(ggridges) cpucores &lt;- 6 In the previous chapter, we have created some datasets encompassing all the data contained within the individual PPIs at different resolutions. We will determine the optimal resolution and then continue using this dataset for further modelling. resolutions &lt;- file.path(&quot;data/processed&quot;, c(&quot;all_500m.RDS&quot;, &quot;all_1000m.RDS&quot;, &quot;all_2000m.RDS&quot;)) data &lt;- lapply(resolutions, function(x) readRDS(x)) names(data) &lt;- c(&quot;500m&quot;, &quot;1000m&quot;, &quot;2000m&quot;) 8.2 Preparing a dataset for modelling As we’re mostly interested in the moment of en masse take-off of birds, and we want to limit the effects of dispersal, we will limit our analysis to the first 5 minutes after 00:05 on January 1st, 2018 (or 23:05 on December 31st, 2017 in UTC), as both radar sites (Den Helder and Herwijnen) show a low VIR prior to and a rapid increase in VIR for that period (see Identifying moment of take-off). Making sure birds are thus still sufficiently ‘linked’ to the take-off sites requires that we limit our analysis to only this one scan. Furthermore, we want to make sure that: 1. the area is ‘covered’ by at least 1 radar, 1. we have an estimate of total_biomassfor these sites, 1. the proportion of urban area (urban) in the PPI pixel is less than .25, 1. VIR is &gt; 0 (otherwise log-conversion will return -Inf), so we replace 0-values with 1e-3, 1. the radar beam does not overshoot birds too much based on the vp. dt_start &lt;- as.POSIXct(&quot;2017-12-31 23:05:00&quot;, tz = &quot;UTC&quot;) dt_end &lt;- as.POSIXct(&quot;2017-12-31 23:10:00&quot;, tz = &quot;UTC&quot;) clean_data &lt;- function(data, scan_start, scan_end, max_distance) { mdl_variables &lt;- c(&quot;VIR&quot;, &quot;dist_radar&quot;, &quot;datetime&quot;, &quot;total_biomass&quot;, &quot;urban&quot;, &quot;agricultural&quot;, &quot;semiopen&quot;, &quot;forests&quot;, &quot;wetlands&quot;, &quot;waterbodies&quot;, &quot;dist_urban&quot;, &quot;human_pop&quot;, &quot;disturb_pot&quot;, &quot;pixel&quot;, &quot;coverage&quot;, &quot;class&quot;) log10_variables &lt;- c(&quot;dist_urban&quot;, &quot;human_pop&quot;, &quot;total_biomass&quot;, &quot;dist_urban&quot;, &quot;disturb_pot&quot;) data %&gt;% filter(coverage &gt; 0, class != 1, datetime &gt;= scan_start &amp; datetime &lt; scan_end, total_biomass &gt; 0, dist_radar &lt; max_distance, urban &lt; 0.25) %&gt;% mutate(VIR = replace_na(VIR, 0.001), VIR = log10(VIR), disturb_pot = human_pop / dist_urban, total_biomass = total_biomass / 1000) %&gt;% dplyr::select(all_of(mdl_variables)) %&gt;% filter_all(all_vars(is.finite(.))) %&gt;% identity() -&gt; data_cleaned data_cleaned } data_cleaned &lt;- lapply(data, function(x) clean_data(x, dt_start, dt_end, 66000)) rm(data) 8.3 Determine model resolution based on performance in simple total_biomass model As we want to correct for the influence of total_biomass on the measured response by the radars, we will test the performance of a simple model using just dist_radar to correct for range-biased measurement error and total_biomass for the resolutions we have generated composte PPIs for so far (500m, 1000m and 2000m). The most performant model is then a logical candidate to continue working with. Learning rates are tweaked roughly to force training of the model without 1) hitting a maximum number of trees limit and 2) avoid overfitting. x_vars &lt;- match(c(&quot;dist_radar&quot;, &quot;total_biomass&quot;), colnames(data_cleaned[[1]])) resolution_models &lt;- mcmapply(function(dataset, lr) gbm.step(data = dataset, gbm.y = 1, gbm.x = x_vars, bag.fraction = 0.5, family = &quot;gaussian&quot;, tree.complexity = 2, learning.rate = lr, silent = TRUE, plot.main = FALSE), dataset = data_cleaned, lr = c(0.5, 0.1, 0.01), SIMPLIFY = FALSE, mc.preschedule = FALSE, mc.cores = cpucores) With these models trained, we can now compare their performance. resolution_perf &lt;-ggPerformance(&quot;500m&quot; = resolution_models[[1]], &quot;1000m&quot; = resolution_models[[2]], &quot;2000m&quot; = resolution_models[[3]]) rm(resolution_models) resolution_perf 500m 1000m 2000m Total.Deviance 2.299291e+08 5.525488e+08 1.901589e+08 Residual.Deviance 1.984452e+08 4.549535e+08 1.848898e+08 Correlation 3.924370e-01 5.267161e-01 2.424428e-01 AUC 0.000000e+00 0.000000e+00 0.000000e+00 Per.Expl 1.369288e+01 1.766274e+01 2.770867e+00 cvDeviance 2.366209e+08 5.622911e+08 1.904390e+08 cvCorrelation 1.171754e-01 9.700350e-02 8.823790e-02 cvAUC 0.000000e+00 0.000000e+00 0.000000e+00 cvPer.Expl -2.910379e+00 -1.763173e+00 -1.472999e-01 It is clear that as the resolution increases (in other words: the cellsize decreases) the performance of total_biomass improves substantially, so it makes sense to continue with the 500m resolution data. Additionally, this suggests that dispersion is not really a problematic factor yet, as in that case the performance of a model based on total_biomass should not have been so good using small pixel sizes. data_cleaned &lt;- data_cleaned[[which.max(resolution_perf[&quot;cvPer.Expl&quot;, ])]] 8.4 Check for correlations among predictors We have to see if variables are strongly correlated and thus unfit for being included in the same model, so we calculate Spearman correlation coefficients for all numerical predictors. As this is ecological data, some degree of correlation is of course inevitable for most variables. mdl_variables &lt;- c(&quot;VIR&quot;, &quot;dist_radar&quot;, &quot;datetime&quot;, &quot;total_biomass&quot;, &quot;urban&quot;, &quot;agricultural&quot;, &quot;semiopen&quot;, &quot;forests&quot;, &quot;wetlands&quot;, &quot;waterbodies&quot;, &quot;dist_urban&quot;, &quot;human_pop&quot;, &quot;disturb_pot&quot;, &quot;pixel&quot;, &quot;coverage&quot;, &quot;class&quot;) predictors &lt;- mdl_variables[!mdl_variables %in% c(&quot;VIR&quot;, &quot;datetime&quot;, &quot;pixel&quot;, &quot;x&quot;, &quot;y&quot;, &quot;coverage&quot;, &quot;class&quot;)] corr_radar &lt;- ggcorrmat(data_cleaned, output = &quot;plot&quot;, type = &quot;spearman&quot;, cor.vars = all_of(predictors), colors = c(&quot;#2166AC&quot;, &quot;#F7F7F7&quot;, &quot;#B2182B&quot;)) corr_radar So obviously there is a strong correlation between the different proxies for disturbance and between agricultural and the other land use proportions. For the disturbance proxies it would make sense to determine which is most performant, but for the land use proportions we have to determine if agricultural should be included in the model with these other predictors. 8.5 Correlated land use proportions Let’s see what the distributions of values is for the different land use proportions. data_cleaned %&gt;% dplyr::select(urban, agricultural, semiopen, forests, wetlands, waterbodies) %&gt;% pivot_longer(cols = everything(), names_to = &quot;landuse&quot;, values_to = &quot;value&quot;) %&gt;% ggplot(aes(x = value, y = landuse, fill = stat(x))) + geom_density_ridges_gradient(scale = 1, rel_min_height = 0.0) + scale_fill_viridis_c(name = &quot;LU Prop.&quot;, option = &quot;C&quot;) + labs(title = &quot;Distribution of land use proportions in modelling dataset&quot;) + xlab(&quot;Land use proportion&quot;) + ylab(&quot;Land use class&quot;) As can be expected in The Netherlands, the dominant land use class is clearly agricultural with the most prominent peak at full PPI pixel coverage (values close to 1) and all other land use classes peaking very strongly at low proportions. This essentially means that when one of the other land use classes increases in proportion, this will almost always come at the cost of the proportion of agricultural and vice versa. So, using all these land use proportions in the model will be uninformative, and will blow up our uncertainty estimates after bootstrapping. Therefore, we will not train a model containing both agricultural and one of the other land use classes. 8.6 Determine the most suitable proxy for disturbance As could be expected, the three ‘disturbance parameters’ (dist_urban, human_pop, disturb_pot) are highly correlated, so we should see which of these is most suitable to include in our model, as it makes no sense to include all. To assess that, we will simply compare the performance of each of the models trained with a single disturbance parameter. lr &lt;- 0.05 max_trees &lt;- nrow(data_cleaned) * 0.25 silence &lt;- TRUE base_model &lt;- c(&quot;dist_radar&quot;, &quot;total_biomass&quot;, &quot;urban&quot;, &quot;semiopen&quot;, &quot;forests&quot;, &quot;wetlands&quot;, &quot;waterbodies&quot;) disturbance_proxies &lt;- c(&quot;dist_urban&quot;, &quot;human_pop&quot;, &quot;disturb_pot&quot;) x_vars &lt;- lapply(disturbance_proxies, function(x) match(c(base_model,x), colnames(data_cleaned))) names(x_vars) &lt;- disturbance_proxies disturbance_models &lt;- mclapply(x_vars, function(x) gbm.step(data = data_cleaned, gbm.y = 1, gbm.x = x, bag.fraction = 0.5, family = &quot;gaussian&quot;, tree.complexity = 2, learning.rate = lr, silent = silence, max.trees = max_trees, step.size = 50, plot.main = FALSE), mc.cores = cpucores, mc.preschedule = FALSE) names(disturbance_models) &lt;- disturbance_proxies saveRDS(disturbance_models, file = &quot;data/models/brt_models_disturbance.RDS&quot;) Let’s compare the performance: disturbance_models &lt;- readRDS(&quot;data/models/brt_models_disturbance.RDS&quot;) disturb_perf &lt;- ggPerformance(&quot;dist_urban&quot; = disturbance_models[[1]], &quot;human_pop&quot; = disturbance_models[[2]], &quot;disturb_pot&quot; = disturbance_models[[3]]) disturb_perf dist_urban human_pop disturb_pot Total.Deviance 3.3009830 3.3009830 3.3009830 Residual.Deviance 0.8799697 0.8397164 0.8615670 Correlation 0.8579778 0.8652355 0.8616432 AUC 0.0000000 0.0000000 0.0000000 Per.Expl 73.3421913 74.5616254 73.8996853 cvDeviance 1.3237012 1.3664894 1.3856431 cvCorrelation 0.7741624 0.7660149 0.7623042 cvAUC 0.0000000 0.0000000 0.0000000 cvPer.Expl 59.8997888 58.6035604 58.0233201 We have a clear winner here: dist_urban performs the best, so we’ll continue with that proxy for disturbance. best_disturbance_proxy &lt;- names(which.max(disturb_perf[&quot;cvPer.Expl&quot;, ])) 8.7 Determine best hyperparameter settings for lr To avoid overfitting to noise, we will limit the maximum nr of trees used in the model to 25% of the number of datapoints, which yields a possible 472 trees to be used. We still want to make sure our model reaches some measure of optimality (in this case a reduction of cross-validated deviance explained), we have to do a hyperparameter search for a suitable combination of maximum number of trees and learning rate. See Elith et al. for an explanation on the linkage between number of trees and learning rate. learning_rates &lt;- c(0.1, 0.15, 0.2, 0.3, 0.4, 0.5) # Lower and higher learning rates certainly do not converge quick enough or increase error hyperparms &lt;- mclapply(learning_rates, function(x) { gbm.step(data = data_cleaned, gbm.y = 1, gbm.x = x_vars[[best_disturbance_proxy]], bag.fraction = 0.5, family = &quot;gaussian&quot;, plot.main = FALSE, tree.complexity = 2, learning.rate = x, max.trees = max_trees, step.size = 50, silent = TRUE)}, mc.preschedule = FALSE, mc.cores = 3) saveRDS(hyperparms, file = &quot;data/models/brt_models_hyperparms.RDS&quot;) hyperparms &lt;- readRDS(file = &quot;data/models/brt_models_hyperparms.RDS&quot;) max_used_trees &lt;- max(unlist(lapply(hyperparms, function(x) {if (!is.null(x)) { max(x$trees.fitted) }}))) trees &lt;- data.frame(trees = seq(from = 50, to = max_used_trees * 2, by = 50)) holdout_deviance &lt;- lapply(hyperparms, function(x) { if (!is.null(x)) { x$cv.values }} ) missing &lt;- which(unlist(lapply(holdout_deviance, is.null))) h &lt;- matrix(data = NA, nrow = dim(trees)[1], ncol = length(holdout_deviance)) for (i in seq_along(holdout_deviance)) { h[1:length(holdout_deviance[[i]]), i] &lt;- holdout_deviance[[i]] } h &lt;- as.data.frame(h) hp &lt;- data.frame(trees, h) colnames(hp) &lt;- c(colnames(trees), learning_rates) hp %&gt;% pivot_longer(cols = c(-trees), names_to = &quot;lr&quot;, values_to = &quot;holdout_deviance&quot;) %&gt;% identity() -&gt; hp ggplot(hp) + geom_line(aes(x = trees, y = holdout_deviance, color = lr), size = 1) ## Warning: Removed 1004 row(s) containing missing values (geom_path). It seems a learning rate of 0.3 is a good balance between speedy conversion and model performance. We can now retrain our most performant disturbance model with these new parameters. 8.8 Retraining the disturbance models using optimised learning rate and maximum number of trees We can now retrain the models with the best combination of learning rate and maximum number of trees. lr &lt;- 0.3 max_trees &lt;- nrow(data_cleaned) * 0.25 silence &lt;- TRUE disturbance_models &lt;- mclapply(x_vars, function(x) gbm.step(data = data_cleaned, gbm.y = 1, gbm.x = x, bag.fraction = 0.5, family = &quot;gaussian&quot;, tree.complexity = 2, learning.rate = lr, silent = silence, max.trees = max_trees, step.size = 50, plot.main = FALSE, verbose = FALSE), mc.preschedule = FALSE, mc.cores = cpucores) names(disturbance_models) &lt;- disturbance_proxies saveRDS(disturbance_models, file = &quot;data/models/brt_models_disturbance.RDS&quot;) And once again we can compare the performance disturbance_models &lt;- readRDS(&quot;data/models/brt_models_disturbance.RDS&quot;) ggPerformance(&quot;dist_urban&quot; = disturbance_models[[1]], &quot;human_pop&quot; = disturbance_models[[2]], &quot;disturb_pot&quot; = disturbance_models[[3]]) dist_urban human_pop disturb_pot Total.Deviance 3.3009830 3.3009830 3.3009830 Residual.Deviance 0.8799697 0.8397164 0.8615670 Correlation 0.8579778 0.8652355 0.8616432 AUC 0.0000000 0.0000000 0.0000000 Per.Expl 73.3421913 74.5616254 73.8996853 cvDeviance 1.3237012 1.3664894 1.3856431 cvCorrelation 0.7741624 0.7660149 0.7623042 cvAUC 0.0000000 0.0000000 0.0000000 cvPer.Expl 59.8997888 58.6035604 58.0233201 So the model with the disturbance proxy of dist_urban continues to perform best and we’ll save it for future use. model_disturbance_best &lt;- disturbance_models[best_disturbance_proxy] saveRDS(model_disturbance_best, file = &quot;data/models/brt_models_disturbance_best.RDS&quot;) 8.9 Relative influence of variables We can now also compare the relative influence of the variables used in this model. relinf &lt;- ggInfluence(disturbance_models$dist_urban, show.signif = FALSE, main = &quot;Relative influence of predictor variables on log(VIR)&quot;) relinf rel.inf dist_radar 37.281624 total_biomass 23.891639 dist_urban 10.926148 forests 7.512595 waterbodies 7.397746 urban 6.188727 semiopen 4.672409 wetlands 2.129112 The high relative importance of total_biomass (23.9%) suggests a clear link between birds counted on the ground (by Sovon’s observers) and what is measured aloft, exactly what one would expect if birds are still sufficiently ‘tied’ to their take-off habitat. 8.10 Models for seperate land uses As the land use proportions are correlated, and agricultural land use is by far the most common in The Netherlands, it may be a worthwhile exercise to model the effects of each land use proportion seperately. This way, we may discover effects that disappear because of the compound effects of correlation among the other predictors. lr &lt;- 0.3 silence &lt;- TRUE base_model &lt;- c(&quot;dist_radar&quot;, &quot;total_biomass&quot;, best_disturbance_proxy) landscapes &lt;- c(&quot;urban&quot;, &quot;agricultural&quot;, &quot;semiopen&quot;, &quot;forests&quot;, &quot;wetlands&quot;, &quot;waterbodies&quot;) x_vars &lt;- lapply(landscapes, function(x) match(c(base_model,x), colnames(data_cleaned))) landscape_models &lt;- mclapply(x_vars, function(x) gbm.step(data = data_cleaned, gbm.y = 1, gbm.x = x, bag.fraction = 0.5, family = &quot;gaussian&quot;, tree.complexity = 2, learning.rate = lr, silent = silence, max.trees = max_trees, plot.main = FALSE), mc.preschedule = FALSE, mc.cores = 3) names(landscape_models) &lt;- c(landscapes) saveRDS(landscape_models, file = &quot;data/models/brt_models_landscapes.RDS&quot;) And once again we compare the performance landscape_models &lt;- readRDS(&quot;data/models/brt_models_landscapes.RDS&quot;) ggPerformance(&quot;urban&quot; = landscape_models[[1]], &quot;agricultural&quot; = landscape_models[[2]], &quot;semiopen&quot; = landscape_models[[3]], &quot;forests&quot; = landscape_models[[4]], &quot;wetlands&quot; = landscape_models[[5]], &quot;waterbodies&quot; = landscape_models[[6]]) urban agricultural semiopen forests wetlands waterbodies Total.Deviance 3.3009830 3.3009830 3.3009830 3.3009830 3.3009830 3.3009830 Residual.Deviance 0.9398659 0.9156381 0.9476384 0.9485355 0.9715621 0.9403277 Correlation 0.8475936 0.8514734 0.8457678 0.8462161 0.8416190 0.8471955 AUC 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 Per.Expl 71.5276976 72.2616530 71.2922361 71.2650584 70.5674913 71.5137062 cvDeviance 1.3719790 1.3070784 1.3213059 1.3510535 1.3368846 1.3466620 cvCorrelation 0.7645865 0.7772946 0.7744176 0.7684092 0.7713873 0.7693182 cvAUC 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 cvPer.Expl 58.4372588 60.4033582 59.9723497 59.0711774 59.5004107 59.2042124 With these models trained as well, we will save the final set of models. models &lt;- c(disturbance_models, landscape_models) saveRDS(models, file = &quot;data/models/brt_models.RDS&quot;) 8.11 Partial dependence Partial dependence plots (PDP) can visualise the modelled marginal effect a feature has on the predicted output of a model. For exploratory purposes, we will create a quick visualisation of the PDPs for the trained model, before we start with a bootstrapping procedure to test the robustness of these results. plot_pdp &lt;- function(model, predictor) { p &lt;- partial(model, train = model$gbm.call$dataframe, pred.var = predictor, type = &quot;regression&quot;, plot = TRUE, plot.engine =&quot;ggplot2&quot;, smooth = TRUE, rug = TRUE, n.trees = model$n.trees) p } modelnames &lt;- names(models) i &lt;- 1 for (model in models) { relinf &lt;- ggInfluence(model, plot = FALSE) plots &lt;- lapply(rownames(relinf), function(x) { plot_pdp(model = model, predictor = x) + xlab(paste(x, &quot; (&quot;, round(relinf[x, 1]), &quot;%)&quot;, sep = &quot;&quot;)) + ylab(&quot;log(VIR)&quot;) }) print(wrap_plots(plots) + plot_annotation(title = modelnames[i])) i &lt;- i + 1 } References "],
["09.Quantifying-model-uncertainty.html", "9 Quantifying model uncertainty 9.1 Setting-up the environment 9.2 Load the constructed models 9.3 Determine evaluation grid 9.4 Bootstrapped retraining of gbm models 9.5 Plot bootstrapped uncertainties 9.6 Show model residuals 9.7 Leaflet alternative", " 9 Quantifying model uncertainty The partial dependence plots generated in the previous chapter visualise the marginal effect of certain predictors on the outcome log(VIR). We use a bootstrapping approach to quantify the model uncertainty. 9.1 Setting-up the environment library(gbm) library(dismo) library(ggBRT) library(parallel) library(ggdist) library(pdp) library(tibble) library(dplyr) library(tidyr) library(forcats) library(patchwork) library(leaflet) library(leafem) library(leaflet.opacity) 9.2 Load the constructed models all_models &lt;- readRDS(&quot;data/models/brt_models.RDS&quot;) cpucores &lt;- 3 We are particularly interested in two models: One containing agricultural as predictor One containing the other land use class proportions as predictors. So we’ll extract these and ignore the rest in the following steps. models &lt;- all_models[c(&quot;dist_urban&quot;, &quot;agricultural&quot;)] rm(all_models) 9.3 Determine evaluation grid In the previous chapter we have seen that the partial dependence plots are difficult to interpret when the data is not distributed evenly across the entire range. A good example of this is the proportion of an area covered by forests: this is always never close to 1 in The Netherlands, as forests are comparatively rare habitats. Instead of bootstrapping estimates for the entire domain of variable values, we will thus limit it by the central 90% of the data. The calculation of the evaluation grid is based off of the plot.gbm.4list() function, originally written by Elith &amp; Leathwick ((???): Add ref), included in ggBRT, so we can directly plug it in the bootstrapping functions provided by the same package. calculate_evaluation_grid &lt;- function(model, qlim = c(0.05, 0.95), continuous.resolution = 100) { variables &lt;- model$var.names grid &lt;- vector(&quot;list&quot;, length(variables)) for (i in seq_along(variables)) { if (typeof(qlim) == &quot;list&quot;) { quantiles &lt;- quantile(model$gbm.call$dataframe[, variables[i]], as.numeric(unlist(qlim[variables[i]]))) } else { quantiles &lt;- quantile(model$gbm.call$dataframe[, variables[i]], qlim) } grid[[i]] &lt;- expand.grid(seq(from = quantiles[1], to = quantiles[2], length.out = continuous.resolution)) colnames(grid[[i]]) &lt;- paste(&quot;X&quot;, i, sep = &quot;&quot;) } grid } qlims &lt;- list(&quot;dist_radar&quot; = c(0, 1), &quot;total_biomass&quot; = c(0.05, 0.95), &quot;dist_urban&quot; = c(0.05, 0.95), &quot;human_pop&quot; = c(0.05, 0.95), &quot;disturb_pot&quot; = c(0.05, 0.95), &quot;agricultural&quot; = c(0, 1), &quot;urban&quot; = c(0, 1), &quot;semiopen&quot; = c(0, 1), &quot;forests&quot; = c(0, 1), &quot;wetlands&quot; = c(0, 1), &quot;waterbodies&quot; = c(0, 1)) grids &lt;- lapply(models, function(x) calculate_evaluation_grid(x, qlim = qlims, continuous.resolution = 1000)) 9.4 Bootstrapped retraining of gbm models Using the bootstrapping procedure provied in ggBRT we can derive uncertainty estimates from the trained models. source(&quot;R/gbm.bootstrap.functions.modified.R&quot;) bootstraps &lt;- parallel::mcmapply(function(m, g) gbm.bootstrap.functions.modified(m, list.predictors = g, n.reps = 200, n.divisions = 1000), m = models, g = grids, SIMPLIFY = FALSE, mc.cores = cpucores, mc.preschedule = TRUE) saveRDS(bootstraps, file = &quot;data/models/brt_models_bootstrap.RDS&quot;) As we bootstrapped the relative importance of the model predictors, we can visualise the distributions of these values as follows: bootstraps &lt;- readRDS(&quot;data/models/brt_models_bootstrap.RDS&quot;) relinf_uncertainty &lt;- function(bootstrap, modelname) { as.data.frame(bootstrap$rel.infs) %&gt;% rownames_to_column(var = &quot;predictor&quot;) %&gt;% pivot_longer(-predictor, names_to = &quot;bootstrap_sample&quot;) -&gt; bootstrap_long bootstrap_long %&gt;% pivot_wider(names_from = predictor, values_from = value) %&gt;% median_qi(.width = c(0.5), .exclude = &quot;bootstrap_sample&quot;) %&gt;% dplyr::select(-contains(&quot;.&quot;)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;predictor&quot;, values_to = &quot;bootstrap_median&quot;) -&gt; predictor_summaries predictor_summaries[order(-predictor_summaries$bootstrap_median), ] %&gt;% rowid_to_column(var = &quot;rank&quot;) %&gt;% dplyr::select(predictor, rank) %&gt;% mutate(rank = fct_rev(as.factor(rank))) %&gt;% mutate(predictor_long = case_when( predictor == &quot;dist_radar&quot; ~ &quot;Distance from radar&quot;, predictor == &quot;total_biomass&quot; ~ &quot;Bird biomass&quot;, predictor == &quot;dist_urban&quot; ~ &quot;Distance to urban area&quot;, predictor == &quot;agricultural&quot; ~ &quot;Prop. agricultural&quot;, predictor == &quot;urban&quot; ~ &quot;Prop. urban&quot;, predictor == &quot;semiopen&quot; ~ &quot;Prop. semiopen&quot;, predictor == &quot;forests&quot; ~ &quot;Prop. forests&quot;, predictor == &quot;wetlands&quot; ~ &quot;Prop. wetlands&quot;, predictor == &quot;waterbodies&quot; ~ &quot;Prop. waterbodies&quot; )) %&gt;% identity() -&gt; predictor_summaries bootstrap_long %&gt;% left_join(predictor_summaries, by = &quot;predictor&quot;) -&gt; relinf_uncertainty relinf_uncertainty %&gt;% ggplot(aes(y = rank, x = value)) + stat_eye(point_interval = median_qi, .width = c(0.95, 0.5)) + # Thinnest black bar represents 95%, other 50% and point = median scale_y_discrete(labels = rev(predictor_summaries$predictor_long)) + labs(x = &quot;Relative influence (%)&quot;, y = &quot;Predictor&quot;, title = &quot;Relative influence of predictors on outcome variable: log(VIR)&quot;, subtitle = paste(unique(bootstrap$gbm.call$dataframe$datetime))) -&gt; p ggsave(filename = paste0(&quot;data/plots/relative-influence/&quot;, modelname, &quot;.png&quot;), width = 7, height = 5) list(relinf_uncertainty, p) } relinf_uncertainties &lt;- mapply(relinf_uncertainty, bootstraps, names(bootstraps), SIMPLIFY = FALSE) relinf_uncertainties[[1]][[2]] relinf_uncertainties[[2]][[2]] The large ‘spread’ of agricultural suggests something strange is going on there. I would imagine this is quite likely to be the result of some retained ground clutter with unusually high reflectivities that affects these outcomes. Additionally, ‘agriculture’ is the baseline land use class of the dataset, so resampling during bootstrapping is quite likely to sample 9.5 Plot bootstrapped uncertainties bootstrap_uncertainty &lt;- function(predictor, boot, grid, model, confidence.intervals = c(0.95, 0.8), central.measure = mean, plot = FALSE, loess = TRUE, loess.span = 0.25, rug = TRUE, rug.limits = c(0, 1), qlim = c(0.05, 0.95), ylab = &quot;log(VIR)&quot;) { k &lt;- match(predictor, boot$gbm.call$predictor.names) # Gather x values from evaluation grid x &lt;- as.data.frame(grid[[k]]) colnames(x) &lt;- &quot;x&quot; # Calculate quantiles probs &lt;- c(sapply(confidence.intervals, function(x) 1 - x), confidence.intervals) quantiles &lt;- t(apply(boot$function.preds[, k, ], 1, quantile, probs)) colnames(quantiles) &lt;- paste(&quot;q&quot;, gsub(&quot;%&quot;, &quot;&quot;, colnames(quantiles)), sep = &quot;&quot;) quantiles &lt;- as.data.frame(quantiles) # Calculate measure of central tendency central &lt;- as.data.frame(apply(boot$function.preds[, k, ], 1, central.measure)) colnames(central) &lt;- &quot;central_measure&quot; # Calculate partial dependence grid_df &lt;- as.data.frame(grid) colnames(grid_df) &lt;- model$var.names grid_df &lt;- grid_df[, predictor, drop = FALSE] model_pdp &lt;- partial(model, train = model$gbm.call$dataframe, pred.var = predictor, type = &quot;regression&quot;, n.trees = model$n.trees, pred.grid = grid_df)[&quot;yhat&quot;] # Smooth if (loess) { variables &lt;- c(colnames(quantiles), colnames(central), colnames(model_pdp)) unsmoothed &lt;- data.frame(x, quantiles, central, model_pdp) loess.smooth &lt;- function(x, span) { predict(loess(formula = paste(x, &quot;x&quot;, sep = &quot;~&quot;), data = unsmoothed, span = loess.span)) } smoothed &lt;- as.data.frame(lapply(variables, loess.smooth, span = loess.span), col.names = variables) uncertainty &lt;- data.frame(x, smoothed) } else { uncertainty &lt;- data.frame(x, quantiles, central, model_pdp) } if (plot) { p &lt;- ggplot(uncertainty) i &lt;- 1 sorted_confints &lt;- sort(confidence.intervals, decreasing = TRUE) for (ci in sorted_confints) { colors &lt;- factor(sorted_confints) ymax &lt;- as.name(paste(&quot;q&quot;, ci * 100, sep = &quot;&quot;)) ymax &lt;- enquo(ymax) ymin &lt;- as.name(paste(&quot;q&quot;, (1 - ci) * 100, sep = &quot;&quot;)) ymin &lt;- enquo(ymin) p &lt;- p + geom_ribbon(aes(x = x, ymin = !!ymin, ymax = !!ymax, fill = !!colors[i]), alpha = 1) i &lt;- i + 1 } p &lt;- p + scale_fill_manual(name = &quot;CI&quot;, values = c(&quot;#cccccc&quot;, &quot;#f7f7f7&quot;, &quot;#969696&quot;)) p &lt;- p + geom_line(aes(x = x, y = central_measure, color = &quot;central_measure&quot;)) # geom_line(aes(x = x, y = central_measure), color = &quot;#252525&quot;) p &lt;- p + # geom_line(data = uncertainty, aes(x = x, y = yhat), colour = &quot;#2b8cbe&quot;, size = 2) + geom_line(data = uncertainty, aes(x = x, y = yhat, color = &quot;yhat&quot;), size = 2) + geom_hline(yintercept = mean(boot$function.preds[, k, ]), linetype = &quot;dashed&quot;, color = &quot;darkgrey&quot;) p &lt;- p + scale_color_manual(&quot;&quot;, values = c(&quot;central_measure&quot; = &quot;#252525&quot;, &quot;yhat&quot; = &quot;#2b8cbe&quot;), labels = c(&quot;Median&quot;, &quot;Final model&quot;)) rug &lt;- as.data.frame(quantile(model$gbm.call$dataframe[, predictor], seq(from = rug.limits[1], to = rug.limits[2], by = 0.1))) colnames(rug) &lt;- &quot;x&quot; p &lt;- p + geom_rug(data = rug, aes(x = x)) minmax &lt;- quantile(model$gbm.call$dataframe[, predictor], qlim) vals &lt;- model$gbm.call$dataframe[model$gbm.call$dataframe[, predictor] &gt;= minmax[1] &amp; model$gbm.call$dataframe[, predictor] &lt;= minmax[2], predictor] vals &lt;- data.frame(vals) colnames(vals) &lt;- &quot;x&quot; # p &lt;- p + # stat_density(data = vals, aes(x = x, y = ..scaled..), geom = &quot;line&quot;) if (predictor %in% c(&quot;agricultural&quot;, &quot;urban&quot;, &quot;semiopen&quot;, &quot;forests&quot;, &quot;wetlands&quot;, &quot;waterbodies&quot;)) { xlabel &lt;- paste(&quot;Prop.&quot;, predictor) } else { xlabel_lookup &lt;- c(&quot;dist_urban&quot; = &quot;Distance to urban area (m)&quot;, &quot;human_pop&quot; = &quot;Human inhabitants (#)&quot;, &quot;disturb_pot&quot; = &quot;Disturbance potential (-)&quot;, &quot;total_biomass&quot; = &quot;Bird biomass (kg)&quot;, &quot;dist_radar&quot; = &quot;Distance from radar (m)&quot;) xlabel &lt;- xlabel_lookup[predictor] } p &lt;- p + xlab(xlabel) + ylab(ylab) p &lt;- p + theme(legend.key = element_rect(colour = &quot;black&quot;)) p } else { uncertainty } } plot_model_pdp &lt;- function(model, bootstrap, grid, relinf_uncertainty, loess = TRUE, loess.span = 0.4) { relinf &lt;- relinf_uncertainty[[1]] %&gt;% group_by(predictor) %&gt;% mutate(mean_relinf = mean(value)) %&gt;% dplyr::select(predictor, mean_relinf, rank) %&gt;% distinct() %&gt;% identity() p &lt;- lapply(model$var.names, function(x) { if (x %in% c(&quot;total_biomass&quot;, &quot;dist_urban&quot;, &quot;human_pop&quot;, &quot;disturb_pot&quot;)) { bootstrap_uncertainty(x, bootstrap, grid, model, confidence.intervals = c(0.95, 0.8), plot = TRUE, central.measure = median, loess.span = loess.span, loess = loess, rug.limits = c(0.05, 0.95)) } else { bootstrap_uncertainty(x, bootstrap, grid, model, confidence.intervals = c(0.95, 0.8), plot = TRUE, central.measure = median, loess.span = loess.span, loess = loess, rug.limits = c(0, 1)) } }) p } a &lt;- mapply(function(model, bootstrap, grid, relinf_uncertainty, name) { plots &lt;- plot_model_pdp(model, bootstrap, grid, relinf_uncertainty, loess = TRUE, loess.span = 0.4) # print(name) p &lt;- wrap_plots(plots, guides = &quot;collect&quot;) + guide_area() + plot_annotation(title = &quot;Partial dependence plots&quot;, subtitle = &quot;The effect of individual predictors while integrating all others,\\nwith bootstrapped confidence intervals and median&quot;) ggsave(filename = paste0(&quot;data/plots/partial-effects-new/&quot;, name, &quot;.png&quot;), width = 10, height = 10) print(p) }, model = models, bootstrap = bootstraps, grid = grids, relinf_uncertainty = relinf_uncertainties, name = names(models)) 9.6 Show model residuals df &lt;- models[[1]]$gbm.call$dataframe df$preds &lt;- predict(models[[1]], n.trees = models[[1]]$n.trees) df$resid &lt;- residuals(models[[1]]) ppi &lt;- readRDS(&quot;data/processed/composite-ppis/500m/201712312305.RDS&quot;) ppi$data@data %&gt;% left_join(dplyr::select(df, pixel, preds, resid), by = &quot;pixel&quot;) -&gt; ppi$data@data ppi$data@data$VIR_log &lt;- log10(ppi$data@data$VIR) ppi$data@data$total_biomass_log &lt;- log10(ppi$data@data$total_biomass / 1000) 9.7 Leaflet alternative pal_resid &lt;- colorspace::diverging_hcl(50, &quot;Blue-Red 3&quot;, power = 2) # z_lims_resid &lt;- c(-2, 2) z_lims_resid &lt;- c(-4, 4) palette_resid &lt;- colorNumeric(pal_resid, na.color = NA, domain = z_lims_resid) raster_resid &lt;- as(ppi$data[&quot;resid&quot;], &quot;RasterLayer&quot;) raster_vir &lt;- as(ppi$data[&quot;VIR_log&quot;], &quot;RasterLayer&quot;) z_lims_vir &lt;- c(0, 6) pal_vir &lt;- &quot;viridis&quot; palette_vir &lt;- colorNumeric(pal_vir, na.color = NA, domain = z_lims_vir) leaflet() %&gt;% addTiles(group = &quot;OSM (default)&quot;) %&gt;% # addProviderTiles(provider = providers$CartoDB.DarkMatter, group = &quot;CartoDB DarkMatter&quot;) %&gt;% addRasterImage(raster_resid, colors = palette_resid, layerId = &quot;resid&quot;, group = &quot;resid&quot;) %&gt;% addImageQuery(raster_resid, layerId = &quot;resid&quot;) %&gt;% addLegend(pal = palette_resid, values = z_lims_resid) %&gt;% # addRasterImage(raster_vir, colors = palette_vir, layerId = &quot;VIR&quot;, group = &quot;VIR&quot;) %&gt;% # addLegend(pal = palette_vir, values = z_lims_vir) %&gt;% addOpacitySlider(layerId = &quot;resid&quot;) %&gt;% # addLowerOpacity(layerId = &quot;VIR&quot;) %&gt;% # addLayersControl( # baseGroups = c(&quot;OSM (default)&quot;, &quot;CartoDB DarkMatter&quot;), # overlayGroups = c(&quot;resid&quot;, &quot;VIR&quot;), # position = &quot;topleft&quot;) %&gt;% identity() pal_resid &lt;- colorspace::diverging_hcl(50, &quot;Blue-Red 3&quot;, power = 2) z_lims_resid &lt;- c(-2, 2) # z_lims_resid &lt;- c(-4, 4) palette_resid &lt;- colorBin(pal_resid, na.color = NA, domain = z_lims_resid, bins = 50) raster_resid &lt;- as(ppi$data[&quot;resid&quot;], &quot;RasterLayer&quot;) raster_vir &lt;- as(ppi$data[&quot;VIR_log&quot;], &quot;RasterLayer&quot;) z_lims_vir &lt;- c(0, 6) pal_vir &lt;- &quot;viridis&quot; palette_vir &lt;- colorNumeric(pal_vir, na.color = NA, domain = z_lims_vir) leaflet() %&gt;% addTiles(group = &quot;OSM (default)&quot;) %&gt;% # addProviderTiles(provider = providers$CartoDB.DarkMatter, group = &quot;CartoDB DarkMatter&quot;) %&gt;% addRasterImage(raster_resid, colors = palette_resid, layerId = &quot;resid&quot;, group = &quot;resid&quot;) %&gt;% addImageQuery(raster_resid, layerId = &quot;resid&quot;) %&gt;% addLegend(pal = palette_resid, values = z_lims_resid) %&gt;% # addRasterImage(raster_vir, colors = palette_vir, layerId = &quot;VIR&quot;, group = &quot;VIR&quot;) %&gt;% # addLegend(pal = palette_vir, values = z_lims_vir) %&gt;% addOpacitySlider(layerId = &quot;resid&quot;) %&gt;% # addLowerOpacity(layerId = &quot;VIR&quot;) %&gt;% # addLayersControl( # baseGroups = c(&quot;OSM (default)&quot;, &quot;CartoDB DarkMatter&quot;), # overlayGroups = c(&quot;resid&quot;, &quot;VIR&quot;), # position = &quot;topleft&quot;) %&gt;% identity() ## Warning in colors(.): Some values were outside the color scale and will be ## treated as NA Random forests and stochastic gradient boosting for predicting tree canopy cover: comparing tuning processes and model performance Freeman, Moisen, Coulston &amp; Wilson 2015 Environmental Predictability as a Cause and Consequence of Animal Movement "],
["A1.Generating-VPs-for-Den-Helder-radar.html", "10 Generating vertical profiles for Den Helder radar", " 10 Generating vertical profiles for Den Helder radar The Den Helder radar is situated very close to the coast, so take-off densities derived from the calculate_vp() function in bioRad (Dokter et al. 2019) are likely underestimated as large swaths of sea (both North and Wadden Sea) are contained within the volume. To correct for this, we will select a section, defined by minimum and maximum azimuths, to generate the vertical profiles for. We load the DBZH from the polar volume containing the peak moment of take-off for the Den Helder radar, which occurs at 23:05 UTC. library(bioRad) pvol_path &lt;- &quot;data/raw/pvol/fireworks-2017-2018/RAD_NL61_VOL_NA_201712312305_ODIM.h5&quot; pvol &lt;- read_pvolfile(pvol_path, param = &quot;DBZH&quot;) To illustrate the problem, let’s plot the lowest scan of the pvol we have loaded: scan &lt;- get_scan(pvol, 0.3) plot(scan) As can be seen, there is a large swath of sea clutter, roughly between azimuths 200 and 325. The Wadden Sea can be seen from azimuths 45 until roughly 90. The area in between is where the majority of birds take off: the mainland of North Holland. We can visualise what a focus on this area would cover by plotting a PPI where all values between azimuths 90 and 200 have been set to a very high value. See below: scan_section &lt;- scan scan_section$params$DBZH[, 90:200] &lt;- 100 ppi &lt;- project_as_ppi(scan, grid_size = 100, range_max = 35000) ppi_section &lt;- project_as_ppi(scan_section, grid_size = 100, range_max = 35000) par(pty = &quot;s&quot;, mfrow = c(1, 2)) plot(ppi) plot(ppi_section) Now with that in mind we can calculate the vps for the entire area and compare that with one that is calculated just from the section above (azimuths between 90 and 200). vp_all_azimuths = calculate_vp(pvol_path, verbose = FALSE) vp_land_based_azimuths = calculate_vp(pvol_path, azim_min = 90, azim_max = 200, verbose = FALSE) And plot the corresponding VPs: plot(vp_all_azimuths, main = &quot;VP calculated from the entire Den Helder radar domain&quot;) plot(vp_land_based_azimuths, main = &quot;VP calculated from the main land area covered by Den Helder radar&quot;) We can see there is a substantial difference in density derived from the VPs when focussing on the main land of North Holland vs. when we look at the entire radar domain and include large swaths of the North and Wadden Sea. References "],
["References.html", "11 References", " 11 References "]
]
