# Quantifying model uncertainty

The partial dependence plots generated in the previous chapter visualise the marginal effect of certain predictors on the outcome `log(VIR)`. We use a bootstrapping approach to quantify the model uncertainty.

## Setting-up the environment
```{r}
library(gbm)
library(dismo)
library(ggBRT)
library(parallel)
library(ggdist)
library(pdp)
library(tibble)
library(dplyr)
library(tidyr)
library(forcats)
```


## Load the constructed models
```{r}
models <- readRDS("data/models/brt_models.RDS")
```

## Determine evaluation grid

In the previous chapter we have seen that the partial dependence plots are difficult to interpret when the data is not distributed evenly across the entire range. A good example of this is the proportion of an area covered by `forests`: this is always never close to `1` in The Netherlands, as forests are comparatively rare habitats. Instead of bootstrapping estimates for the entire domain of variable values, we will thus limit it by the central 90% of the data. The calculation of the evaluation grid is based off of the `plot.gbm.4list()` function, originally written by Elith & Leathwick (@TODO: Add ref), included in `ggBRT`, so we can directly plug it in the bootstrapping functions provided by the same package.

```{r}
calculate_evaluation_grid <- function(model, qlim = c(0.05, 0.95), continuous.resolution = 100) {
  variables <- model$var.names
  grid <- vector("list", length(variables))
  for (i in seq_along(variables)) {
    if (typeof(qlim) == "list") {
      quantiles <- quantile(model$gbm.call$dataframe[, variables[i]], as.numeric(unlist(qlim[variables[i]])))
    } else {
      quantiles <- quantile(model$gbm.call$dataframe[, variables[i]], qlim)
    }
    grid[[i]] <- expand.grid(seq(from = quantiles[1], to = quantiles[2], length.out = continuous.resolution))
    colnames(grid[[i]]) <- paste("X", i, sep = "")
  }
  grid
}

qlims <- list(c(0, 1), c(0.05, 0.95), c(0, 1), c(0, 1), c(0, 1), c(0, 1), c(0, 1), c(0, 1), c(0.05, 0.95))
names(qlims) <- models[[1]]$var.names

grids <- lapply(models, function(x) calculate_evaluation_grid(x, qlim = qlims, continuous.resolution = 10000))
```


```{r}
bootstraps <- mcmapply(function(m, g) gbm.bootstrap.functions.modified(m, list.predictors = g, n.reps = 2), m = models, g = grids, SIMPLIFY = FALSE)
saveRDS(bootstraps, file = "data/models/brt_models_bootstrap.RDS")
```

```{r}
as.data.frame(bootstraps[[2]]$rel.infs) %>% 
  rownames_to_column(var = "predictor") %>% 
  pivot_longer(-predictor, names_to = "bootstrap_sample") -> bootstrap_long

bootstrap_long %>%
  pivot_wider(names_from = predictor, values_from = value) %>%
  median_qi(.width = c(0.5), .exclude = "bootstrap_sample") %>%
  select(-contains(".")) %>%
  pivot_longer(cols = everything(), names_to = "predictor", values_to = "bootstrap_median") -> predictor_summaries

predictor_summaries[order(-predictor_summaries$bootstrap_median), ] %>% 
  rowid_to_column(var = "rank") %>% 
  select(predictor, rank) %>%
  mutate(rank = fct_rev(as.factor(rank))) -> predictor_summaries

bootstrap_long %>%
  left_join(predictor_summaries, by = "predictor") %>%
  ggplot(aes(y = rank, x = value)) + 
  stat_eye(point_interval = median_qi, .width = c(0.90, 0.5)) +  # Thinnest black bar represents 95%, other 50% and point = median
  scale_y_discrete(labels = rev(predictor_summaries$predictor)) + 
  labs(x = "Relative influence (%)",
       y = "Predictor", 
       title = "") +
  coord_cartesian(xlim = c(0, 30), expand = FALSE)
```
