# (PART) Fireworks disturbance {-}

# Modelling fireworks disturbance

We have so far:

1. Pre-processed the radar data by removing clutter and applying the range-bias correction [@kranstauber2020].
2. Annotated the PPIs with land use proportions and indicators of disturbance, i.e. distance to inhabited urban areas and (human) population density.
3. Annotated the PPIs with the total biomass calculated from the Sovon counts.

With the dataset of annotated PPIs, we can start to explore the relation between fireworks disturbance and the birds measured aloft during NYE 2017-2018.

The following parameters we assume are important predictors for measured bird densities aloft:

1. The total biomass of birds on the ground.
2. The take-off habitat of these birds.
3. The human population in the vicinity of birds.
4. The distance to the nearest inhabited urban area.

## Setting-up the environment

Load the required packages.

```{r setup_modelling_environment, warning=FALSE, message=FALSE}
library(ggstatsplot)
library(ggplot2)
library(dplyr)
library(readr)
library(tidyr)
library(gbm)
library(dismo)
library(ggBRT)
library(patchwork)
library(pdp)
library(parallel)
library(ggridges)
cpucores <- 6
```

In the previous chapter, we have created some datasets encompassing all the data contained within the individual PPIs at different resolutions. We will determine the optimal resolution and then continue using this dataset for further modelling.

```{r load_data}
resolutions <- file.path("data/processed", c("all_500m.RDS", "all_1000m.RDS", "all_2000m.RDS"))
data <- lapply(resolutions, function(x) readRDS(x))
names(data) <- c("500m", "1000m", "2000m")
```

## Preparing a dataset for modelling

As we're mostly interested in the moment of en masse take-off of birds, and we want to limit the effects of dispersal, we will limit our analysis to the first 5 minutes after 00:05 on January 1st, 2018 (or 23:05 on December 31st, 2017 in UTC), as both radar sites (Den Helder and Herwijnen) show a low `VIR` prior to and a rapid increase in `VIR` for that period (see [Identifying moment of take-off]). Making sure birds are thus still sufficiently 'linked' to the take-off sites requires that we limit our analysis to only this one scan.

Furthermore, we want to make sure that:
1. the area is 'covered' by at least 1 radar,
1. we have an estimate of `total_biomass`for these sites,
1. the proportion of urban area (`urban`) in the PPI pixel is less than .25,
1. `VIR` is > 0 (otherwise log-conversion will return `-Inf`), so we replace 0-values with 1e-3,
1. the radar beam does not overshoot birds too much based on the `vp`.

```{r prepare_modelling_dataset}
dt_start <- as.POSIXct("2017-12-31 23:05:00", tz = "UTC")
dt_end <- as.POSIXct("2017-12-31 23:10:00", tz = "UTC")

clean_data <- function(data, scan_start, scan_end, max_distance) {
  mdl_variables <- c("VIR", "dist_radar", "datetime", "total_biomass", "urban", "agricultural", "semiopen", "forests", "wetlands", "waterbodies", 
                     "dist_urban", "human_pop", "disturb_pot", "pixel", "coverage", "class", "x", "y")
  log10_variables <- c("dist_urban", "human_pop", "total_biomass", "dist_urban", "disturb_pot")
  
  data %>%
    filter(coverage > 0,
           class != 1,
           datetime >= scan_start & datetime < scan_end,
           total_biomass > 0,
           dist_radar < max_distance,
           urban < 0.25) %>%
    mutate(VIR = replace_na(VIR, 0.001),
           VIR = log10(VIR),
           disturb_pot = human_pop / dist_urban,
           total_biomass = total_biomass / 1000) %>%
    dplyr::select(all_of(mdl_variables)) %>%
    filter_all(all_vars(is.finite(.))) %>%
    identity() -> data_cleaned
  
  data_cleaned
}

data_cleaned <- lapply(data, function(x) clean_data(x, dt_start, dt_end, 66000))
rm(data)
```

## Determine model resolution based on performance in simple `total_biomass` model

As we want to correct for the influence of `total_biomass` on the measured response by the radars, we will test the performance of a simple model using just `dist_radar` to correct for range-biased measurement error and `total_biomass` for the resolutions we have generated composte PPIs for so far (500m, 1000m and 2000m). The most performant model is then a logical candidate to continue working with. Learning rates are tweaked roughly to force training of the model without 1) hitting a maximum number of trees limit and 2) avoid overfitting.

```{r}
x_vars <- match(c("dist_radar", "total_biomass"), colnames(data_cleaned[[1]]))
resolution_models <- mcmapply(function(dataset, lr) gbm.step(data = dataset, gbm.y = 1, gbm.x = x_vars, bag.fraction = 0.5, family = "gaussian", 
                                                             tree.complexity = 2, learning.rate = lr, silent = TRUE, plot.main = FALSE),
                              dataset = data_cleaned, lr = c(0.5, 0.1, 0.01), SIMPLIFY = FALSE, mc.preschedule = FALSE, mc.cores = cpucores)
```

With these models trained, we can now compare their performance.

```{r}
resolution_perf <-ggPerformance("500m" = resolution_models[[1]], "1000m" = resolution_models[[2]], "2000m" = resolution_models[[3]])
rm(resolution_models)
resolution_perf
```

It is clear that as the resolution increases (in other words: the cellsize decreases) the performance of `total_biomass` improves substantially, so it makes sense to continue with the 500m resolution data. Additionally, this suggests that dispersion is not really a problematic factor yet, as in that case the performance of a model based on `total_biomass` should not have been so good using small pixel sizes.

```{r}
data_cleaned <- data_cleaned[[which.max(resolution_perf["cvPer.Expl", ])]]
```

## Check for correlations among predictors

We have to see if variables are strongly correlated and thus unfit for being included in the same model, so we calculate Spearman correlation coefficients for all numerical predictors. As this is ecological data, some degree of correlation is of course inevitable for most variables.

```{r check_for_correlations, warning=FALSE, fig.width=10}
mdl_variables <- c("VIR", "dist_radar", "datetime", "total_biomass", "urban", "agricultural", "semiopen", "forests", "wetlands", "waterbodies", 
                   "dist_urban", "human_pop", "disturb_pot", "pixel", "coverage", "class")
predictors <- mdl_variables[!mdl_variables %in% c("VIR", "datetime", "pixel", "x", "y", "coverage", "class")]
corr_radar <- ggcorrmat(data_cleaned, output = "plot", type = "spearman", cor.vars = all_of(predictors), colors = c("#2166AC", "#F7F7F7", "#B2182B"))
corr_radar
```

So obviously there is a strong correlation between the different proxies for disturbance and between `agricultural` and the other land use proportions. For the disturbance proxies it would make sense to determine which is most performant, but for the land use proportions we have to determine if `agricultural` should be included in the model with these other predictors.

## Correlated land use proportions

Let's see what the distributions of values is for the different land use proportions.

```{r show_correlated_land_use, message=FALSE}
data_cleaned %>%
  dplyr::select(urban, agricultural, semiopen, forests, wetlands, waterbodies) %>%
  pivot_longer(cols = everything(), names_to = "landuse", values_to = "value") %>%
  ggplot(aes(x = value, y = landuse, fill = stat(x))) +
  geom_density_ridges_gradient(scale = 1, rel_min_height = 0.0) +
  scale_fill_viridis_c(name = "LU Prop.", option = "C") +
  labs(title = "Distribution of land use proportions in modelling dataset") +
  xlab("Land use proportion") +
  ylab("Land use class")
```

As can be expected in The Netherlands, the dominant land use class is clearly `agricultural` with the most prominent peak at full PPI  pixel coverage (values close to 1) and all other land use classes peaking very strongly at low proportions. This essentially means that when one of the other land use classes increases in proportion, this will almost always come at the cost of the proportion of `agricultural` and vice versa. So, using all these land use proportions in the model will be uninformative, and will blow up our uncertainty estimates after bootstrapping. Therefore, we will not train a model containing *both* `agricultural` and one of the other land use classes.

## Determine the most suitable proxy for disturbance

As could be expected, the three 'disturbance parameters' (`dist_urban`, `human_pop`, `disturb_pot`) are highly correlated, so we should see which of these is most suitable to include in our model, as it makes no sense to include all. To assess that, we will simply compare the performance of each of the models trained with a single disturbance parameter.

```{r compare_disturbance_proxies}
lr <- 0.05
max_trees <- nrow(data_cleaned) * 0.25
silence <- TRUE

base_model <- c("dist_radar", "total_biomass", "urban", "semiopen", "forests", "wetlands", "waterbodies")
disturbance_proxies <- c("dist_urban", "human_pop", "disturb_pot")
x_vars <- lapply(disturbance_proxies, function(x) match(c(base_model,x), colnames(data_cleaned)))
names(x_vars) <- disturbance_proxies
```

```{r train_disturbance_models, eval=full_repro}
disturbance_models <- mclapply(x_vars, function(x) gbm.step(data = data_cleaned, gbm.y = 1, gbm.x = x, bag.fraction = 0.5, family = "gaussian", 
                                                            tree.complexity = 2, learning.rate = lr, silent = silence, max.trees = max_trees, 
                                                            step.size = 50, plot.main = FALSE), mc.cores = cpucores, mc.preschedule = FALSE)
names(disturbance_models) <- disturbance_proxies
saveRDS(disturbance_models, file = "data/models/brt_models_disturbance.RDS")
```

Let's compare the performance:

```{r compare_disturbance_proxies_performance, results='hold'}
disturbance_models <- readRDS("data/models/brt_models_disturbance.RDS")
disturb_perf <- ggPerformance("dist_urban" = disturbance_models[[1]], "human_pop" = disturbance_models[[2]], "disturb_pot" = disturbance_models[[3]])
disturb_perf
```

We have a clear winner here: `dist_urban` performs the best, so we'll continue with that proxy for disturbance.

``` {r set_best_disturbance_proxy}
best_disturbance_proxy <- names(which.max(disturb_perf["cvPer.Expl", ]))
```

## Determine best hyperparameter settings for `lr`

To avoid overfitting to noise, we will limit the maximum nr of trees used in the model to 25% of the number of datapoints, which yields a possible `r round(nrow(data_cleaned) * 0.25)` trees to be used. We still want to make sure our model reaches some measure of optimality (in this case a reduction of cross-validated deviance explained), we have to do a hyperparameter search for a suitable combination of maximum number of trees and learning rate. See Elith et al. for an explanation on the linkage between number of trees and learning rate.

```{r determine_learning_rates}
learning_rates <- c(0.1, 0.15, 0.2, 0.3, 0.4, 0.5)  # Lower and higher learning rates certainly do not converge quick enough or increase error
```
```{r train_model_hyperparameters, eval=full_repro}
hyperparms <- mclapply(learning_rates, function(x) { gbm.step(data = data_cleaned, gbm.y = 1, gbm.x = x_vars[[best_disturbance_proxy]], bag.fraction = 0.5,
                                                              family = "gaussian", plot.main = FALSE, tree.complexity = 2, learning.rate = x, 
                                                              max.trees = max_trees, step.size = 50, silent = TRUE)}, 
                       mc.preschedule = FALSE, mc.cores = 3)
saveRDS(hyperparms, file = "data/models/brt_models_hyperparms.RDS")
```

```{r compare_hyperparameter_settings}
hyperparms <- readRDS(file = "data/models/brt_models_hyperparms.RDS")
max_used_trees <- max(unlist(lapply(hyperparms, function(x) {if (!is.null(x)) { max(x$trees.fitted) }})))
trees <- data.frame(trees = seq(from = 50, to = max_used_trees * 2, by = 50))
holdout_deviance <- lapply(hyperparms, function(x) { if (!is.null(x)) { x$cv.values }} )
missing <- which(unlist(lapply(holdout_deviance, is.null)))

h <- matrix(data = NA, nrow = dim(trees)[1], ncol = length(holdout_deviance))
for (i in seq_along(holdout_deviance)) {
  h[1:length(holdout_deviance[[i]]), i] <- holdout_deviance[[i]]
}
h <- as.data.frame(h)

hp <- data.frame(trees, h)
colnames(hp) <- c(colnames(trees), learning_rates)

hp %>%
  pivot_longer(cols = c(-trees), names_to = "lr", values_to = "holdout_deviance") %>%
  identity() -> hp

ggplot(hp) +
  geom_line(aes(x = trees, y = holdout_deviance, color = lr), size = 1)
```

It seems a learning rate of 0.3 is a good balance between speedy conversion and model performance. We can now retrain our most performant disturbance model with these new parameters.

## Retraining the disturbance models using optimised learning rate and maximum number of trees

We can now retrain the models with the best combination of learning rate and maximum number of trees.

```{r retrain_disturbance_models, eval=full_repro}
lr <- 0.3
max_trees <- nrow(data_cleaned) * 0.25
silence <- TRUE

disturbance_models <- mclapply(x_vars, function(x) gbm.step(data = data_cleaned, gbm.y = 1, gbm.x = x, bag.fraction = 0.5, family = "gaussian", 
                                                          tree.complexity = 2, learning.rate = lr, silent = silence, max.trees = max_trees, step.size = 50,
                                                          plot.main = FALSE, verbose = FALSE), mc.preschedule = FALSE, mc.cores = cpucores)
names(disturbance_models) <- disturbance_proxies
saveRDS(disturbance_models, file = "data/models/brt_models_disturbance.RDS")
```

And once again we can compare the performance

```{r compare_performance_retrained_disturbance_models}
disturbance_models <- readRDS("data/models/brt_models_disturbance.RDS")
ggPerformance("dist_urban" = disturbance_models[[1]], "human_pop" = disturbance_models[[2]], "disturb_pot" = disturbance_models[[3]])
```

So the model with the disturbance proxy of ``r best_disturbance_proxy`` continues to perform best and we'll save it for future use.

```{r save_model}
model_disturbance_best <- disturbance_models[best_disturbance_proxy]
saveRDS(model_disturbance_best, file = "data/models/brt_models_disturbance_best.RDS")
```

## Relative influence of variables

We can now also compare the relative influence of the variables used in this model.

```{r determine_relatieve_influence_of_predictors, results='hold'}
relinf <- ggInfluence(disturbance_models$dist_urban, show.signif = FALSE, main = "Relative influence of predictor variables on log(VIR)")
relinf
```

The high relative importance of `total_biomass` (`r round(relinf["total_biomass", ], digits = 1)`%) suggests a clear link between birds counted on the ground (by Sovon's observers) and what is measured aloft, exactly what one would expect if birds are still sufficiently 'tied' to their take-off habitat.

## Models for seperate land uses

As the land use proportions are correlated, and `agricultural` land use is by far the most common in The Netherlands, it may be a worthwhile exercise to model the effects of each land use proportion seperately. This way, we may discover effects that disappear because of the compound effects of correlation among the other predictors.

```{r train_landscape_models, eval=full_repro}
lr <- 0.3
silence <- TRUE

base_model <- c("dist_radar", "total_biomass", best_disturbance_proxy)
landscapes <- c("urban", "agricultural", "semiopen", "forests", "wetlands", "waterbodies")
x_vars <- lapply(landscapes, function(x) match(c(base_model,x), colnames(data_cleaned)))

landscape_models <- mclapply(x_vars, function(x) gbm.step(data = data_cleaned, gbm.y = 1, gbm.x = x, bag.fraction = 0.5, family = "gaussian", 
                                                          tree.complexity = 2, learning.rate = lr, silent = silence, max.trees = max_trees, plot.main = FALSE),
                             mc.preschedule = FALSE, mc.cores = 3)
names(landscape_models) <- c(landscapes)
saveRDS(landscape_models, file = "data/models/brt_models_landscapes.RDS")
```

And once again we compare the performance

```{r compare_performance_of_landscape_models}
landscape_models <- readRDS("data/models/brt_models_landscapes.RDS")
ggPerformance("urban" = landscape_models[[1]], "agricultural" = landscape_models[[2]], "semiopen" = landscape_models[[3]], "forests" = landscape_models[[4]], 
              "wetlands" = landscape_models[[5]], "waterbodies" = landscape_models[[6]])
```

With these models trained as well, we will save the final set of models.

```{r save_all_models}
models <- c(disturbance_models, landscape_models)
saveRDS(models, file = "data/models/brt_models.RDS")
```

## Partial dependence

Partial dependence plots (PDP) can visualise the modelled marginal effect a feature has on the predicted output of a model. For exploratory purposes, we will create a quick visualisation of the PDPs for the trained model, before we start with a bootstrapping procedure to test the robustness of these results.

```{r visualise_partial_dependencies, warning=FALSE, message=FALSE, results='hold'}
plot_pdp <- function(model, predictor) {
  p <- partial(model, train = model$gbm.call$dataframe, pred.var = predictor, type = "regression", plot = TRUE, plot.engine ="ggplot2",
               smooth = TRUE, rug = TRUE, n.trees = model$n.trees)
  p
}
modelnames <- names(models)
i <- 1

for (model in models) {
  relinf <- ggInfluence(model, plot = FALSE)
  plots <- lapply(rownames(relinf), function(x) {
    plot_pdp(model = model, predictor = x) + 
      xlab(paste(x, " (", round(relinf[x, 1]), "%)", sep = "")) +
      ylab("log(VIR)")
    })
  print(wrap_plots(plots) + plot_annotation(title = modelnames[i]))
  i <- i + 1
}
```

## Interactions

We can now also see which interactions have been modelled (supported by data).

```{r}
interactions <- suppressMessages(ggInteract_list(models$dist_urban))
plots <- mapply(function(x, y) ggInteract_2D(models$dist_urban, x = x, y = y, z.range = c(-6, 6)), 
                x = as.character(interactions$var1.names), y = as.character(interactions$var2.names), SIMPLIFY = FALSE)
interactions
plots
```


## Spatial autocorrelation

Model residuals should not show autocorrelation as then the independency of errors assumption is violated (see for example [this](https://doi.org/10.1186/s41610-019-0118-3)). Additionally, a model with strong autocorrelation in the residuals suggests that it lacks a spatial component within the predictors.

```{r calculate_model_residual_autocorrelation, eval=full_repro}
df <- models$dist_urban$gbm.call$dataframe
df$residual <- resid(models$dist_urban)

df %>%
  slice_sample(prop = 0.33) -> df_sample

correlogram <- correlog(df_sample[, c("x", "y")], df_sample[, "residual"], method = "Moran", nbclass = 20)
saveRDS(correlogram , file = "data/processed/correlogram.RDS")
```
```{r plot_model_residual_autocorrelation}
plot(correlogram)
```
