# (PART) Fireworks disturbance {-}

# Modelling fireworks disturbance

We have so far:

1. Pre-processed the radar data by removing clutter and applying the range-bias correction [@kranstauber2020].
2. Annotated the PPIs with land use proportions and indicators of disturbance, i.e. distance to inhabited urban areas and (human) population density.
3. Annotated the PPIs with the total biomass calculated from the Sovon counts.

With the dataset of annotated PPIs, we can start to explore the relation between fireworks disturbance and the birds measured aloft during NYE 2017-2018.

The following parameters we assume are important predictors for measured bird densities aloft:

1. The total biomass of birds on the ground.
2. The take-off habitat of these birds.
3. The human population in the vicinity of birds.
4. The distance to the nearest inhabited urban area.

## Processing environment

```{r setup_modelling_environment, warning=FALSE, message=FALSE}
library(ggstatsplot)
library(ggplot2)
library(dplyr)
library(readr)
library(tidyr)
library(parallel)
library(ggridges)
library(pgirmess)
library(mboost)
library(leaflet)
library(leaflet.opacity)
library(leafem)
library(usdm)
library(mapview)
```

In the previous chapter, we have created some datasets encompassing all the data contained within the individual PPIs at different resolutions. We will determine the optimal resolution and then continue using this dataset for further modelling.

```{r load_data}
resolutions <- file.path("data/processed", c("all_500m.RDS", "all_1000m.RDS", "all_2000m.RDS"))
data <- lapply(resolutions, function(x) readRDS(x))
names(data) <- c("500m", "1000m", "2000m")
```

## Preparing a dataset for modelling

As we're mostly interested in the moment of en masse take-off of birds, and we want to limit the effects of dispersal, we will limit our analysis to the first 5 minutes after 00:05 on January 1st, 2018 (or 23:05 on December 31st, 2017 in UTC), as both radar sites (Den Helder and Herwijnen) show a low `VIR` prior to and a rapid increase in `VIR` for that period (see [Identifying moment of take-off]). Making sure birds are thus still sufficiently 'linked' to the take-off sites requires that we limit our analysis to only this one scan.

Furthermore, we want to make sure that:
1. the area is 'covered' by at least 1 radar,
1. we have an estimate of `total_biomass`for these sites,
1. the proportion of urban area (`urban`) in the PPI pixel is less than .25,
1. `VIR` is > 0 (otherwise log-conversion will return `-Inf`), so we replace 0-values with 1e-3,
1. the radar beam does not overshoot birds too much based on the `vp`.

```{r prepare_modelling_dataset}
dt_start <- as.POSIXct("2017-12-31 23:05:00", tz = "UTC")
dt_end <- as.POSIXct("2017-12-31 23:10:00", tz = "UTC")

clean_data <- function(data, scan_start, scan_end, max_distance) {
  mdl_variables <- c("VIR", "dist_radar", "datetime", "total_biomass", "agricultural", "semiopen", "forests", "wetlands", "waterbodies", "urban",
                     "dist_urban", "human_pop", "disturb_pot", "pixel", "coverage", "class", "x", "y")
  log10_variables <- c("dist_urban", "human_pop", "total_biomass", "dist_urban", "disturb_pot")
  
  data %>%
    filter(coverage > 0,
           class != 1,
           datetime >= scan_start & datetime < scan_end,
           total_biomass > 0,
           dist_radar < max_distance, 
           urban < 0.1) %>%
    mutate(VIR = replace_na(VIR, 0.1),
           VIR = log10(VIR),
           disturb_pot = human_pop / dist_urban,
           total_biomass = total_biomass / 1000) %>%
    dplyr::select(all_of(mdl_variables)) %>%
    filter_all(all_vars(is.finite(.))) %>%
    identity() -> data_cleaned
  
  data_cleaned
}

data_cleaned <- lapply(data, function(x) clean_data(x, dt_start, dt_end, 66000))
rm(data)
```

## Determine model resolution based on performance in simple `total_biomass` model

As we want to correct for the influence of `total_biomass` on the measured response by the radars, we will test the performance of a simple model using just `dist_radar` to correct for range-biased measurement error and `total_biomass` for the resolutions we have generated composte PPIs for so far (500m, 1000m and 2000m).

Let's start with a calculation of the RMSE.

```{r determine_model_resolution}
resolution_models <- mcmapply(function(dataset) mboost(VIR ~ bbs(dist_radar) + bbs(total_biomass), 
                                                       data = dataset, control = boost_control(mstop = 10000, trace = TRUE)),
                              dataset = data_cleaned, SIMPLIFY = FALSE)
RMSE <- function(error) { sqrt(mean(error^2)) }
sapply(resolution_models, function(x) RMSE(residuals(x)))
```

We can also use percentage of [deviance explained](https://doi.org/10.1016/S0304-3800(00)00354-9)

```{r resolution_model_deviance_explained}
deviance_explained <- function(observed, predicted) {
  p <- predicted
  o <- observed
  i <- rep(mean(observed), length(observed))  # Intercept
  total.deviance <- sum((o - i) * (o - i)) / length(observed)  # Deviance from an intercept-only model
  resid.deviance <- sum((o - p) * (o - p)) / length(observed)  # Deviance from the fitted model
  (total.deviance - resid.deviance) / total.deviance
}

mapply(function(data, model) { deviance_explained(data$VIR, predict(model))}, data = data_cleaned, model = resolution_models)
```

Turns out most models perform quite similarly. However, to make the link between birds and the habitat they take off from as strong as possible, it makes sense to continue just with the 500m model.

```{r set_final_resolution}
data_cleaned <- data_cleaned$`500m`
saveRDS(data_cleaned, file = "data/models/data_cleaned.RDS")
```

## Check for correlations among predictors

We have to see if variables are strongly correlated and thus unfit for being included in the same model, so we calculate Spearman correlation coefficients for all numerical predictors. As this is ecological data, some degree of correlation is of course inevitable for most variables.

```{r check_for_correlations, warning=FALSE, fig.width=12, fig.height=12}
mdl_variables <- c("VIR", "dist_radar", "datetime", "total_biomass", "agricultural", "semiopen", "forests", "wetlands", "waterbodies", "urban", 
                   "dist_urban", "human_pop", "disturb_pot", "pixel", "coverage", "class")
predictors <- mdl_variables[!mdl_variables %in% c("VIR", "datetime", "pixel", "x", "y", "coverage", "class")]
corr_radar <- ggcorrmat(data_cleaned, output = "plot", type = "spearman", cor.vars = all_of(predictors), colors = c("#2166AC", "#F7F7F7", "#B2182B"))
corr_radar
```

So obviously there is correlation between the disturbance proxies and between `agricultural` and other land use proportions. For the disturbance proxies it would make sense to determine which is most performant (including all does not make sense), but we can continue using the `agricultural` predictor as boosted models do not suffer from multicollinearity problems like traditional (e.g.) GAMs.

## Correlated land use proportions

While we're at it, let's see what the distributions of values are for the different land use proportions.

```{r show_correlated_land_use, message=FALSE}
data_cleaned %>%
  dplyr::select(agricultural, semiopen, forests, wetlands, waterbodies) %>%
  pivot_longer(cols = everything(), names_to = "landuse", values_to = "value") %>%
  ggplot(aes(x = value, y = landuse, fill = stat(x))) +
  geom_density_ridges_gradient(scale = 1, rel_min_height = 0.0) +
  scale_fill_viridis_c(name = "LU Prop.", option = "C") +
  labs(title = "Distribution of land use proportions in modelling dataset") +
  xlab("Land use proportion") +
  ylab("Land use class")
```

As can be expected in The Netherlands, the dominant land use class is clearly `agricultural` with the most prominent peak at full PPI  pixel coverage (values close to 1) and all other land use classes peaking very strongly at low proportions. This essentially means that when one of the other land use classes increases in proportion, this will almost always come at the cost of the proportion of `agricultural` and vice versa.

## Determine the most suitable proxy for disturbance

Now we will compare the disturbance proxies in a similar fashion to how we compared the performance of the model using different PPI resolutions, by calculating deviance explained and RMSE.

```{r compare_proxies_for_disturbance}
fm_dist_urban <- VIR ~ bbs(dist_radar) + bbs(total_biomass) + bbs(semiopen) + bbs(forests) + bbs(wetlands) + bbs(waterbodies) + bbs(agricultural) + bbs(dist_urban)
fm_human_pop <- VIR ~ bbs(dist_radar) + bbs(total_biomass) + bbs(semiopen) + bbs(forests) + bbs(wetlands) + bbs(waterbodies) + bbs(agricultural) + bbs(human_pop)
fm_disturb_pot <- VIR ~ bbs(dist_radar) + bbs(total_biomass) + bbs(semiopen) + bbs(forests) + bbs(wetlands) + bbs(waterbodies) + bbs(agricultural) + bbs(disturb_pot)
formulas <- list(fm_dist_urban, fm_human_pop, fm_disturb_pot)

disturbance_models <- mcmapply(function(formula) mboost(formula, data = data_cleaned, control = boost_control(mstop = 10000, trace = TRUE)), 
                               formula = formulas, SIMPLIFY = FALSE)
names(disturbance_models) <- c("dist_urban", "human_pop", "disturb_pot")
RMSE <- function(error) { sqrt(mean(error^2)) }
rmse <- sapply(disturbance_models, function(x) RMSE(residuals(x)))
d2 <- mapply(function(model) { deviance_explained(data_cleaned$VIR, predict(model))}, model = disturbance_models)
as.data.frame(list("RMSE" = as.matrix(rmse), "D2" = as.matrix(d2)))
```

It is clear `dist_urban` performs the best, though just by a slight margin. As it is probably the most interpretable proxy from a policy perspective, we can continue using it without any regrets.

```{r set_final_disturbance_proxy}
model <- disturbance_models$dist_urban
saveRDS(model, file = "data/models/model.RDS")
```

## Determine the optimal number of boosting steps

The main hyperparameter to be tuned for the boosted GAMs using `mboost` is the number of boosting iterations/steps. By stopping boosting before model performance (measured using cross-validation) worsens, we can avoid overfitting. We limit this test of model performance for a maximum of 50,000 boosts.

```{r train_model_50000_boosts, eval=full_repro}
model_boosts <- mboost(fm_dist_urban, data = data_cleaned, control = boost_control(mstop = 50000, trace = FALSE))
saveRDS(model_boosts, file = "data/models/model_boosts.RDS")
```

```{r determine_boosting_steps, eval=full_repro}
model_boosts <- readRDS("data/models/model_boosts.RDS")
cv10f <- cv(model.weights(model_boosts), type = "kfold", B = 10)
cvm <- cvrisk(model_boosts, folds = cv10f, papply = lapply)
saveRDS(cvm, file = "data/models/cvm_boosts.RDS")
```

```{r plot_boosting_steps}
cvm <- readRDS("data/models/cvm_boosts.RDS")
plot(cvm)
```

We can see that model performance improves very little beyond a few thousand boosting iterations. As we have to quantify model uncertainty using bootstrapping techniques, it makes little computational sense to push much beyond 10000 boosts, as that would just make the bootstrapping procedure last much longer.

## Variable importance

We can quantify the importance of variables within our model using the `varimp` function which returns variable importance, a measure of the total improvement to model deviance a variable is responsible for.

```{r variable importance}
plot(varimp(model))
```

## Model marginal effects

Let's quickly visualise marginal effects of this model, showing how individual variables influence model outcome with all other variables held constant.

```{r show_marginal_effects}
par(mfrow = c(2, 4))
plot(model)
```

## Spatial autocorrelation

Model residuals should not show strong autocorrelation as then the independency of errors assumption is violated (see for example [this](https://doi.org/10.1186/s41610-019-0118-3)). Additionally, a model with strong autocorrelation in the residuals suggests that it lacks a spatial component within the predictors. As it is computationally too difficult to calculate the spatial autocorrelation for the entire dataset at once, we subsample 25% of datapoints for these calculations. Results may thus vary to some degree.

```{r calculate_model_residual_autocorrelation, eval=full_repro}
df <- data_cleaned
df$residual <- resid(model)

df %>%
  filter(dist_radar < 66000) %>%
  slice_sample(prop = 0.25) -> df_sample

correlogram <- correlog(df_sample[, c("x", "y")], df_sample[, "residual"], method = "Moran", nbclass = NULL)
saveRDS(correlogram , file = "data/models/correlogram.RDS")
```
```{r plot_model_residual_autocorrelation}
correlogram <- readRDS("data/models/correlogram.RDS")
plot(correlogram)
```

This shows there's some spatial autocorrelation in residuals remaining. We use a plot of residuals to assess where this occurs and if this indicates our model is missing some important spatial effect, or if there is another explanation.

```{r plot_model_residuals_in_space}
df$preds <- predict.mboost(model, newdata = data_cleaned)
ppi <- readRDS("data/processed/composite-ppis/500m/201712312305.RDS")
 
ppi$data@data %>%
  left_join(dplyr::select(df, pixel, preds, residual), by = "pixel") -> ppi$data@data
ppi$data@data$VIR_log <- log10(ppi$data@data$VIR)
ppi$data@data$total_biomass_log <- log10(ppi$data@data$total_biomass / 1000)

pal_resid <- colorspace::diverging_hcl(50, "Blue-Red 3", power = 2)
z_lims_resid <- c(-3, 3)
palette_resid <- colorNumeric(pal_resid, na.color = "#00000005", domain = z_lims_resid)
raster_resid <- as(ppi$data["residual"], "RasterLayer")
raster_resid[raster_resid <= z_lims_resid[1]] <- z_lims_resid[1]
raster_resid[raster_resid >= z_lims_resid[2]] <- z_lims_resid[2]

leaflet() %>%
  addTiles(group = "OSM (default)") %>%
  addRasterImage(raster_resid, colors = palette_resid, layerId = "resid", group = "resid") %>%
  addImageQuery(raster_resid, layerId = "resid") %>%
  addLegend(pal = palette_resid, values = z_lims_resid) %>%
  addOpacitySlider(layerId = "resid") %>%
  identity()
```

The plot shows how residuals become larger as the distance from the radars increases, and this is a more pronounced effect in the domain of the Den Helder radar. This effect is partially a consequence of radar range-bias and using a single `distance to radar` vs. 2 for each radar separately. You can see how this results in mostly 'overprediction' within the Herwijnen radar domain at long distances and 'underprediction' of `VIR` at long distances within the domain of Den Helder.

To illustrate this, we can limit the calculation of residual spatial autocorrelation to datapoints that are substantially closer to the radar. This should then result in Moran's I values substantially closer to 0.

```{r calculate_model_residual_autocorrelation_limited_domain, eval=full_repro}
df <- data_cleaned
df$residual <- resid(model)

df %>%
  filter(dist_radar < 50000) %>%
  slice_sample(prop = 0.25) -> df_sample

correlogram <- correlog(df_sample[, c("x", "y")], df_sample[, "residual"], method = "Moran", nbclass = NULL)
saveRDS(correlogram , file = "data/models/correlogram_limited_domain.RDS")
```
```{r plot_residual_autocorrelation_limited_domain}
correlogram <- readRDS("data/models/correlogram_limited_domain.RDS")
plot(correlogram)
```

As indeed is the case.

To illustrate this even better, we can calculate local Moran's I values to see where exactly spatial autocorrelation occurs.

```{r calculate_local_moran_i, eval=full_repro}
r <- as(ppi$data["residual"], "RasterLayer")
local_sa <- lisa(x = r, d1 = 0, d2 = 1500, statistic = "I")
saveRDS(local_sa, file = "data/models/local_sa.RDS")
```
```{r plot_local_moran_i}
local_sa <- readRDS("data/models/local_sa.RDS")
mapview(local_sa)
```

We can see - indeed - that spatial autocorrelation in the residuals is strongest at distance from the radar and thankfully does not occur in areas that contain areas of scarce land use types (eg forests). While some degree of spatial autocorrelation in residuals is present, this indicates it does not affect our interpretation of results.
