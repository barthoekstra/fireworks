# (PART) Fireworks disturbance {-}

# Modelling fireworks disturbance

We have so far:

1. Pre-processed the radar data by removing clutter and applying the range-bias correction [@kranstauber2020].
2. Annotated the PPIs with land use proportions and indicators of disturbance, i.e. distance to inhabited urban areas and (human) population density.
3. Annotated the PPIs with the calculated total biomass calculated from the Sovon counts.

With the dataset of annotated PPIs, we can start to explore the relation between fireworks disturbance and the birds measured aloft during NYE 2017-2018.

The following parameters we assume are important predictors for measured bird densities aloft:

1. The total biomass of birds on the ground.
2. The take-off habitat of these birds.
3. The human population in the vicinity of birds.
4. The distance to the nearest inhabited urban area.

## Setting-up the environment

Load the required packages.

```{r setup_modelling_environment}
library(ggstatsplot)
library(ggplot2)
library(dplyr)
library(readr)
library(tidyr)
library(gbm)
library(dismo)
library(ggBRT)
library(patchwork)
library(pdp)
```

In the previous chapter, we have created a dataset encompassing all the data contained within the individual PPIs. We will use this for further modelling.

```{r load_data}
data <- readRDS("data/processed/all-500m.RDS")
```

## Preparing a dataset for modelling

As we're mostly interested in the moment of en masse take-off of birds, and we want to limit the effects of dispersal, we will limit our analysis to the first 10 minutes after 00:05 on January 1st, 2018 (or 23:05 on December 31st, 2017 in UTC), as both radar sites (Den Helder and Herwijnen) show a low `VIR` prior to, a rapid increase in `VIR` for that period, and the start of a decreasing `VIR` after that period (see [Identifying moment of take-off]). Making sure birds are still sufficiently 'linked' to the take-off sites requires that we limit our analyses to at most these scans.

```{r prepare_modelling_dataset}
dt_start <- as.POSIXct("2017-12-31 23:05:00", tz = "UTC")
dt_end <- as.POSIXct("2017-12-31 23:15:00", tz = "UTC")

mdl_variables <- c("VIR", "dist_radar", "datetime", "total_biomass", "urban", "agricultural", "semiopen", "forests", "wetlands", "waterbodies", 
                   "dist_urban", "human_pop", "disturb_pot", "pixel")
log10_variables <- c("dist_urban", "human_pop", "total_biomass", "dist_urban", "disturb_pot")

data %>%
  filter(coverage > 0,
         datetime >= dt_start & datetime < dt_end,
         total_biomass > 0,
         dist_radar < 80000) %>%
  mutate(disturb_pot = human_pop / dist_urban,
         total_biomass = total_biomass / 1000,
         VIR = log10(VIR)) %>%
  dplyr::select(all_of(mdl_variables)) %>%
  filter_all(all_vars(is.finite(.))) %>%
  identity() -> data_cleaned
```

## Check for correlations among predictors

We have to see if variables are strongly correlated and thus unfit for being included in the same model, so we calculate Spearman correlation coefficients for all numerical predictors. As this is ecological data, some degree of correlation is of course inevitable for most variables.

```{r check_for_correlations, warning=FALSE, fig.width=10}
predictors <- mdl_variables[!mdl_variables %in% c("VIR", "datetime", "pixel", "x", "y")]
corr_radar <- ggcorrmat(data_cleaned, output = "plot", type = "spearman", cor.vars = all_of(predictors), colors = c("#2166AC", "#F7F7F7", "#B2182B"))
corr_radar
```

## Determine the most suitable proxy for disturbance

As could be expected, the three 'disturbance parameters' (`dist_urban`, `human_pop`, `disturb_pot`) are highly correlated, so we have to see which of these is most suitable to include in our model. To assess that, we will simply compare the performance of each of the models trained with a single disturbance parameter.

```{r compare_disturbance_proxies, results='hold'}
lr <- 0.35
silence <- TRUE

base_model <- c("dist_radar", "total_biomass", "urban", "agricultural", "semiopen", "forests", "wetlands", "waterbodies")
brt_dist_urban <- gbm.step(data = data_cleaned, gbm.y = 1, gbm.x = match(c(base_model, "dist_urban"), colnames(data_cleaned)), 
                           bag.fraction = 0.75, family = "gaussian", tree.complexity = 2, learning.rate = lr, 
                           tolerance.method = "fixed", tolerance = 8e-4, silent = silence)
brt_human_pop <- gbm.step(data = data_cleaned, gbm.y = 1, gbm.x = match(c(base_model, "human_pop"), colnames(data_cleaned)), 
                          bag.fraction = 0.75, family = "gaussian", tree.complexity = 2, learning.rate = lr, 
                          tolerance.method = "fixed", tolerance = 8e-4, silent = silence)
brt_disturb_pot <- gbm.step(data = data_cleaned, gbm.y = 1, gbm.x = match(c(base_model, "disturb_pot"), colnames(data_cleaned)), 
                            bag.fraction = 0.75, family = "gaussian", tree.complexity = 2, learning.rate = lr, 
                            tolerance.method = "fixed", tolerance = 8e-4, silent = silence)

ggPerformance("dist_urban" = brt_dist_urban, "human_pop" = brt_human_pop, "disturb_pot" = brt_disturb_pot)
```

It turns out performance is broadly very similar across the measures of disturbance potential, but results may vary due to the non-deterministic behaviour of gradient-boosted machines. So, assuming `human_pop` captures the 'intensity' of fireworks and `dist_urban` captures some 'distance-dependence' of this effect, we will simply continue using the combined feature of 'disturbance potential' defined as $disturb\_pot = \frac{human\_pop}{dist\_urban}$.

## Train a model for the each of selected scans

For the timestamps we have now selected, we can train `gbm` models to determine for which timestamp the model performs best.

```{r train_model_for_seperate_scans, results='hold'}
nr_scans <- length(unique(data_cleaned$datetime))
datetimes <- sort(unique(data_cleaned$datetime))
models <- vector("list", nr_scans)
lr <- 0.05

for (i in seq_along(datetimes)) {
  scan_start <- datetimes[i]
  scan_end <- scan_start + 5 * 60
  
  data_cleaned %>%
    filter(datetime >= scan_start & datetime < scan_end) -> data_scan
  
  models[[i]] <- gbm.step(data = data_scan, gbm.y = 1, gbm.x = match(c(base_model, "disturb_pot"), colnames(data_scan)),
                          bag.fraction = 0.75, family = "gaussian", tree.complexity = 2, learning.rate = lr,
                          tolerance.method = "fixed", tolerance = 8e-4, plot.main = TRUE, verbose = FALSE)
}

saveRDS(models, file = "data/models/brt_models.RDS")
```

```{r compare_performance_scan_models}
models <- readRDS("data/models/brt_models.RDS")
names(models) <- datetimes
perf <- do.call(ggPerformance, m)

best_model_id <- which.max(perf["cvPer.Expl", ])

perf
```

We can now also compare the relative influence of the variables used in these models.

```{r determine_relatieve_influence_of_predictors, results='hold'}
relinf <- suppressMessages(do.call(ggMultiInfluence, models))
rownames(relinf) <- relinf$Predictor
relinf <- relinf[, c(2:3)]
```

The model trained on the `r best_model_id`nd scan (starting at `r strftime(as.POSIXct(colnames(perf)[best_model_id]), format = "%H:%M %Z")`) has the highest cross-validated percentage deviance explained. Additionally, the relative influence of `total_biomass` (`r round(relinf["total_biomass", 2], digits = 2)`) suggests a clear link between birds counted on the ground and what is measured aloft, which one would expect if birds are still sufficiently 'tied' to their take-off habitats.

## Partial dependence

Partial dependence plots (PDP) can visualse the modelled marginel effect a feature has on the predicted output of a model. For exploratory purposes, we will create a quick visualisation of the PDPs for both trained models, before we start with a bootstrapping procedure to test the robustness of these results.

```{r visualise_partial_dependencies, warning=FALSE, message=FALSE, results='hold'}
plot_pdp <- function(model, predictor) {
  p <- partial(model, train = model$gbm.call$dataframe, pred.var = predictor, type = "regression", plot = TRUE, plot.engine ="ggplot2",
               smooth = TRUE, rug = TRUE, n.trees = model$n.trees)
  p
}
modelnames <- names(models)
i <- 1

for (model in models) {
  plots <- lapply(rownames(relinf), function(x) {
    plot_pdp(model = model, predictor = x) + 
      xlab(paste(x, " (", round(relinf[x, i]), "%)", sep = "")) +
      ylab("log(VIR)")
    })
  print(wrap_plots(plots) + plot_annotation(title = modelnames[i]))
  i <- i + 1
}
```


